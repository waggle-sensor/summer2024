{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Langchain RAG\n",
    "\n",
    "This notebook shows the following: \n",
    "1. loading Markdown, Python, and Jupyter Notebook files from three different Sage repos\n",
    "2. embedding that data with the \"mxbai-embed-large\" model and store in a Chroma vector database\n",
    "3. querying that VB with RAG chains using llama3.1\n",
    "4. testing different prompt templates\n",
    "5. testing code generation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import DirectoryLoader, UnstructuredMarkdownLoader, PythonLoader, NotebookLoader\n",
    "from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter, PythonCodeTextSplitter\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Ingestion\n",
    "Using specific data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"./sage-website/docs/\", \"./sage-data-client/\", \"./pywaggle/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively loads all \"file_type\" files in a list of directory \"paths\" with the appropriate \"loader_class\"\n",
    "## returns a combined list of Documents from all paths\n",
    "def repo_class_loader(paths, glob, loader_cls):\n",
    "    combined_docs = []\n",
    "    for path in paths:\n",
    "        dir_loader = DirectoryLoader(path, glob=glob, loader_cls=loader_cls, recursive=True)\n",
    "        docs = dir_loader.load()\n",
    "        combined_docs.extend(docs)\n",
    "\n",
    "    return combined_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 68.8 ms, total: 1.13 s\n",
      "Wall time: 1.18 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sage-website/docs/contact-us.md'}, page_content='sidebar_label: Contact us\\n\\nContact us\\n\\nEmail\\n\\nFor support, general questions, or comments, you can always reach us at:\\n\\nsupport@waggle-edge.ai\\n\\nMessage Board\\n\\nWe also encourage developers and users to start a new topic or issue on the Waggle sensor message board:\\n\\nGitHub Discussions'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/sesctl.md'}, page_content=\"sidebar_label: sesctl sidebar_position: 2\\n\\nsesctl: a tool to schedule jobs in Waggle edge computing\\n\\nThe tool sesctl is a command-line tool that communicates with an Edge scheduler in the cloud to manage user jobs. Users can create, edit, submit, suspend, and remove jobs via the tool.\\n\\nInstallation\\n\\nThe tool can be downloaded from the edge scheduler repository and be run on person's desktop or laptop.\\n\\n:::note Please make sure to download the correct version of the tool based on the system architecture. For example, if you run it on a Mac download sesctl-darwin-amd64. :::\\n\\nbash chmod +x sesctl-<system>-<arch> ln sesctl-<system>-<arch> sesctl sesctl\\n\\nSubmit a job\\n\\nYou can follow the tutorial to submit an example job to understand how to design your own job.\\n\\nFor more tutorials\\n\\nThe in-depth tutorials on the functionalities that sesctl offers can be found in the README.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/pluginctl.md'}, page_content='sidebar_label: pluginctl sidebar_position: 1\\n\\npluginctl: a tool to develop and test plugins on a node\\n\\nWe developed the tool pluginctl to help end users develop and test their edge application (i.e., plugin) on a node before registering the plugin in Edge code repository. The tool helps on simplifying the process of testing the edge code and making changes as needed for development, by buildig the code into a container, running the container inside the node, and checking the result from the container.\\n\\nAll of Waggle nodes have the tool already installed. For plugin developers who have access to nodes, they can simply type the following to start with once they are logged into a node, bash sudo pluginctl\\n\\nThe in-depth tutorials on the functionalities that pluginctl offers can be found in the README.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/triggers.md'}, page_content='sidebar_label: Trigger examples sidebar_position: 3\\n\\nTrigger Examples\\n\\nThis page provides a few examples of triggers within Sage. Triggers are programs which generally use data and events from the edge or cloud to automatically drive or notify other behavior in the system.\\n\\nCloud-to-Edge Examples\\n\\nCloud-to-edge triggers are programs running in the cloud which monitor events or external data sources and then, in response, change some behavior on the nodes.\\n\\nSevere Weather Trigger\\n\\nThis example starts and stops jobs in response to severe weather events scraped from the National Weather Service API.\\n\\nWildfire Trigger\\n\\nThis example looks at results from the smoke detector job and modify its own scheduling interval in response. The concept is that as smoke is detected, we want to run more frequent detections.\\n\\nEdge-to-Cloud Examples\\n\\nEdge-to-cloud triggers are programs which monitor data published from the nodes and use it, potentially along with additional data sources, to perform some computation or actions.\\n\\nSage Data Client Batch Trigger\\n\\nThis is a simple batch trigger example of using Sage Data Client to print nodes where the internal mean temperature exceeds a threshold every 5 minutes.\\n\\nSage Data Client Stream Trigger\\n\\nThis is an example of using Sage Data Client to watch the data stream and print nodes where the internal temperature exceeds a threshold.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Developer quick reference\\n\\nDisclaimer\\n\\n:warning: This is a quick-reference guide, not a complete guide to making a plugin. Use this to copy-paste commands while working on plugins and to troubleshoot them in the testing and scheduling stages. Please consult the official :green_book:Plugin Tutorials for detailed guidance.\\n\\nTips\\n\\n:information_source: Plugin=App\\n\\n:green_book: = recommended code docs and tutorials from Sage.\\n\\n:point_right: First make a minimalistic app with a core functionality to test on the node. Later you may add all the options you want.\\n\\n:point_up: Avoid making a plugin from scratch. Use another plugin or this template for your first plugin or use :new: Cookiecutter Template.\\n\\n:warning: Repository names should be all in small alphanumeric letters and - (Do not use _)\\n\\nRequirements : Install Docker, git, and Python\\n\\nComponents of a plugin\\n\\nTypical components of a Sage plugin are described below:\\n\\n1. An application\\n\\nThis is just your usual Python program, either a single .py script or a set of directories with many components (e.g. ML models, unit tests, test data, etc).\\n\\n:point_right: First do this step on your machine and perfect it until you are happy with the core functionality.\\n\\napp/app.py* : the main Python file (sometimes also named main.py) contains the code that defines the functionality of the plugin or calls other scripts to do tasks. It usually has from waggle.plugin import Plugin call to get the data from in-built sensors and publishes the output.\\n\\nNote: Variable names in plugin.publish should be descriptive and specific.\\n\\nInstall pywaggle pip3 install -U \\'pywaggle[all]\\'\\n\\napp/test.py : optional but recommended file, contains the unit tests for the plugin.\\n\\n2. Dockerizing the app\\n\\n:point_right: Put everything in a Docker container using a waggle base image and make it work. This may require some work if libraries are not compatible. Always use the latest base images from Dockerhub\\n\\nDockerfile* : contains instructions for building a Docker image for the plugin. It specifies the waggle base image from dockerhub, sets up the environment, installs dependencies, and sets the entrypoint for the container.\\n\\n:warning: Keep it simple ENTRYPOINT [\"python3\", \"/app/app.py\"]\\n\\nrequirements.txt* : lists the Python dependencies for the plugin. It is used by the Dockerfile to install the required packages using pip.\\n\\nbuild.sh : is an optional shell script to automate building the complicated Docker image with tags etc.\\n\\nMakefile : optional but the recommended file includes commands for building the Docker image, running tests, and deploying the plugin.\\n\\n3. ECR configs and docs\\n\\nYou can do this step (except sage.yaml) after testing on the node but before the ERC submission. :smile:\\n\\nsage.yaml* : is the configuration file useful for ECR and job submission? Most importantly it specifies the version and input arguments.\\n\\nREADME.md and ecr-meta/ecr-science-description.md* : a Markdown file describing the scientific rationale of the plugin as an extended abstract. This includes a description of the plugin, installation instructions, usage examples, data downloading code snippets, and other relevant information.\\n\\n:bulb: Keep the same text in both files and follow the template of ecr-science-description.md.\\n\\necr-meta/ecr-icon.jpg : is an icon (512px x 512px or smaller) for the plugin in the Sage portal.\\n\\necr-meta/ecr-science-image.jpg : is a key image or figure plot that best represents the scientific output of the plugin.\\n\\n:::info :green_book: Check Sage Tuorial Part1 and Part2 :::\\n\\nGetting access to the node\\n\\nFollow this page: https://portal.sagecontinuum.org/account/access to access the nodes.\\n\\nTo test your connection the first time, execute ssh waggle-dev-sshd and enter your ssh key passphrase. You should get the following output,\\n\\nEnter passphrase for key /Users/bhupendra/.ssh/id_rsa: no command provided Connection to 192.5.86.5 closed.\\n\\nEnter the passphrase to continue.\\n\\nTo connect to the node, execute ssh waggle-dev-node-V032 and enter your passphrase (required twice).\\n\\nYou should see the following message,\\n\\nWe are connecting you to node V032\\n\\n:::info :green_book: See Sage Tuorial: Part 3 for details on this topic. :::\\n\\nTesting plugins on the nodes\\n\\n:::danger :warning: Do not run any app or install packages directly on the node. Use Docker container or pluginctl commands. :::\\n\\n1. Download and run it\\n\\nDownload\\n\\nIf you have not already done it, you need your plugin in a public GitHub repository at this stage.\\n\\nTo test the app on a node, go to nodes W0xx (e.g. W023) and clone your repo there using the command git clone.\\n\\nAt this stage, you can play with your plugin in the docker container until you are happy. Then if there are changes made to the plugin, I reccomend replicating the same in your local repository and pushing it to the github and node.\\n\\nor do git commit -am \\'changes from node\\' and git push -u origin main.\\n\\nHowever, before commiting from node, you must run following commands at least once in your git repository on the node. git config [--locale] user.name \"Full Name\" git config [--locale] user.email \"email@address.com\"\\n\\n:::danger :warning: Make sure your Dockerfile has a proper entrypoint or the pluginctl run will fail. :::\\n\\nTesting with Pluginctl\\n\\n:::info :green_book: For more details on this topic check pluginctl docs. :::\\n\\nThen to test execute the command sudo pluginctl build .. This will output the plugin-image registry address at the end of the build. Example: 10.31.81.1:5000/local/my-plugin-name\\n\\nTo run the plugin without input argument, use sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name>\\n\\nExecute the command with input arguments. sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name> -- -i top_camera.\\n\\nIf you need GPU, use the selector sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name> -- -i top_camera.\\n\\n:exclamation: -- is a separator. After the -- all arguments are for your entrypoint i.e. app.py.\\n\\nTo check running plugins, execute sudo pluginctl ps.\\n\\nTo stop the plugin, execute sudo pluginctl rm cloud-motion.\\n\\nTo check the log pluginctl logs cloud-motion :warning:Do not forget to stop the plugins after testing or it will run forever.\\n\\nTesting USBSerial devices\\n\\n:point_right:The USBserial device template is in Cookiecutter Template. Also check wxt536 plugin.\\n\\nSteps for working with a USB serial device\\n\\nFirst, you need to confirm which computing unit the USB device is connected to, RPi or nxcore.\\n\\nThen, you add the --selector and --privileged options to the pluginctl command during testing and specifying the same in the job.yaml for scheduling.\\n\\nTo test the plugin on nxcore, which has the USB device, use the command sudo pluginctl run -n testname --selector zone=core --privileged 10.31.81.1:5000/local/plugin-name.\\n\\nThe --selector and --privileged attributes should be added to the pluginSpec in the job submission script as shown in the example YAML code.\\n\\nYou can check which computing unit is being used by the edge scheduler by running the kubectl describe pod command and checking the output.\\n\\n:warning: Re/Check that you are using the correct USB port for the device if getting empty output or folder not found error.\\n\\n2. Check if it worked?\\n\\nLogin to the Sage portal and follow the instructions from the section See Your Data on Sage Portal\\n\\n3. Check why it failed?\\n\\nWhen you encounter a failing/long pending job with an error, you can use the following steps to help you diagnose the issue:\\n\\nFirst check the Dockerfile entrypoint.\\n\\nUse the command sudo kubectl get pod to get the name of the pod associated with the failing job.\\n\\nUse the command sudo kubectl logs <<POD_NAME>> to display the logs for the pod associated with the failing job. These logs will provide you with information on why the job failed.\\n\\nUse the command sudo kubectl describe pod POD_NAME to display detailed information about the pod associated with the failing job.\\n\\nThis information can help you identify any issues with the pod itself, such as issues with its configuration or resources.\\n\\nBy following these steps, you can better understand why the job is failing and take steps to resolve the issue.\\n\\n4. Troubleshooting inside the container using pluginctl\\n\\nFollow this tutorial to get in an already running container to troubleshoot the issue. If the plugin fails instantly and your are not able to get inside the container use following commands to override the entrypoint\\n\\nFirst Deploy with Custom Entrypoint --entrypoint /bin/bash: sudo pluginctl deploy -n testnc --entrypoint /bin/bash 10.31.81.1:5000/local/plugin-mobotix-scan -- -c \\'while true; do date; sleep 1; done\\' Note the -c \\'while true; do date; sleep 1; done\\' instead of your usual plugin arguments. Now if you do sudo pluginctl logs testnc you will see the logs i.e. date.\\n\\nAccess the Plugin Container: sudo pluginctl exec -ti testnc -- /bin/bash\\n\\nEdge Code Repository\\n\\nHow to get your plugin on ECR\\n\\nTo publish your Plugin on ECR, follow these steps: 1. Go to https://portal.sagecontinuum.org/apps/. 2. Click on \"Explore the Apps Portal\". 3. Click on \"My Apps\". You must be logged in to continue. 4. Click \"Create App\" and enter your Github Repo URL. 5. \\'Click \"Register and Build App\". 6. On Your app page click on the \"Tags\" tab to get the registry link when you need to run the job on the node either using pluginctl or job script. This will look like:docker pull registry.sagecontinuum.org/bhupendraraut/mobotix-move:1.23.3.2 7. Repeat the above process for updating the plugin.\\n\\n:::warning After the build process is complete, you need to make the plugin public to schedule it. :::\\n\\n:point_right: If you have skipped step 3. ECR Configs and Docs, do it before submitting it to the ECR. Ensure that your ecr-meta/ecr-science-description.md and sage.yaml files are properly configured for this process.\\n\\nVersioning your code\\n\\n:::danger You can not resubmit the plugin to ECR with the same version number again. ::: - So think about how you change it every time you resubmit to ERC and make your style of versioning. :thinking_face: - I use \\'vx.y.m.d\\' e.g. \\'v0.23.3.4\\' but then I can only have 1 version a day, so now I am thinking of adding an incremental integer to it.\\n\\nAfter ECR registry test (generally not required)\\n\\nGenerally successfully tested plugins just work. However, in case they are failing in the scheduled jobs after running for a while or after successfully running in the above tests, do the following.\\n\\nTo test a plugin on a node after it has been built on the ECR, follow these steps: sudo pluginctl run --name test-run registry.sagecontinuum.org/bhupendraraut/cloud-motion:1.23.01.24 -- -input top\\n\\nThis command will execute the plugin with the specified ECR image (version 1.23.01.24), passing the \"-input top\" argument to the plugin (Note -- after the image telling pluginctl that these arguments are for the plugin).\\n\\n:point_right: Note the use of sudo in all pluginctl and docker commands on the node.\\n\\nAssuming that the plugin has been installed correctly and the ECR image is available, running this command should test the \"test-motion\" plugin on the node.\\n\\nYou may also have to call the kubectl <POD> commands as in the testing section if this fails.\\n\\nScheduling the job\\n\\n:::warning :exclamation: If you get an error like registry does not exist in ECR, then check that your plugin is made public. :::\\n\\nFollow this link to get an understanding of how to submit a job\\n\\nHere are the parameters we set for the Mobotix sampler plugin,\\n\\nless= -name thermalimaging registry.sagecontinuum.org/bhupendraraut/mobotix-sampler:1.22.4.13 \\\\ --ip 10.31.81.14 \\\\ -u userid \\\\ -p password \\\\ --frames 1 \\\\ --timeout 5 \\\\ --loopsleep 60 - Your science rule can be a cronjob (More information can be found here - This runs every 15 minutes \"thermalimaging\": cronjob(\"thermalimaging\", \"*/15 * * * *\"). - Use Crontab Guru. - You can also make it triggered by a value. Please read this for supported functions.\\n\\nScheduling scripts\\n\\n:sparkles: Check user friendly job submission UI.\\n\\n:green_book: Check sesctl docs for command line tool.\\n\\n:point_up: Do not use _, upper case letters or . in the job name. Use only lowercase letters, numbers and -.\\n\\n:point_up: Ensure that the plugin is set to \\'public\\' in the Sage app portal.\\n\\njob.yaml example for USB device\\n\\nyaml= name: atmoswxt plugins: - name: waggle-wxt536 pluginSpec: image: registry.sagecontinuum.org/jrobrien/waggle-wxt536:0.23.4.13 privileged: true selector: zone: core nodeTags: [] nodes: W057: true W039: true scienceRules: - \\'schedule(\"waggle-wxt536\"): cronjob(\"waggle-wxt536\", \"1/10 * * * *\")\\' successCriteria: - WallClock(\\'1day\\')\\n\\nMultiple jobs example\\n\\nIf you want to run your plugins not all at the same time. Use this example.\\n\\n```yaml= name: w030-k3s-upgrade-test plugins: - name: object-counter-bottom pluginSpec: image: registry.sagecontinuum.org/yonghokim/object-counter:0.5.1 args: - -stream - bottom_camera - -all-objects selector: resource.gpu: \"true\" - name: cloud-cover-bottom pluginSpec: image: registry.sagecontinuum.org/seonghapark/cloud-cover:0.1.3 args: - -stream - bottom_camera selector: resource.gpu: \"true\" - name: surfacewater-classifier pluginSpec: image: registry.sagecontinuum.org/seonghapark/surface_water_classifier:0.0.1 args: - -stream - bottom_camera - -model - /app/model.pth - name: avian-diversity-monitoring pluginSpec: image: registry.sagecontinuum.org/dariodematties/avian-diversity-monitoring:0.2.5 args: - --num_rec - \"1\" - --silence_int - \"1\" - --sound_int - \"20\" - name: cloud-motion-v1 pluginSpec: image: registry.sagecontinuum.org/bhupendraraut/cloud-motion:1.23.02.20 args: - --input - bottom_camera - name: imagesampler-bottom pluginSpec: image: registry.sagecontinuum.org/theone/imagesampler:0.3.1 args: - -stream - bottom_camera - name: audio-sampler pluginSpec: image: registry.sagecontinuum.org/seanshahkarami/audio-sampler:0.4.1 nodeTags: [] nodes: W030: true scienceRules: - \\'schedule(object-counter-bottom): cronjob(\"object-counter-bottom\", \"/5 * * * \")\\' - \\'schedule(cloud-cover-bottom): cronjob(\"cloud-cover-bottom\", \"01-59/5 * * * \")\\' - \\'schedule(surfacewater-classifier): cronjob(\"surfacewater-classifier\", \"02-59/5 * * * \")\\' - \\'schedule(\"avian-diversity-monitoring\"): cronjob(\"avian-diversity-monitoring\", \" * * * \")\\' - \\'schedule(\"cloud-motion-v1\"): cronjob(\"cloud-motion-v1\", \"03-59/5 * * * \")\\' - \\'schedule(imagesampler-bottom): cronjob(\"imagesampler-bottom\", \"04-59/5 * * * \")\\' - \\'schedule(audio-sampler): cronjob(\"audio-sampler\", \"/5 * * * \")\\' successCriteria: - Walltime(1day)\\n\\n```\\n\\nhere objecct-counter runs at 0, 5, 10, etc cloud-cover: 1, 6, 11, etc. surface water: 2, 7, 12, etc. cloud-motion: 3, 8, 13, etc. image-sampl: 4, 9, 14, etc.\\n\\nDebugging failed jobs\\n\\nDo you know how to identify why a job is failing\\n\\n:sparkles: When the job failures are seen as red markers on your job page, you can click them to see the error.\\n\\nOr detail errors can be found using using sage_data_client\\n\\nRequirements: sage_data_client and utils.py\\n\\nBy specifying the plugin name and node, the following code will print out the reasons for job failure within the last 60 minutes.\\n\\n```python= from utils import *\\n\\nmynode = \"w030\"\\n\\nmyplugin = \"water\" df = fill_completion_failure(parse_events(get_data(mynode, start=\"-60m\"))) for _, p in df[df[\"plugin_name\"].str.contains(myplugin)].iterrows(): print(p[\"error_log\"]) ```\\n\\nDownloading the data\\n\\nSage docs for accessing-data\\n\\nSee Your Data on Sage Portal\\n\\nTo check your data on Sage Portal, follow these steps: 1. Click on the Data tab at the top of the portal page. 2. Select Data Query Browser from the dropdown menu. 3. Then, select your app in the filter. This will show all the data that is uploaded by your app using the plugin.publish() and plugin.upload() methods.\\n\\nIn addition, you can data visualize as a time series and select multiple variables to visualize together in a chart, which can be useful for identifying trends or patterns.\\n\\nDownload all images with wget\\n\\nVisit https://training-data.sagecontinuum.org/\\n\\nselect the node and period for data.\\n\\nSelect the required data and download the text file urls-xxxxxxx.txt with urls\\n\\nTo select only the top camera images, use the vim command: g/^\\\\(.*top\\\\)\\\\@!.*$/d. This will delete URLs that do not contain the word \\'top\\'\\n\\nCopy the following command from the website and run it in your terminal. wget -r -N -i urls-xxxxxxx.txt\\n\\nSage data client for text data\\n\\nSage data client python Notebook Example\\n\\npypi link pip install sage-data-client\\n\\n:::info :green_book: Documentation for accessing the data. :::\\n\\nQuerying data example\\n\\nThe sage_data_client provides query() function which takes the parameters:\\n\\n```python import sage_data_client import pandas as pd\\n\\ndf = sage_data_client.query( start=\"2023-01-08T00:00:09Z\", # Start time in \"YYYY-MM-DDTHH:MM:SSZ\" or \"YYYYMMDD-HH:MM:SS\" format end=\"2024-01-08T23:23:24Z\", # End time in the same format as start time filter={ \"plugin\": \".mobotix-scan.\", # Regex for filtering by plugin name \"vsn\": \"W056\", # Specific node identifier \"name\": \"upload\", # Specific data field \"filename\": \".*_position1.nc\" # Regex for filtering filenames } )\\n\\ndf.sort_values(\\'timestamp\\') df ```\\n\\nFilter Criteria\\n\\nstart and end: Time should be specified in UTC, using the format YYYY-MM-DDTHH:MM:SSZ or YYYYMMDD-HH:MM:SS.\\n\\nfilter: A dictionary for additional filtering criteria. Each key is a column name in the df.\\n\\nUse regular expressions (denoted as .*pattern.*) for flexible matching within text fields like plugin or filename.\\n\\nDownloading Files\\n\\nUse additional pandas operations on df to to include only the records of interest and download the files using a function like the one provided below, which gets the URLs in the value column, using authentication.\\n\\n```python import requests import os from requests.auth import HTTPBasicAuth\\n\\nuname = \\'username\\' upass = \\'token_as_password\\'\\n\\ndef download_files(df, download_path, uname, upass): # check download directory if not os.path.exists(download_path): os.makedirs(download_path)\\n\\nfor index, row in df.iterrows(): # \\'value\\' column has url url = row[\\'value\\']\\n\\n  filename = url.split(\\'/\\')[-1]\\n\\n  # Download using credentials\\n  response = requests.get(url, auth=HTTPBasicAuth(uname, upass))\\n  if response.status_code == 200:\\n     # make the downloads path\\n     file_path = os.path.join(download_path, filename)\\n     # Write a new file\\n     with open(file_path, \\'wb\\') as file:\\n     file.write(response.content)\\n     print(f\"Downloaded {filename} to {file_path}\")\\n  else:\\n     print(f\"Failed to download {url}, status code: {response.status_code}\")\\n\\nusage\\n\\ndownload_files(df, \\'/Users/bhupendra/projects/epcape_pier/data/downloaded/nc_pos1\\', uname, upass) ```\\n\\nMore data analysis resources\\n\\nSAGE Examples\\n\\nCROCUS Cookbooks\\n\\nMiscellaneous\\n\\nFind PT Mobotix thermal camera ip on the node\\n\\nLogin to the node where the PTmobotix camera is connected. 1. run nmap -sP 10.31.81.1/24\\n\\nNmap scan report for ws-nxcore-000048B02D3AF49F (10.31.81.1) Host is up (0.0012s latency). Nmap scan report for switch (10.31.81.2) Host is up (0.0058s latency). Nmap scan report for ws-rpi (10.31.81.4) Host is up (0.00081s latency). Nmap scan report for 10.31.81.10 Host is up (0.0010s latency). Nmap scan report for 10.31.81.15 Host is up (0.00092s latency). Nmap scan report for 10.31.81.17 Host is up (0.0014s latency). Nmap done: 256 IP addresses (6 hosts up) scanned in 2.42 seconds\\n\\nFrom the output run any command for each ip e.g. curl -u admin:meinsm -X POST http://10.31.81.15/control/rcontrol?action=putrs232&rs232outtext=%FF%01%00%0F%00%00%10\\n\\nThe ip for which output is OK is the Mobotix.\\n\\nSSH \\'Broken Pipe\\' Issue and Solution\\n\\nA \\'Broken pipe\\' occurs when the SSH session to waggle-dev-node is inactive for longer than 10/15 minutes, resulting in a closed connection.\\n\\nclient_loop: send disconnect: Broken pipe Connection to waggle-dev-node-w021 closed by remote host. Connection to waggle-dev-node-w021 closed.\\n\\nSolution\\n\\nTo prevent the SSH session from timing out and to maintain the connection, the following configuration options can be added to the SSH config file: ```ssh\\n\\nKeep the SSH connection alive by sending a message to the server every 60 seconds\\n\\nHost * TCPKeepAlive yes ServerAliveInterval 60 ServerAliveCountMax 999 ```'),\n",
       " Document(metadata={'source': 'sage-website/docs/installation-manuals/wsn-manual.md'}, page_content='Wild Sage Node manual\\n\\nThe Wild Sage Node \"Getting Started\" manual is a complete overview of getting started with your new WSN.\\n\\nDownload WSN manual'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='sidebar_label: Architecture sidebar_position: 2\\n\\nArchitecture\\n\\nThe cyberinfrastructure consists of coordinating hardware and software services enabling AI at the edge. Below is a quick summary of the different infrastructure pieces, starting at the highest-level and zooming into each component to understand the relationships and role each plays.\\n\\nHigh-Level Infrastructure\\n\\nThere are 2 main components of the cyberinfrastructure: - Nodes that exist at the edge - The cloud that hosts services and storage systems to facilitate running “science goals” @ the edge\\n\\nEvery edge node maintains connections to 2 core cloud components: one to a Beehive and one to a Beekeeper\\n\\nBeekeeper\\n\\nThe Beekeeper is an administrative server that allows system administrators to perform actions on the nodes such as gather health metrics and perform software updates. All nodes \"phone home\" to their Beekeeper and maintain this \"life-line\" connection.\\n\\nDetails & source code: https://github.com/waggle-sensor/beekeeper\\n\\nBeehive\\n\\nThe Node-to-Beehive connection is the pipeline for the science. It is over this connection that instructions for the node will be sent, in addition to how data is published into the Beehive storage systems from applications (plugins) running on the nodes.\\n\\nThe overall infrastructure supports multiple Beehives, where each node is associated with a single Beehive. The set of nodes associated with a Beehive creates a \"project\" where each \"project\" is separate, having its own data store, web services, etc.\\n\\nIn the example above, there are 2 nodes associated with Beehive 1, while a single node is associated with Beehive 2. With all nodes, in this example, being administered by a single Beekeeper.\\n\\nNote: the example above shows a single Beekeeper, but a second Beekeeper could have been used for administrative isolation.\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-beehive-v2\\n\\nBeehive Infrastructure\\n\\nLooking deeper into the Beehive infrastructure, it contains 2 main components: - software services such as the Edge Scheduler (ES), Lambda Triggers (LT), data APIs, and websites/portals - data storage systems such as the Data Repository (DR) and the Edge Code Repository (ECR)\\n\\nThe Beehive is the “command center” for interacting with the Waggle nodes at the edge. Hosting websites and interfaces allowing scientists to create science goals to run plugins at the edge & browse the data produced by those plugins.\\n\\nThe software services and data storage systems are deployed within a kubernetes environment to allow for easy administration and to support running in a multiple server architecture, supporting redundancy and service replication.\\n\\nWhile the services running within Beehive are many (both graphical and REST style API interfaces), the following is an outline of the most vital.\\n\\nData Repository (DR)\\n\\nThe Data Repository is the data store for housing all the edge produced plugin data. It consists of different storage technologies (i.e. influxdb) and techniques to store simple textual data (i.e. key-value pairs) in addition to large blobular data (i.e. audio, images, video). The Data Repository additionally has an API interface for easy access to this data.\\n\\nThe data store is a time-series database of key-value pairs with each entry containing metadata about how and when the data originated @ the edge. Included in this metadata is the data collection timestamp, plugin version used to collect the data, the node the plugin was run on, and the specific compute unit within the node that the plugin was running on.\\n\\njson { \"timestamp\":\"2022-06-10T22:37:47.369013647Z\", \"name\":\"iio.in_temp_input\", \"value\":25050, \"meta\":{ \"host\":\"0000dca632ed6d06.ws-rpi\", \"job\":\"sage\", \"node\":\"000048b02d35a97c\", \"plugin\":\"plugin-iio:0.6.0\", \"sensor\":\"bme680\", \"task\":\"iio-rpi\", \"vsn\":\"W08C\" } }\\n\\nIn the above example, the value of 25050 was collected @ 2022-06-10T22:37:47.369013647Z from the bme680 sensor on node 000048b02d35a97c via the plugin-iio:0.6.0 plugin.\\n\\nNote: see the Access and use data site for more details and data access examples.\\n\\nDetails & source code: https://github.com/waggle-sensor/data-repository\\n\\nEdge Scheduler (ES)\\n\\nThe Edge Scheduler is defined as the suite of services running in Beehive that facilitate running plugins @ the edge. Included here are user interfaces and APIs for scientists to create and manage their science goals. The Edge Scheduler continuously analyzes node workloads against all the science goals to determine how the science goals are deployed to the Beehive nodes. When it is determined that a node\\'s science goals are to be updated, the Edge Scheduler interfaces with WES running on those nodes to update the node\\'s local copy of the science goals. Essentially, the Edge Scheduler is the overseer of all the Beehive\\'s nodes, deploying science goals to them to meet the scientists plugin execution objectives.\\n\\nDetails & source code: https://github.com/waggle-sensor/edge-scheduler\\n\\nEdge Code Repository (ECR)\\n\\nThe Edge Code Repository is the \"app store\" that hosts all the tested and benchmarked edge plugins that can be deployed to the nodes. This is the interface allowing users to discover existing plugins (for potential inclusion in their science goals) in addition to submitting their own. At it\\'s core, the ECR provides a verified and versioned repository of plugin Docker images that are pulled by the nodes when a plugin is to be downloaded as run-time component of a science goal.\\n\\nDetails & source code: https://github.com/waggle-sensor/edge-code-repository\\n\\nLambda Triggers (LT)\\n\\nThe Lambda Triggers service provides a framework for running reactive code within the Beehive. There are two kinds of reaction triggers considered here: From-Edge and To-Edge.\\n\\nFrom-Edge triggers, or messages that originate from an edge node, can be used to trigger lambda functions -- for example, if high wind velocity is detected, a function could be triggered to determine how to reconfigure sensors or launch a computation or send an alert.\\n\\nTo-Edge triggers are messages that are to change a node\\'s behavior. For example an HPC calculation or cloud-based data analysis could trigger an Edge Scheduler API call to request a science goal to be run on a particular set of edge nodes.\\n\\nDetails & source code: https://github.com/waggle-sensor/lambda-triggers\\n\\nNodes\\n\\nNodes are the edge computing component of the cyberinfrastructure. All nodes consist of 3 items: 1. Persisent storage for housing downloaded plugins and caching published data before it is transferred to the node\\'s Beehive 2. CPU and GPU compute modules where plugins are executed and perform the accelerated inferences 3. Sensors such as environment sensors, cameras and LiDAR systems\\n\\nEdge nodes enable fast computation @ the edge, leveraging the large non-volatile storage to handle caching of high frequency data (including images, audio and video) in the event the node is \"offline\" from its Beehive. Through expansion ports the nodes support the adding and removing of sensors to fully customize the node deployments for the particular deployment environment.\\n\\nOverall, even though the nodes may use different CPU architectures and different sensor configurations, they all leverage the same Waggle Edge Stack (WES) to run plugins.\\n\\nWild Sage Node (Wild Waggle Node)\\n\\nThe Wild Sage Node (or Wild Waggle Node) is a custom built weather-proof enclosure intended for remote outdoor installation. The node features software and hardware resilience via a custom operating system and custom circuit board. Internal to the node is a power supply and PoE network switch supporting the addition of sensors through standard Ethernet (PoE), USB and other embedded protocols via the node expansion ports.\\n\\nThe technical capabilities of these nodes consists of: - NVidia Xavier NX ARM64 Node Controller w/ 8GB of shared CPU/GPU RAM - 1 TB of NVMe storage - 4x PoE expansion ports - 1x USB2 expansion port - optional Stevenson Shield housing a RPi 4 w/ environmental sensors & microphone - optional 2nd NVidia Xavier NX ARM64 Edge Processor\\n\\nNode installation manual: https://sagecontinuum.org/docs/installation-manuals/wsn-manual\\n\\nDetails & source code: https://github.com/waggle-sensor/wild-waggle-node\\n\\nBlade Nodes\\n\\nA Blade Node is a standard commercially available server intended for use in a climate controlled machine room, or extended temperature range telecom-grade blades for harsher environments. The AMD64 based operating system supports these types of nodes, enabling the services needed to support WES.\\n\\nThe above diagram shows the basic technical configuration of a Blade Node: - Multi-core ARM64 - 32GB of RAM - Dedicated NVida T4 GPU - 1 TB of SSD storage\\n\\nNote: it is possible to add the same optional Stevenson Shield housing that is available to the Wild Sage Nodes\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-blade\\n\\nRunning plugins @ the Edge\\n\\nIncluded in the Waggle operating systems are the core components necessary to enable running plugins @ the edge. At the heart of this is k3s, which creates a protected & isolated run-time environment. This environment combined with the tools and services provided by WES enable plugin access to the node\\'s CPU, GPU, sensors and cameras.\\n\\nWaggle Edge Stack (WES)\\n\\nThe Waggle Edge Stack is the set of core services running within the edge node\\'s k3s run-time environment that supports all the features that plugins need to run on the Waggle nodes. The WES services coordinate with the core Beehive services to download & run scheduled plugins (including load balancing) and facilitate uploading plugin published data to the Beehive data repository. Through abstraction technologies and WES provided tools, plugins have access to sensor and camera data.\\n\\nThe above diagram demonstrates 2 plugins running on a Waggle node. Plugin 1 (\"neon-kafka\") is an example plugin that is running alongside Plugin 2 (\"data-smooth\"). In this example, \"neon-kafka\" (via the WES tools) is reading metrics from the node\\'s sensors and then publishing that data within the WES run-time environment (internal to the node). At the same time, the \"data-smooth\" plugin is subscribing to this data stream, performing some sort of inference and then publishing the inference results (via WES tools) to Beehive.\\n\\nNote: see the Edge apps guide on how to create a Waggle plugin.\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-edge-stack\\n\\nWhat is a plugin?\\n\\nPlugins are the user-developed modules that the cyberinfrastructure is designed around. At it\\'s simplest definition a \"plugin\" is code that runs @ the edge to perform some task. That task may be simply collecting sample camera images or a complex inference combining sensor data and results published from other plugins. A plugin\\'s code will interface with the edge node\\'s sensor(s) and then publish resulting data via the tools provided by WES. All developed plugins are hosted by the Beehive Edge Code Repository.\\n\\nSee how to create plugins for details.\\n\\nScience Goals\\n\\nA \"science goal\" is a rule-set for how and when plugins are run on edge nodes. These science goals are created by scientist to accomplish a science objective through the execution of plugins in a specific manner. Goals are created, in a human language, and managed within the Beehive Edge Scheduler. It is then the cyberinfrastucture responsibility to deploy the science goals to the edge nodes and execute the goal\\'s plugins. The tutorial walks through running a science goal.\\n\\nLoRaWAN\\n\\nThe Waggle Edge Stack includes the ChirpStack software stack and other services to facilitate communication between Nodes and LoRaWAN devices. This empowers Nodes to effortlessly establish connections with wireless sensors, enabling your plugins to seamlessly access and harness valuable data from these sensors.\\n\\nTo get started using LoRaWAN, head over to the Contact Us page. A tutorial will be available soon showing you how to get started with LoRaWAN.\\n\\nThe above diagram demonstrates the hardware in Nodes and services in WES that enable Nodes to use LoRaWAN and publish the measurements to a Beehive. The following sections will explain each componenent and service.\\n\\nsource code: - wes-chirpstack - wes-chirpstack-server - wes-rabbitmq - Tracker - Lorawan Listener Plugin\\n\\nWhat is LoRaWAN?\\n\\nLoRaWAN, short for \"Long Range Wide Area Network,\" is a wireless communication protocol designed for low-power, long-range communication between IoT (Internet of Things) devices. It employs a low-power wide-area network (LPWAN) technology, making it ideal for connecting remote sensors and devices. For more information view the documentation here.\\n\\nChirpstack\\n\\nChirpStack is a robust and open-source LoRaWAN Network Server that enables efficient management of LoRaWAN devices, gateways, and data. Its architecture consists of several crucial components, each serving a distinct role in LoRaWAN network operations. Below, we provide a brief overview of these components along with links to ChirpStack documentation for further insights.\\n\\nChirpstack documentation\\n\\nUDP Packet Forwarder\\n\\nThe UDP Packet Forwarder is an essential component that acts as a bridge between LoRa gateways and the ChirpStack Network Server. It receives incoming packets from LoRa gateways and forwards them to the ChirpStack Gateway Bridge for further processing. To learn more about the UDP Packet Forwarder, refer to the documentation here.\\n\\nChirpStack Gateway Bridge\\n\\nThe ChirpStack Gateway Bridge is responsible for translating gateway-specific protocols into a standard format for the ChirpStack Network Server. It connects to a UDP Packet Forwader, ensuring that data is properly formatted and can be seamlessly processed by the network server. For in-depth information on the ChirpStack Gateway Bridge, explore the documentation here.\\n\\nMQTT Broker\\n\\nWES includes a MQTT (Message Queuing Telemetry Transport) broker to handle communication between various services. MQTT provides a lightweight and efficient messaging system. This service ensures that data flows smoothly between the network server, gateways, and applications. You can find detailed information about the MQTT broker integration in the ChirpStack documentation here.\\n\\nChirpStack Server\\n\\nThe ChirpStack Server serves as the core component, managing device sessions, data, and application integrations. It utilizes Redis for device sessions, metrics, and caching, ensuring efficient data handling and retrieval. For persistent data storage, ChirpStack uses PostgreSQL, accommodating records for tenants, applications, devices, and more. For a comprehensive understanding of the ChirpStack Server and its associated database technologies, consult the ChirpStack documentation here.\\n\\nNOTE: Chirpstack v4 combined the application and network server into one component.\\n\\nTracker\\n\\nThe Tracker is a service designed to record the connectivity of LoRaWAN devices to the Nodes. This service uses the information received from the MQTT broker to call ChirpStack\\'s gRPC API. The information received from the API is then used to keep the Node\\'s manifest up-to-date. Subsequently, it forwards this updated manifest to the Beehive. For more information, view the documentation here.\\n\\nLorawan Listener Plugin\\n\\nThe LoRaWAN Listener is a plugin designed to publish measurements collected from LoRaWAN devices. It simplifies the process of extracting and publishing valuable data from these devices. For more information about the plugin view the plugin page here.\\n\\nLorawan Device Compatibility\\n\\nThe Wild Sage Node is designed to support a wide range of Lorawan devices, ensuring flexibility and adaptability for various applications. If you are wondering which Lorawan devices can be connected to a Wild Sage Node, the device must have the following tech specs:\\n\\ndesigned for US915 (902–928 MHz) frequency region.\\n\\ncompatible with Lorawan Mac versions 1.0.0 - 1.1.0\\n\\ncompatible with Chirpstack\\'s Lorawan Network Server\\n\\nThe device supports Over-The-Air Activation (OTAA) or Activation By Personalization (ABP)\\n\\nThe device has a Lorawan device class of A, B, or C\\n\\nIt is important to note that all channels within the US915 frequency band are enabled in a Wild Sage Node. If you wish to learn more about our Lorawan Gateway, please visit our portal. For inquiries about supporting Lorawan regions other than US915, please Contact Us.\\n\\nDevice Examples\\n\\nWhether you are designing your own Lorawan sensor, looking for a Lorawan data logger, or seeking an off-the-shelf Lorawan device the Wild Sage Node will support it, we have examples for you:\\n\\nDesigning your own Lorawan sensor?\\n\\nArduino MKR WAN 1310\\n\\nLooking for a Lorawan data logger?\\n\\nICT International MFR Node\\n\\nLooking for an off-the-shelf Lorawan device?\\n\\nICT International SFM1X Sap Flow Meter\\n\\nSeeking Lorawan device manufacturers?\\n\\nICT International\\n\\nRAKwireless\\n\\nThe Things Network Device Marketplace\\n\\nDecentLab'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='sidebar_label: Overview sidebar_position: 1\\n\\nSage: A distributed software-defined sensor network.\\n\\nWhat is Sage?\\n\\nGeographically distributed sensor systems that include cameras, microphones, and weather and air quality stations can generate such large volumes of data that fast and efficient analysis is best performed by an embedded computer connected directly to the sensor. Sage is exploring new techniques for applying machine learning algorithms to data from such intelligent sensors and then building reusable software that can run programs within the embedded computer and transmit the results over the network to central computer servers. Distributed, intelligent sensor networks that can collect and analyze data are an essential tool for scientists seeking to understand the impacts of global urbanization, natural disasters such as flooding and wildfires, and climate change on natural ecosystems and city infrastructure.\\n\\nSage is deploying sensor nodes that support machine learning frameworks in environmental testbeds in California, Colorado, and Kansas and in urban environments in Illinois and Texas. The reusable cyberinfrastructure running on these testbeds will give climate, traffic, and ecosystem scientists new data for building models to study these coupled systems. The software components developed are open source and provide an open architecture to enable scientists from a wide range of fields to build their own intelligent sensor networks.\\n\\nPartners are deploying testbeds in Australia, Japan, UK, and Taiwan, providing scientists with even more data for analysis. The toolkit is also extending the current educational curriculum used in Chicago to inspire young people – with an emphasis on women and minorities, to pursue science, technology, and mathematics careers – by providing a platform for students to explore measurement-based science questions related to the natural and built environments.\\n\\nThe data from sensors and applications is hosted in the cloud to facilitate easy data analysis.\\n\\nWho are the users?\\n\\nThe most common users have included:\\n\\nDomain scientists interested in developing edge AI applications.\\n\\nUsers interested in sensor and application-produced datasets.\\n\\nCyberinfrastructure researchers interested in platform research.\\n\\nDomain scientists interested in adding new sensors and deploying nodes to answer specific science questions.\\n\\nHow do I use the platform?\\n\\nThis depends on your desired interaction interest. The platform consists of edge compute applications which process data (ex. sensor readings, camera images, audio recordings, etc). These edge applications then produce their own data (ex. inferences) and upload the results to a cloud database. This cloud database can be accessed directly and/or additional compute can be performed on the cloud data.\\n\\nThe entry-point into learning about your interaction with the system might be best directed by getting answers (by following the links) to the question(s) you are most interested in.\\n\\nHow do I access sensors? - Want to learn about existing, supported sensors? - Do you have a new sensor that you want to write an edge application for?\\n\\nHow do I run edge apps? - Want to know how to create an edge app? - Want to know how your edge app can get access to edge sensor data? - Want to share your edge app data with other edge applications? - Want to know how to upload data to the cloud?\\n\\nHow do I access and use data? - Want to learn about how data is stored/organized? - Do you have data that is up in the cloud and want to know how to access it?\\n\\nHow do I compute in the cloud? - Want to know how to autonomously react to edge produced data? - Want to know how to trigger an HPC event? - Want to get a text message when your edge application does something cool?\\n\\nHow do I build my own device? - Want to set up your own device for local edge app development? - Want to teach AI to a classroom of students?\\n\\nHow is the cyberinfrastructure architected?\\n\\nIf you are interested in learning more about how the cyberinfrastructure works you can head on over to the Architecture Overview page.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='sidebar_position: 3\\n\\nSubmit your job\\n\\nAre you ready to deploy your plugins to measure the world? We will use edge scheduler to submit a job and demonstrate how you can deploy plugins to field-deployed Waggle nodes.\\n\\n:::caution If you have not created your account, please go to https://portal.sagecontinuum.org and sign in to create a new account with your email. Once signed in, you will be able to create and edit your jobs, but will need a permission to submit jobs to the scheduler. Please contact-us to request for the job submission permission. :::\\n\\nJobs are an instance of a science goal. They detail what needs to be accomplished on Waggle nodes. A science goal may have multiple jobs to fill the missing data to answer scientific questions of the goal. A job describes, - plugins that are registered and built in edge code repository with specification including any plugin arguments, - a list of Waggle nodes on which the plugins will be scheduled and run, - science rules describing a condition-action set that includes when the plugins should be scheduled, - conditions to determine when the job is considered as completed\\n\\nCreating and submitting jobs are an important step for successful science mission using Waggle nodes.\\n\\nCreate a job\\n\\nWe create a job file in YAML format (JSON format is also supported. Please check out details of job attributes.)\\n\\n```bash cat << EOF > myjob.yaml\\n\\nname: myjob plugins: - name: image-sampler pluginSpec: image: registry.sagecontinuum.org/theone/imagesampler:0.3.0 args: - -stream - bottom_camera nodes: W023: scienceRules: - \"schedule(image-sampler): cronjob(\\'image-sampler\\', \\' * * * \\')\" successcriteria: - WallClock(1d) EOF ```\\n\\nIn this example, we want to schedule a plugin named image-sampler to collect an image from the camera named bottom_camera on W023 node. As a result of the job execution, we will get images from the node\\'s camera. The job also specifies that the plugin needs to be scheduled every minute (i.e., * * * * * in crontab expression). The job completes 24 hours after the job started to run on the node.\\n\\n:::info We support human-friendly names for the sensors we host. The \"bottom_camear\" is named based on the orientation the camera is attached to the node. The full list of sensors including cameras for the W023 node can be found here :::\\n\\n:::note We currently do not check job\\'s success criteria. This means that once a job is submitted it is served forever. We will update our system to support different conditions for the success criteria attribute. :::\\n\\nUpload your job to the scheduler\\n\\nsesctl is a command-line tool to manage jobs in the scheduler. You can download the latest version from our Github repository. Please make sure you download the tool supported for your machine. For example, on Linux desktop or laptop you would download linux-amd64 version of the tool. Please see the sesctl document for more details.\\n\\n:::note Once you have contacted us for access permissions, you will need a token provided from the access page. Replace the <<user token>> below with the access token provided on this page. :::\\n\\nYou can set the SES host and user token as an environmental variable to your terminal. Please follow your shell\\'s guidance to set them properly. In Bash shell, bash export SES_HOST=https://es.sagecontinuum.org export SES_USER_TOKEN=<<user token>>\\n\\nLet\\'s ping the scheduler in the cloud, bash sesctl ping\\n\\nYou will get a response \"pong\" from the scheduler, { \"id\": \"Cloud Scheduler (cloudscheduler-sage)\", \"version\": \"0.18.0\" }\\n\\nTo create a job using the job file, bash sesctl create --file-path myjob.yaml\\n\\nThe scheduler will return a job id and the state for the job creation, bash { \"job_id\": \"56\", \"job_name\": \"myjob\", \"state\": \"Created\" }\\n\\nTo verify that we have uploaded the job, bash sesctl stat\\n\\nYou will see the job entry from the response of the command, bash JOB_ID NAME USER STATUS AGE ==================================================================== ... 56 myjob theone Created - ...\\n\\nSubmit the job\\n\\nTo submit the job,\\n\\nbash sesctl submit --job-id 56\\n\\nThe response should indicate that the job state is changed to \"Submitted\", bash { \"job_id\": \"56\", \"state\": \"Submitted\" }\\n\\n:::note You may receive a list of errors from the scheduler if the job fails to be validated. For instance, your account may not have scheduling permission on the node W023. Please consult with us for any error, especially errors related to scheduling permission on nodes in the job. :::\\n\\nCheck status of jobs\\n\\nWe check status of the job we submitted, bash sesctl stat --job-id 56\\n\\nThe tool will print details of the job, ```bash ===== JOB STATUS ===== Job ID: 56 Job Name: myjob Job Owner: Job Status: Submitted Job Starttime: 2022-10-10 02:21:37.373437 +0000 UTC\\n\\n===== SCHEDULING DETAILS ===== Science Goal ID: 45afe963-5b8b-4e15-654c-54e2946f2ddb Total number of nodes 1 ```\\n\\nThe job status can be also shown in job status page.\\n\\nAccess to data\\n\\nA few minutes later, the W023 Waggle node would start collecting images by scheduling the plugin on the node. Collected images are transferred to Beehive for users to download.\\n\\nconsole curl -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-5m\", \"filter\": { \"task\": \"image-sampler\", \"vsn\": \"W023\", \"name\": \"upload\" } } \\'\\n\\nClean it up\\n\\nAs we approach to the end of this tutorial, we need to clean up the job because otherwise it will be served forever. To remove the job from the scheduler, ```bash\\n\\nsince the job is running, we remove the job forcefully\\n\\nsesctl rm --force 56 ```\\n\\nYou should see output that looks like, bash { \"job_id\": \"56\", \"state\": \"Removed\" }\\n\\nMore tutorials using sesctl\\n\\nMore tutorials can be found in our Github repository.\\n\\nCreating job description with advanced science rules for supporting realistic science mission\\n\\nThe science rule used in the tutorial asked the scheduler to schedule the image sampler plugin every minute. For collecting training images from a set of Waggle nodes this makes total sense with the science rule. However, users in Waggle should want more complex behaviors at the node to not only schedule plugins, but enable cloud computation triggered by sending local events to the cloud. The events and triggers can be captured by creating science rules that monitor local sensor measurement on nodes. Please visit the science rules to know more complex science rules that user can create.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-waggle.md'}, page_content='sidebar_position: 7\\n\\nBuilding your own Waggle device\\n\\nAre you a professor that wants to use affordable Waggle devices to teach students interested in AI? Are you someone interested in developing a new edge app using a local development platform? Are you a Waggle user interested in using a new sensor (i.e. a new camera, a bat signal detector, a custom sensor they built)? If you would like to build, design and deploy software that could answer your questions above, then Waggle is the right choice for you.\\n\\nThis tutorial will guide you in preparing your own Waggle device and (optionally) registering it to upload data to a shared development Beehive. This Waggle device is a fully unlocked development platform running the same WES infrastructure that runs in production Waggle edge devices (ex. the Wild Waggle Node). This is an ideal platform for users interested in developing a new edge app and/or experimenting with a new sensor.\\n\\nGetting Started\\n\\nTo get started in boot-strapping your Waggle Edge Computing kit you can follow the instructions for the various supported platforms on the node-platforms GitHub page.\\n\\nWe currently support a limited set of hardware platform because making edge devices into Waggle requires some hardware specific instructions. Check out the platforms we support as of now. More platforms will be added in the future. However, if you would like to add support for other platforms go ahead and submit a pull request to node-platforms.\\n\\nRegistering your Waggle device\\n\\nDuring the bootstrapping process you will have the option to register your device within the web portal here. It is highly recommended to register your device, as this enables all the core WES tools to be automatically downloaded, enabling the edge app development and run-time environment. Additionally, this enables your edge apps to publish data to the development Beehive, accessible to cloud-based analysis tools and workflow frameworks.\\n\\nTo register your device, use the dev devices form. Enter your device ID (which you will obtain through the hardware boot-strapping process) then click \"Get Keys\" button. A \"registration zip\" file will be generated and available for download. Then follow the instructions for your device to load the registration keys.\\n\\nYou may register as many times as you want. But note that each registration key has a short expiration time and should be used shortly after generation.\\n\\nNow you are ready to develop your edge apps and/or introduce new sensors to the Waggle platform. Head over to the overview to find the instructions you need for development.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='sidebar_position: 4\\n\\nAccess and use data\\n\\nRaw sensor data is collected by edge code. This edge code can either talk to sensor hardware directly or may obtain data from an abstraction layer (not show in image above). Edge code may forward unprocessed sensor data, do light processing to convert raw sensor values into final data products, or may use CPU/GPU-intensive workloads (e.g. AI application) to extract information from data-intensive sensors such as cameras, microphone or LIDAR.\\n\\nSensor data from nodes that comes in numerical or textual form (e.g. temperature) is stored natively in our time series database. Sensor data in form of large files (images, audio, movies..) is stored in the Waggle object store, but is referenced in the time series data (thus the dashed arrow in the figure above). Thus, the primary way to find all data (sensor and large files) is via the Waggle sensor query API described below.\\n\\nCurrently the Waggle sensor database contains data such as:\\n\\nRelative humidity, barometric pressure, ambient temperature and gas (VOC) BME680.\\n\\nRainfall measurements (Hydreon RG-15).\\n\\nAI-based cloud coverage estimation from camera images.\\n\\nAI-based object counts from camera images.\\n\\nSystem data such as uptime, cpu and memory.\\n\\nData can be accessed in realtime via our data API or in bulk via data bundles.\\n\\nData API\\n\\nWaggle provides a data API for immediate and flexible access to sensor data via search over time and metadata tags. It is primarily intended to support exploratory and near real time use cases.\\n\\nDue to the wide variety of possible queries, we do not attempt to provide DOIs for results from the data API. Instead, we leave it up to users to organize and curate datasets for their own applications. Long term, curated data is instead provided via data bundles.\\n\\nThere are two recommended approaches to working with the Data API:\\n\\nUsing the Python Sage Data Client.\\n\\nUsing the HTTP API.\\n\\nEach is appropriate for different use cases and integrations, but generally the following rule applies:\\n\\nIf you just want to get data into a Pandas dataframe for analysis and plotting, use the sage-data-client, otherwise use the HTTP API.\\n\\nUsing Sage data client\\n\\nThe Sage data client is a Python library which streamlines querying the data API and getting the results into a Pandas dataframe. For details on installation and usage, please see the Python package.\\n\\nUsing HTTP API\\n\\nThis example shows how to retrieve data the latest data from a specific sensor (you can adjust the start field if you do not get any recent data):\\n\\n```console curl -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-10s\", \"filter\": { \"sensor\": \"bme680\" } } \\'\\n\\nExample results:json {\"timestamp\":\"2021-08-09T19:26:03.880781217Z\",\"name\":\"iio.in_humidityrelative_input\",\"value\":70.905,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.878659392Z\",\"name\":\"iio.in_pressure_input\",\"value\":975.78,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.872652127Z\",\"name\":\"iio.in_resistance_input\",\"value\":93952,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.874998057Z\",\"name\":\"iio.in_temp_input\",\"value\":27330,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} ```\\n\\n:::tip More details of using the data API and the data model can be found here and here. :::\\n\\nData bundles\\n\\nData bundles provide sensor data and associated metadata in a single, large, downloadable file. Soon, each Data Bundle available for download will have a DOI that can be used for publication citations.\\n\\nData Bundles are compiled nightly and may be downloaded in this archive.\\n\\nAccessing file uploads\\n\\nUser applications can upload files for AI training purposes. These files stored in an S3 bucket hosted by the Open Storage Network.\\n\\nTo find these files use the filter \"name\":\"upload\" and specify additional filters to limit search results, for example:\\n\\nconsole curl -s -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\'{ \"start\": \"2021-09-10T12:51:36.246454082Z\", \"end\":\"2021-09-10T13:51:36.246454082Z\", \"filter\": { \"name\":\"upload\", \"plugin\":\"imagesampler-left:0.2.3\" } }\\'\\n\\nOutput: json {\"timestamp\":\"2021-09-10T13:19:27.237651354Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d05a0a4/1631279967237651354-2021-09-10T13:19:26+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d05a0a4\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T13:50:32.29028603Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bc3d/1631281832290286030-2021-09-10T13:50:32+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bc3d\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T12:52:59.782262376Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdc2/1631278379782262376-2021-09-10T12:52:59+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T13:49:49.084350086Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdd2/1631281789084350086-2021-09-10T13:49:48+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bdd2\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}}\\n\\nFor a quick way to only extract the urls from the json objects above, a tool like jq can be used:\\n\\nconsole curl -s -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\'{ \"start\": \"2021-09-10T12:51:36.246454082Z\", \"end\":\"2021-09-10T13:51:36.246454082Z\", \"filter\": { \"name\":\"upload\", \"plugin\":\"imagesampler-left:0.2.3\" } }\\' | jq -r .value > urls.txt\\n\\nThe resulting file urls.txt will look like this: text https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d05a0a4/1631279967237651354-2021-09-10T13:19:26+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bc3d/1631281832290286030-2021-09-10T13:50:32+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdc2/1631278379782262376-2021-09-10T12:52:59+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdd2/1631281789084350086-2021-09-10T13:49:48+0000.jpg\\n\\nTo download the files: console wget -i urls.txt\\n\\nIf many files are downloaded, it is better to preserve the directory tree structure to prevent filename collision: console wget -r -i urls.txt\\n\\nProtected data\\n\\nWhile most Waggle data is open and public - some types of data, such as raw images and audio from sensitive locations, may require additional steps:\\n\\nYou will need a Sage account.\\n\\nYou will need to sign our Data Use Agreement for access.\\n\\nYou will need to provide authentication to tools you are using to download files. (ex. wget, curl)\\n\\nAttempting to download protected files without meeting these criteria will yield a 401 Unauthorized response.\\n\\nIf you\\'ve identified protected data you are interested in, please contact us so we can help get you access.\\n\\nIn the case of protected files, you\\'ll need to provide authentication to your tool of choice. These will be your portal username and access token which can be found in the Access Credentials section of the site.\\n\\nThese can be provided to tools like wget and curl as follows:\\n\\n```console\\n\\nexample using wget\\n\\nwget --user=\\n\\nexample using curl\\n\\ncurl -u'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-an-account.md'}, page_content='sidebar_label: Create an account sidebar_position: 1\\n\\nOverview\\n\\nWhile some Sage features are open for public use, you\\'ll need an approved account to perform tasks such as:\\n\\nGet access to protected data.\\n\\nPublish apps to the ECR.\\n\\nSchedule app on nodes.\\n\\nIn this document, we\\'ll walk though creating an account.\\n\\nCreating an account\\n\\nClick on the Portal button in the upper right corner.\\n\\nClick on the Sign In button in the upper right corner.\\n\\nThis will take you to the Globus login page where you\\'ll need to provide your organization credentials. If you do not see your organization, please see the \"Didn\\'t find your organization?\" note at the bottom of the Globus login page.\\n\\nFinally, if this is your first time signing in, you\\'ll need to choose a username which will complete your account creation.\\n\\nAt this point, our team will need to review and approve your account before you\\'ll have permission to perform certain tasks. If you your account is not approved within 72 hours or you have special requirements, please Contact us so that we can help perform any account configuration.\\n\\nNext steps\\n\\nOnce your account is approved, you will have scheduling access and protected data browsing in the portal for nodes we\\'ve assigned to your account.\\n\\nFor CLI tools and SSH access to nodes, please go to Portal → Your Account → Access Creds and follow the Update SSH Public Keys and Finish Setup for Node Access instructions.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content=\"sidebar_position: 5\\n\\nAccess Waggle sensors\\n\\nA Waggle sensor is an entity that produces measurements of a phenomenon and that helps users analyze what is happening in the environment. There are sensors already hosted by Waggle and also sensors that are being integrated into Waggle as a user-hosted sensor. A sensor does not necessarily mean a physical device, but can be a program producing measurements from data -- we call it software-defined sensor. Once those sensors become available in Waggle nodes edge applications running inside the nodes can pull measurements from the sensors to process them.\\n\\nIn general, Waggle sensors are desinged to be accessible from any edge applications running on the Waggle node that hosts the sensors, but can be limited their access to groups and personnel. For example, a pan-tilt-zoom featured camera may be only accessed from authorized applications in order to prevent other applications from operating the camera. Ideally, Waggle sensors can form and support the Waggle ecosystem where sensor measurements are integrated and used by edge applications for higher level computation and complex decision making.\\n\\nWaggle physical sensors\\n\\nThe Waggle node is designed to accommodate sensors commonly used to support environmental science, but not limited to host other sensors. The currently supported sensors are,\\n\\nNOTE: not all Waggle nodes have the same set of sensors, and the sensor configuration depends on what to capture from the environment where the node is deployed\\n\\nBME680 temperature, humidity, pressure, and gas preview RG-15 rainfall preview ETS ML1-WS 20-16 kHz microphone recording sound XNV-8080R 5 MP camera with 92.1 degree horizontal and 67.2 degree vertical angle view XNV-8082R 6 MP camera with 114 degree horizontal and 62 degree vertical angle view XNF-8010RV 6 MP fisheye camera with 192 degree horizontal and vertical angle view XNV-8081Z 5 MP digital pan-tilt-rotate-zoom camera\\n\\nAny collaborators and user communities can bring up their sensors to Waggle node. The node can easily host sensor devices that support serial interface as well as network interface (e.g., http, rtsp, etc). Other currently supported user sensors include:\\n\\nSoftware-defined Radio: detecting raindrops and snow flakes\\n\\nRadiation detector: radiation detector\\n\\nLIDAR: distance of nearby objects\\n\\nMobotix: infrared camera\\n\\n[view more...]\\n\\nWaggle software-defined sensors\\n\\nSoftware-defined sensors are limitless as edge applications define them. You can start building your edge application that publishes outputs using PyWaggle's basic example that can become a software-defined sensor. Later, such outputs can be consumed by other edge applications to produce higher level information about the measurements. A few example of Waggle software-defined sensors are,\\n\\nObject Counter: env.count.OBJECT counts objects from an image, where OBJECT is the object name that is recognized\\n\\nCloud Coverage Estimator: env.coverage.cloud provides a percentage of cloud covered in an image\\n\\nAccess to Waggle sensors\\n\\nWaggle sensors are integrated into Waggle using the PyWaggle library. PyWaggle utilizes AMQP, the message publishing and subscribing mechanism, to support exchanging sensor measurements between device plugins and edge applications. An edge application can subscribe and process those measurements using PyWaggle's subscriber. The application then produces its output and publishes it as a measurement back to the system using PyWaggle publisher.\\n\\nPyWaggle often provides edge applications direct access to physical sensors. For sensors that support realtime protocols like RTSP and RTP and others, PyWaggle exposes those protocols to edge applications, and it is up to the applications to process data using given protocol. For example, RTSP protocol can be handled by OpenCV's VideoCapture class inside an application. If any physical sensor device that requires a special interfacing to the device, an edge application that supports the interfacing need to run in order to publish sensor measurements to the system, and later those measurements are used by other edge applications.\\n\\nExample: sampling images from camera\\n\\nIt is often important to sample images from cameras in the field to create initial dataset for a machine learning algorithm. The example describes how to access to a video stream from a camera sensor using PyWaggle.\\n\\nBring your own sensor to Waggle\\n\\nUsers may need to develop their own device plugin to expose the sensor to the system, or to publish measurement data from the sensor to the cloud. Unlike an edge application or software-defined sensors, device plugins communicating with a physical sensor may need special access, e.g. serial port, in order to talk to the sensor attached to Waggle node. Such device plugin may need to be verified by the Waggle team. Visit the Building your own Waggle device page for the guide to set up your Waggle device.\\n\\nTo integrate your sensor device into Waggle, head over to the Contact Us page\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/cloud-compute.md'}, page_content='sidebar_position: 6\\n\\nCloud compute & HPC on edge data\\n\\nWaggle provides a number of interfaces which other computing and HPC systems can build on top of. In this section, we explore some of the most common applications of Waggle.\\n\\nTriggering on data from the edge\\n\\nA common application is monitoring data from the edge and triggering actions when values exceed a threshold or an unusual event is detected.\\n\\nAs a getting started example, we demonstrate an outline of how this can be done in Waggle using the Sage data client.\\n\\n```python import sage_data_client import time\\n\\nwhile True: # query pressure data in recent 10 minute window df = sage_data_client.query( start=\"-10m\", filter={ \"name\": \"env.pressure\", \"sensor\": \"bme680\", } )\\n\\n# compute stddev for nodes\\' pressure data in window\\nstd = df.groupby(\"meta.vsn\").value.std()\\n\\n# find all pressure events exceeding an example threshold\\nevents = std[std > 8.0]\\n\\n# \"post\" vsn to alert system\\nfor vsn in events.index:\\n    print(f\"post {vsn} to alert system\")\\n\\ntime.sleep(60)\\n\\n```\\n\\nThe above code queries a 10 minute window of atmospheric pressure data every minute and \"posts\" alerts for nodes exceeding a predefined standard deviation threshold.\\n\\nThis example and more can be found in the Sage data client examples directory.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/4-publishing-to-ecr.md'}, page_content='sidebar_position: 4\\n\\nPart 4: Publishing to ECR\\n\\nNow that we\\'ve finished preparing our code and testing it, we\\'re almost ready to publish it to the Edge Code Repository!\\n\\nPreparing our app\\n\\nBefore publishing an app to the Edge Code Repository, we need to add a few packaging items to it.\\n\\nFirst, update the homepage in your sage.yaml to point to your app-tutorial Github repo and verify that it matches the following:\\n\\nyaml name: \"app-tutorial\" version: \"0.1.0\" description: \"My really amazing app!\" keywords: \"\" authors: \"Your name\" collaborators: \"\" funding: \"\" license: \"\" homepage: \"https://github.com/username/app-tutorial\" source: architectures: - \"linux/amd64\" - \"linux/arm64\"\\n\\nNext, create an ecr-meta directory in your repo and populate it with the following text and media:\\n\\necr-science-description.md - Markdown with in depth description of the science being done here (1 page of text).\\n\\necr-icon.jpg - An icon for the project/work 512x512px.\\n\\necr-science-image.jpg - A science image for the project with a minimum size of 1920x1080px.\\n\\nOnce we\\'ve commited and pushed those files to your repo, we\\'re ready to publish our app!\\n\\nPublishing our app\\n\\nPlease visit the Edge Code Repository and complete the following steps:\\n\\nGo to \"Sign In\" and follow the instructions.\\n\\nGo to \"My Apps\".\\n\\nGo to \"Create app\" and follow the instructions.\\n\\nIf everything is successful, your plugin will appeared and be marked as \"Built\".\\n\\nConclusion\\n\\nCongratulation! You\\'ve successfully written, tested and published an app to ECR!\\n\\nWe encourage you to check out other apps in the ECR and explore additional functionality provided by pywaggle.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/1-intro-to-edge-apps.md'}, page_content=\"sidebar_position: 1\\n\\nPart 1: Intro to edge apps\\n\\nWhat are edge apps?\\n\\nEdge apps are programs which read data (ex. sensors, audio, video), process it and then publish information derived from that data.\\n\\nA basic example of an app is one which reads and publishes a value from a sensor every minute. A more complex example could publish the number of birds in a scene using a deep learning model.\\n\\nEdge apps are composed of code, dependencies and models which are packaged so they can be scheduled on Waggle nodes. At a high level, the typical app lifecycle is:\\n\\nExploring existing edge apps\\n\\nOne of the major goals of Waggle is to provide the science community with a diverse catalog of edge apps to enable the sharing of new research. This catalog is maintained as part of the Edge Code Repository where you can find more background information and links to their source repos.\\n\\nWe encourage users to explore the ECR to get familiar with existing apps as well a references if you develop your own edge app.\\n\\nNext steps\\n\\nIf this sounds exciting and you'd like to write you own edge app, please continue to part 2!\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='sidebar_position: 2\\n\\nPart 2: Creating an edge app\\n\\nIn part 1, we showed an overview of what edge apps are and how they fit into the Waggle ecosystem. Now, we\\'ll dive right in and start writing our very own edge app!\\n\\nPrerequisites\\n\\nFor this part of the tutorial, we\\'ll assume you are developing directly on a laptop or machine with a camera or webcam available. You should have some basic development experience in Python and with git for version control.\\n\\nDevelopment workflow\\n\\nIn the next few parts of this tutorial, we\\'ll deep dive into the following app development workflow:\\n\\nFirst, data and model selection is where you scope the problem and identify a new or existing model for your application. This typically happens outside of our ecosystem.\\n\\nSecond, develop and test is where you begin to integrate your initial code with our ecosystem, test and finally build your application in ECR.\\n\\nFinally, deploy and iterate is where you schedule your application for deployment and look at the results.\\n\\nA driving example\\n\\nIn order to illustrate progress through each of these stages, we\\'ll start with a concrete code example and iterate on it over the next few sections.\\n\\nIn practice, lots of work goes into the data and model selection step. For now, we\\'ll assume that groundwork has already been done and we\\'ve settled on the following code snippit to start with.\\n\\n```python import numpy as np import cv2\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): # read example image from file image = cv2.imread(\"example.jpg\")\\n\\n# compute mean color\\nmean_color = compute_mean_color(image)\\n\\n# print mean color\\nprint(mean_color)\\n\\nif name == \"main\": main() ```\\n\\nBootstrapping our app from a template\\n\\nWe\\'ll start our by using a cookiecutter template to bootstrap our app.\\n\\nFirst, ensure the latest cookiecutter is installed:\\n\\nsh pip3 install --upgrade cookiecutter\\n\\nNow, run the following command:\\n\\nsh cookiecutter gh:waggle-sensor/cookiecutter-sage-app\\n\\nYou should be prompted to fill in the following fields:\\n\\ntxt [1/5] name (my-amazing-app-name): my-amazing-app-name [2/5] description (My really amazing app!): [3/5] author (My name): [4/5] version (0.1.0): [5/5] Select kind 1 - vision 2 - usbserial_sensor 3 - minimal 4 - tutorial <<< use 4 for tutorial Choose from [1/2/3/4] (1): 4\\n\\nIf this succeeds, a new app-tutorial directory will be created with the following files:\\n\\nName Description main.py Main code requirements.txt Code dependencies Dockerfile App build instructions sage.yaml App metadata\\n\\nInstalling the dependencies\\n\\nThe first step in preparing our example for the edge is to install pywaggle in our local development environment.\\n\\npywaggle is our Python SDK which provides edge apps access to devices (ex. cameras and microphones) and messaging within a node.\\n\\nFor this tutorial, we\\'ll install the latest version of the requirements included in the template:\\n\\nsh pip3 install --upgrade --requirement requirements.txt\\n\\nAccessing a camera\\n\\nNow that we have pywaggle, the first change we\\'ll make is to use a camera as input rather than a static image file. We\\'ll use the following shapshot() function to take an RGB snapshot from the camera.\\n\\n```python import numpy as np\\n\\nfrom waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n# compute mean color\\nmean_color = compute_mean_color(snapshot.data)\\n\\n# print mean color\\nprint(mean_color)\\n\\nif name == \"main\": main() ```\\n\\nNow, we can try this out by running:\\n\\nsh python3 main.py\\n\\nYou should see output like:\\n\\ntxt [51.43575738 51.83611871 54.64226671]\\n\\nYou\\'re exact numbers may differ as this is computed using your default camera.\\n\\nPublishing results\\n\\nThe next change we\\'ll make is to publish our data to the Beehive Data Repository instead of just print it. This will allow it to be sent to a Beehive once it\\'s scheduled on a node.\\n\\n```python import numpy as np\\n\\nfrom waggle.plugin import Plugin from waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): with Plugin() as plugin: # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n    # compute mean color\\n    mean_color = compute_mean_color(snapshot.data)\\n\\n    # publish mean color\\n    plugin.publish(\"color.mean.r\", mean_color[0], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.g\", mean_color[1], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.b\", mean_color[2], timestamp=snapshot.timestamp)\\n\\nif name == \"main\": main() ```\\n\\nNow, we\\'ll run this using:\\n\\nsh python3 main.py\\n\\nYou may notice something... there\\'s no output! Usually, published data is sent to a beehive where it can be viewed later. However, because we\\'re developing locally and have not configured a beehive, the data isn\\'t going anywhere. In the next section, we\\'ll see how we can tap into our published data.\\n\\nViewing run logs\\n\\nIn order to make developing and debugging apps easier, pywaggle can write out a log directory as follows:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nThis will create a new directory named test-run and will contain a file named data.ndjson which contains something like:\\n\\njson {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":32.67932074652778} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":19.087491319444446} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":10.337491319444444}\\n\\nIf we run python3 main.py again, then we\\'ll see new data appended to that file:\\n\\njson {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":32.67932074652778} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":19.087491319444446} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":10.337491319444444} {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":30.90709743923611} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":16.61302517361111} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":8.565154079861111}\\n\\nThis provides a convenient way to understand the behavior of an app, particularly one with a more complicated flow.\\n\\nUploading a snapshot\\n\\nFinally, the last change we\\'ll make is to upload our snapshots after publishing the mean color.\\n\\nWe\\'ll upload every snapshot for demonstration purposes, but you wouldn\\'t want to do this in a real app. Instead, you\\'d typically upload in response to detecting an event such as an anomalous object or loud noise.\\n\\n```python import numpy as np\\n\\nfrom waggle.plugin import Plugin from waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): with Plugin() as plugin: # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n    # compute mean color\\n    mean_color = compute_mean_color(snapshot.data)\\n\\n    # publish mean color\\n    plugin.publish(\"color.mean.r\", mean_color[0], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.g\", mean_color[1], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.b\", mean_color[2], timestamp=snapshot.timestamp)\\n\\n    # save and upload image\\n    snapshot.save(\"snapshot.jpg\")\\n    plugin.upload_file(\"snapshot.jpg\", timestamp=snapshot.timestamp)\\n\\nif name == \"main\": main() ```\\n\\nLet\\'s run our app again using:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nIf you take a look in the test-run/uploads directory, you should now see an image.\\n\\nUploads are added to the run log directory using the format nstimestamp-filename.\\n\\nYou should also see a corresponding item in the data.ndjson file.\\n\\njson {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":29.601871744791666} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":16.004838324652777} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":8.217218967013888} {\"meta\":{\"filename\":\"snapshot.jpg\"},\"name\":\"upload\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":\"/Users/sean/dev/pw-example/test-run/uploads/1661279974985679000-snapshot.jpg\"}\\n\\nTools for analyzing run logs (Optional)\\n\\nIf you find yourself working with run logs frequently, we recommend the Sage data client which provides convenient functionality for loading and doing analysis on the data.ndjson file. See the \"Load results from file\" example for more info.\\n\\nNext steps\\n\\nCongratulations! You\\'ve finished preparing our example code for the edge!\\n\\nIn the part 3, we\\'ll look at how we can build and test our app on a real node!'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='sidebar_position: 3\\n\\nPart 3: Testing an edge app\\n\\nIn the previous part, we took a code snippit and iterated on it until it was ready for the edge. By the end, we had basic camera access and publishing working!\\n\\nNow, we\\'re ready to start testing it on a development node and describing our build steps.\\n\\nAccessing development nodes\\n\\nThe first thing we need to do is get access to a development node. Unfortunately, we are still developing the infrastructure to open this up to general users.\\n\\nFor now, please contact us to request access to a development node and we\\'ll work with you to setup access.\\n\\nCreating a repo for our app\\n\\nBefore connecting to our node, let\\'s take a moment to organize our code into a repo we will later use on the node.\\n\\nGo ahead and create a new Github repo named app-tutorial and commit the files from previous part.\\n\\nBuilding our app\\n\\nNow that we\\'ve setup node access, ssh to the node then clone and cd into your app-tutorial repo:\\n\\nsh git clone https://github.com/username/app-tutorial cd app-tutorial\\n\\nThe first thing we\\'ll do is build our app on the node:\\n\\nsh sudo pluginctl build .\\n\\nThis may take some time, but once it completes you should see something like:\\n\\n```txt Sending build context to Docker daemon 59.39kB Step 1/6 : FROM waggle/plugin-base:1.1.1-base ... Step 2/6 : WORKDIR /app ... Step 3/6 : COPY requirements.txt . ... Step 4/6 : RUN pip3 install --no-cache-dir -r requirements.txt ... Step 5/6 : COPY . . ... Step 6/6 : ENTRYPOINT [\"python3\", \"main.py\"] ... b38bc0a208d0: Pushed 1101ffccd70a: Pushed latest: digest: sha256:7bee2a62fbcc9913f1c53bbdab79e973e70947618ffe4db90cae6a8f0ff6c8d7 size: 2407 Successfully built plugin\\n\\n10.31.81.1:5000/local/app-tutorial ```\\n\\nOnce we see Successfully built plugin, we can continue to running our app.\\n\\nRunning our app\\n\\nWhen we successfully built our app, the last line of output was 10.31.81.1:5000/local/app-tutorial. We will now use this reference to run our app.\\n\\nsh sudo pluginctl run --name app-tutorial 10.31.81.1:5000/local/app-tutorial\\n\\nWhen you run this, you\\'ll see that there\\'s a bug in the code:\\n\\n```sh Launched the plugin app-tutorial-1659971085 successfully INFO: 2022/08/08 15:04:45 run.go:63: Plugin is in \"Pending\" state. Waiting...\\n\\n[ WARN:0@0.032] global /io/opencv/modules/videoio/src/cap_v4l.cpp (902) open VIDEOIO(V4L2:/dev/video0): can\\'t open camera by index Traceback (most recent call last): File \"main.py\", line 32, in\\n\\nThis was caused by the fact that most nodes have multiple cameras, so we need to be more specific about which camera to use.\\n\\nTo address this, we\\'ll change the following line in main.py from:\\n\\npython with Camera() as camera:\\n\\nto:\\n\\npython with Camera(\"left\") as camera:\\n\\nThe specific camera name will depend on your specific node. If you are having problems accessing a camera, please contact us for more details.\\n\\nAfter rebuilding and running this again, the plugin should run and exit cleanly:\\n\\n```txt Launched the plugin app-tutorial-1659971085 successfully INFO: 2022/08/08 15:04:45 run.go:63: Plugin is in \"Pending\" state. Waiting...\\n\\nshould exit cleanly with no output\\n\\n```\\n\\nNow that we know this works, please commit and push the change to the repo from your machine.\\n\\nFinally, if you are rebuilding and running code frequently, you can combine the build and run into a single step as follows:\\n\\nsh sudo pluginctl run --name app-tutorial $(sudo pluginctl build .)\\n\\nViewing our output\\n\\nWe\\'ll close this part, by looking at the data we just published. To do this, we\\'ll query the Beehive Data Repository:\\n\\nsh curl -s \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-5m\", \"filter\": { \"task\": \"app-tutorial\" } }\\'\\n\\nYou should see some results like:\\n\\njson {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.b\",\"value\":133.61671793619792,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.g\",\"value\":136.46639404296874,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.r\",\"value\":134.48696818033855,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/Pluginctl/sage-app-tutorial-app-tutorial/000048b02d15bdc2/1659971088820981933-snapshot.jpg\",\"meta\":{\"filename\":\"snapshot.jpg\",\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}}\\n\\nThese are exactly the mean color values we computed and published!\\n\\nThis is intended to be a quick preview of how to access data to help get you started. If you are interested, we cover this topic in much depth here.\\n\\nNext steps\\n\\nNow we\\'ve been able to build, run and even fix a bug in our code! In part 4, we\\'ll see how to publish a first release of our code to the Edge Code Repository!'),\n",
       " Document(metadata={'source': 'sage-website/docs/events/past/hackathon-2023.md'}, page_content='sidebar_label: August 2023 Hackathon\\n\\nAugust 2023 Sage Hackathon\\n\\nWe are hosting a Hackathon for Sage in late August ~~with preliminary dates August 30-31~~!\\n\\nUpdate: Based on user feedback, we will run afternoon sessions on both August 30 and 31 starting at 1pm CST. We will email out an agenda and invite to Slack to participants who have signed up a few days before the event.\\n\\nThe goal of the Hackathon is to set aside a few hours dedicated to working through user applications, code and science examples in depth. If this interests you, please fill out the signup form as soon as possible!\\n\\nPrior to the Hackathon, we request that you do a few things: 1. Read the Sage Overview in the Sage docs to understand what Sage is and how it might connect to your work. 2. Start assembling the people on your team likely to participate. 3. Ensure they have accounts in the Sage Portal. 4. Start working through the Edge apps tutorial in the Sage docs.\\n\\nDuring the Hackathon, we will review how to use the portal and parts of the Edge app tutorial. However, already having done some preliminary work will allow more time for our team to provide support for your unique application.'),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content='Sage Data Client\\n\\nThis is the official Sage Python data API client. Its main goal is to make writing queries and working with the results easy. It does this by:\\n\\nProviding a simple query function which talks to the data API.\\n\\nProviding the results in an easy to use Pandas data frame.\\n\\nInstallation\\n\\nSage Data Client can be installed with pip using:\\n\\nsh pip3 install sage-data-client\\n\\nIf you prefer to install this package into a Python virtual environment or are unable to install it system wide, you can use the venv module as follows:\\n\\n```sh\\n\\n1. Create a new virtual environment called my-venv.\\n\\npython3 -m venv my-venv\\n\\n2. Activate the virtual environment\\n\\nsource my-venv/bin/activate\\n\\n3. Install sage data client in the virtual environment\\n\\npip3 install sage-data-client ```\\n\\nNote: If you are using Linux, you may need to install the python3-venv package which is outside of the scope of this document.\\n\\nNote: You will need to activate this virtual environment when opening a new terminal before running any Python scripts using Sage Data Client.\\n\\nUsage Examples\\n\\nQuery API\\n\\n```python import sage_data_client\\n\\nquery and load data into pandas data frame\\n\\ndf = sage_data_client.query( start=\"-1h\", filter={ \"name\": \"env.temperature\", } )\\n\\nprint results in data frame\\n\\nprint(df)\\n\\nmeta columns are expanded into meta.fieldname. for example, here we print the unique nodes\\n\\nprint(df[\"meta.vsn\"].unique())\\n\\nprint stats of the temperature data grouped by node + sensor.\\n\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"])) ```\\n\\n```python import sage_data_client\\n\\nquery and load data into pandas data frame\\n\\ndf = sage_data_client.query( start=\"-1h\", filter={ \"name\": \"env.raingauge.*\", } )\\n\\nprint number of results of each name\\n\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size()) ```\\n\\nLoad results from file\\n\\nIf we have saved the results of a query to a file data.json, we can also load using the load function as follows:\\n\\n```python import sage_data_client\\n\\nload results from local file\\n\\ndf = sage_data_client.load(\"data.json\")\\n\\nprint number of results of each name\\n\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size()) ```\\n\\nIntegration with Notebooks\\n\\nSince we leverage the fantastic work provided by the Pandas library, performing things like looking at dataframes or creating plots is easy.\\n\\nA basic example of querying and plotting data can be found here.\\n\\nAdditional Examples\\n\\nAdditional code examples can be found in the examples directory.\\n\\nIf you\\'re interested in contributing your own examples, feel free to add them to examples/contrib and open a PR!\\n\\nReference\\n\\nThe query function accepts the following arguments:\\n\\nstart. Absolute or relative start timestamp. (required)\\n\\nend. Absolute or relative end timestamp.\\n\\nhead. Limit results to head earliest values per series. (Only one of head or tail can be provided.)\\n\\ntail. Limit results to tail latest values per series. (Only one of head or tail can be provided.)\\n\\nfilter. Key-value patterns to filter data on.'),\n",
       " Document(metadata={'source': 'pywaggle/README.md'}, page_content='Waggle Python Module\\n\\npywaggle is a Python module for implementing Waggle plugins and system services.\\n\\nInstallation Guides\\n\\nMost users getting started with pywaggle will want to install latest version with all optional dependencies using:\\n\\nsh pip install -U pywaggle[all]\\n\\nAdvanced users can install specific subsets of functionality using the following extras flags:\\n\\naudio - Audio and microphone support for plugins.\\n\\nvision - Image, video and camera support for plugins.\\n\\n```sh\\n\\ninstall only core plugin features\\n\\npip install pywaggle\\n\\ninstall only audio features\\n\\npip install pywaggle[audio]\\n\\ninstall only vision features\\n\\npip install pywaggle[vision]\\n\\ninstall both audio and vision features\\n\\npip install pywaggle[audio,vision] ```\\n\\nUsage Guides\\n\\nWriting a plugin'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='Writing a plugin\\n\\nAlthough this doc is still useful as a reference for more advanced pywaggle features, we have migrated the getting started portions to their own tutorial on our central docs site. The pywaggle docs will become more of a reference guide in the near future.\\n\\nIn this guide, we\\'ll walk through writing a basic plugin and exploring some of the functionality provided by pywaggle.\\n\\nThat being said, we do want to emphasize that pywaggle is designed to make it easy to interface existing Python code with the Waggle stack. To a first approximation, pywaggle aims to augment print statements with publish statements.\\n\\nIf you\\'d like to jump ahead to real code, please see the following examples:\\n\\nMinimal Numpy Example\\n\\nHello World ML Example\\n\\nThese repos can be used as starter templates for your own plugin development.\\n\\nWhat is a plugin?\\n\\nA plugin is a self-contained program which typically reads sensors, audio or video data, does some processing and finally publishes results derived from that data.\\n\\nThe most basic example of a plugin is one which simply reads and publishes a value from a sensor. A more complex plugin could publish the number of cars seen in a video stream using a deep learning model.\\n\\nPlugins fit into the wider Waggle infrastructure by being tracked in the Edge Code Repository, deployed to nodes and publishing data to our data repository.\\n\\nWriting \"Hello World\" plugin code\\n\\nNote: In this guide, we currently only cover writing the plugin __code__. We still are updating the docs on building and running a plugin inside Virtual Waggle and natively. As such, this guide will help you structure and run your code locally but not against the rest of platform.\\n\\nWe\\'ll walk through writing a \"hello world\" plugin which simply publishes a increasing counter as measurement hello.world.counter every second.\\n\\n1. Install pywaggle\\n\\nFirst, we\\'ll install the latest version of pywaggle.\\n\\nsh pip install -U pywaggle[all]\\n\\nThis will install the core pywaggle modules along with the extra developer modules.\\n\\n2. Create empty plugin directory\\n\\nCreate a new empty directory which we\\'ll write our plugin in.\\n\\nsh mkdir plugin-hello-world cd plugin-hello-world\\n\\n3. Create a requirements.txt dependency file\\n\\nCreate a new requirements.txt file and add this following:\\n\\nsh pywaggle[all]\\n\\nThis will be used when building our plugin to ensure all dependencies are available. Right now, it only contains pywaggle but you can add your own custom dependencies here.\\n\\n4. Create main.py file\\n\\nCreate a new file called main.py with the following code:\\n\\n```python from waggle.plugin import Plugin import time\\n\\nwith Plugin() as plugin: for i in range(10): print(\"publishing value\", i) plugin.publish(\"hello.world.value\", i) time.sleep(1) ```\\n\\n5. Run plugin\\n\\nThe plugin can now be run using:\\n\\nsh python3 main.py\\n\\nYou should see the output:\\n\\ntxt publishing value 0 publishing value 1 publishing value 2 ...\\n\\n6. Access run logs (Optional)\\n\\nAs you\\'re developing and debugging a plugin, it can be very helpful to see the run log of published messages and uploads.\\n\\nYou can enable this by defining the PYWAGGLE_LOG_DIR=path/to/run/logs environment variable as follows:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nNow, you should see a new directory named test-run with the following contents:\\n\\ntxt test-run/ data.ndjson <- measurements published uploads/ <- timestamped files uploaded nstimestamp1-filename1 nstimestamp2-filename2 ...\\n\\nThe data.ndjson file is a newline delimited JSON file containing raw measurement messages.\\n\\nHere\\'s an example from a more complete plugin:\\n\\njson {\"name\":\"env.temperature\",\"timestamp\":\"2022-08-23T13:27:10.562104000\",\"meta\":{\"sensor\":\"bme280\"},\"value\":23.0} {\"name\":\"upload\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{\"camera\":\"top\",\"filename\":\"test.png.webp\"},\"value\":\"/Users/sean/git/pywaggle-log-dir-example/testrun/uploads/1661279233561615000-test.png.webp\"} {\"name\":\"image.cats\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{},\"value\":0} {\"name\":\"image.birds\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{\"camera\":\"left\"},\"value\":8} {\"name\":\"timeit.inference\",\"timestamp\":\"2022-08-23T13:27:11.562104000\",\"meta\":{},\"value\":1005408000}\\n\\nThe contents of the log directory operates in an append mode, so you may safely run the plugin multiple times without losing previous data.\\n\\nAdding \"Hello World\" plugin packaging info\\n\\nNow that we have the basic plugin code working, let\\'s prepare this code to be submitted to the Edge Code Repository.\\n\\n1. Create a Github repo for plugin\\n\\nFirst, we need to create a Github repo for our plugin. Go ahead a create one called \"plugin-hello-world\" and add the contents from our plugin-hello-world directory.\\n\\nFor the purposes of this example, we\\'ll assume our plugin URL is https://github.com/username/plugin-hello-world.\\n\\n2. Add Dockerfile\\n\\nCreate and add a new file called Dockerfile with the following contents:\\n\\ndockerfile FROM waggle/plugin-base:1.1.1-ml COPY requirements.txt /app/ RUN pip3 install --no-cache-dir --upgrade -r /app/requirements.txt COPY . /app/ WORKDIR /app ENTRYPOINT [\"python3\", \"/app/main.py\"]\\n\\nThis file defines what base image should be used by a plugin and how it should be run. In more complex examples, additional dependencies may be specified here.\\n\\n3. Add sage.yaml\\n\\nCreate and add a new file called sage.yaml with the following contents:\\n\\nyaml name: \"hello-world\" description: \"My hello world plugin\" keywords: \"hello, testing\" authors: \"Your Name <your.email@somewhere.org>, A Coworker <your.coworker@somewhere.org>\" collaborators: \"Helpful Collaborator <our.collaborator@otherplace.edu>\" funding: \"\" license: \"\" homepage: \"https://github.com/username/plugin-hello-world/blob/main/README.md\" source: architectures: - \"linux/amd64\" - \"linux/arm64\"\\n\\nThis file contains metadata about what your plugin is called and what it\\'s supposed to do. It is used by the Edge Code Repository when submitting plugins.\\n\\n4. Add ECR media\\n\\nCreate a ecr-meta directory and populate it with the following text and media:\\n\\necr-science-description.md - Markdown with in depth description of the science being done here (1 page of text).\\n\\necr-icon.jpg - An icon for the project/work 512x512px.\\n\\necr-science-image.jpg - A science image for the project with a minimum size of 1920x1080px.\\n\\nBeyond the basics\\n\\nMore about the publish function\\n\\nIn the previous example, we saw the most basic usage of the publish function. Now, we want to talk about a couple additional features available to you.\\n\\nFirst, metadata can be added to measurements to provide context to how a measurement was created. For example, suppose we had a left and a right facing camera on a node and wanted to track which one was used.\\n\\npython plugin.publish(\"my.sensor.name\", 123, meta={\"camera\": \"left\"})\\n\\nThis will bind the meta data together with the measurement and will be available throughout the rest of the data pipeline.\\n\\nSecond, you can explicitly provide a timestamp for situations where you have more information on when a measurement was taken. For example:\\n\\npython plugin.publish(\"my.sensor.name\", 123, timestamp=my_timestamp_in_ns)\\n\\nNote: Timestamps are expected to be in nanoseconds since epoch. In Python 3.7+, this is available through the standard time.time_ns() function.\\n\\nSubscribing to other measurements\\n\\nPlugins can subscribe to measurements published by other plugins running on the same node. This allows users to leverage existing work or compose a larger application of multiple independent components.\\n\\nThe followng basic example simply waits for measurements named \"my.sensor.name\" and prints the value it received.\\n\\n```python from waggle.plugin import Plugin from random import random\\n\\nwith Plugin() as plugin: plugin.subscribe(\"my.sensor.name\")\\n\\nwhile True:\\n    msg = plugin.get()\\n    print(\"Another plugin published my.sensor.name value\", msg.value)\\n\\n```\\n\\nIn the case you need multiple multiple measurements, you can simply use:\\n\\npython plugin.subscribe(\"my.sensor.name1\", \"my.sensor.name2\", \"my.sensor.name3\")\\n\\nTo differentiate the results, you can use the message name:\\n\\npython msg = plugin.get() if msg.name == \"my.sensor.name1\": # do something elif msg.name == \"my.sensor.name2\": # do something else\\n\\nIn more complex examples, the full message metadata can also be used to differentiate behavior:\\n\\n```python plugin.subscribe(\"env.temperature\")\\n\\nwhile True: msg = plugin.get() if msg.meta.get(\"sensor\") == \"bme280\": # do something elif msg.meta.get(\"sensor\") == \"bme680\": # do something else ```\\n\\nMore about the subscribe function\\n\\nThe subscribe function can match two kinds of wildcard patterns. Measurement names are treated as \"segments\" broken up by a dot and we can match various segments using the star and hash operators.\\n\\nFirst, we can match a single wildcard segment using the \"my.sensor.*\" pattern. This will match all measurements with exactly three segments and whose first segment is \"my\", second segment is \"sensor\" and third segment can be anything.\\n\\nSecond, we can match zero or more segments using the \"my.#\" pattern. This will match all measurements whose first segment is \"my\" like \"my.sensor\", \"my.sensor.name\" or \"my.sensor.name.is.cool\".\\n\\nWorking with camera and microphone data\\n\\npywaggle provides a simple abstraction to cameras and microphones.\\n\\nAccessing a video stream\\n\\n```python from waggle.plugin import Plugin from waggle.data.vision import Camera import time\\n\\nuse case 1: take a snapshot and process\\n\\nwith Plugin() as plugin: sample = Camera().snapshot() # do processing result = process(sample.data) plugin.publish(\"my.measurement\", result, timestampe=sample.timestamp)\\n\\nuser case 2: process camera frames\\n\\nwith Plugin() as plugin, Camera() as camera: # process samples from video stream for sample in camera.stream(): count = count_cars_in_image(sample.data) if count > 10: sample.save(\"cars.jpg\") plugin.upload_file(\"cars.jpg\") ```\\n\\nThe camera.snapshot() function returns a camera frame. The function provides a convenient way to capture a frame and process it.\\n\\nThe camera.stream() function yields a sequence of ImageSample with the following properties:\\n\\nsample.data. captured image\\'s numpy data array.\\n\\nsample.timestamp. captured image\\'s nanosecond timestamp.\\n\\nAdditionally, the Camera class accepts URLs and video files as input. For example:\\n\\n```python\\n\\nopen an mjpeg-over-http stream\\n\\ncamera = Camera(\"http://camera-server/profile1.mjpeg\")\\n\\nopen an rtsp stream\\n\\ncamera = Camera(\"rtsp://camera-server/v0.mp4\")\\n\\nopen a local file using file:// url\\n\\ncamera = Camera(\"file://path/to/my_cool_video.mp4\")\\n\\nopen a camera by device id (when plugin runs on a node)\\n\\ncamera = Camera(\"bottom_camera\") ```\\n\\nCamera buffering and use cases\\n\\n```\\n\\nQ1. How often do you need camera frames? A1. (As many as possible) ---> Refer to use case 1 A2. (Occasionally) ---> Go to Q2\\n\\nQ2. How sensitive is your application to a short delay when capturing an image? A1. (Very sensitive) ---> Refer to use case 1 A2. (A second is ok) ---> Refer to use case 2 ```\\n\\nThe Camera class wrapped in the Python with statement runs a background thread to keep up with the camera stream. This allows users to get the latest frame whenever .stream() or .snapshot() are called. However, this may be uncessary when users want to close the stream after grabbing a frame or the Camera class is used with a file, not a stream.\\n\\nTherefore, it is highly recommended to use the Camera class with the Python with statement when users want to process consequtive frames.\\n\\nUse case 1\\n\\n```python from time import sleep from waggle.data.vision import Camera\\n\\nwith Camera() as camera: former_frame = camera.snapshot() sleep(5) # the current_frame gets the latest frame current_frame = camera.snapshot() calculate_motion(current_frame, former_frame) ```\\n\\nFor simple grab-and-go use cases, users use the Camera class without the with statement to avoid the background process and its resource consumption.\\n\\nUse case 2 ```python from time import sleep from waggle.data.vision import Camera\\n\\nThe Camera class closes the stream after obtaining\\n\\na frame\\n\\ncamera = Camera() former_frame = camera.snapshot() sleep(5)\\n\\nThe Camera class opens the stream and grabs a frame\\n\\ncurrent_frame = camera.snapshot() calculate_motion(current_frame, former_frame) ```\\n\\nRecording video data\\n\\n```python from waggle.data.vision import Camera\\n\\ncamera = Camera()\\n\\nrecord a 30-second video from the camera\\n\\nvideo = camera.record(duration=30) with video: for frame in video: process(frame.data) ```\\n\\nThe Camera class allows users to record a video from camera and store the clip into a file. Because it relies on ffmpeg user code and its container (if in a Docker container) must have ffmpeg installed. You may install it as follow,\\n\\n```bash\\n\\nfor ubuntu\\n\\napt-get update && apt-get install -y ffmpeg ```\\n\\nAlso, the .record() function may NOT be used with Python with statement for USB cameras.\\n\\n```python from waggle.data.vision import Camera\\n\\nthe camera is a USB camera\\n\\ndevice = \"/dev/camera0\" with Camera(device) as camera: # this raises an exception as the camera stream is already open by the with statement video = camera.record(duration=30)\\n\\nUSB cameras can be used as below\\n\\nvideo = Camera(device).record(duration=30) ```\\n\\nRecording audio data\\n\\n```python from waggle.plugin import Plugin from waggle.data.audio import Microphone import time\\n\\nwith Plugin() as plugin, Microphone() as microphone: # record and upload a 10s sample periodically while True: sample = microphone.record(10) sample.save(\"sample.ogg\") plugin.upload_file(\"sample.ogg\") time.sleep(300) ```\\n\\nSimilar to ImageSample, AudioSample provide the following properties:\\n\\nsample.data. captured audio\\'s numpy data array.\\n\\nsample.timestamp. captured audio\\'s nanosecond timestamp.\\n\\nsample.samplerate. captured audio\\'s sample rate.\\n\\nAudioFolder and ImageFolder for testing\\n\\nWe provide a couple simple classes to provide audio and image data from a directory for testing.\\n\\nIn the following example, we assume we have directories:\\n\\ntxt audio_data/ example1.ogg example2.ogg ... image_data/ img1.png cat-7.png example10.jpg ...\\n\\nWe can load up all the audio files in the audio_data folder for testing as follows:\\n\\n```python from waggle.data.audio import AudioFolder\\n\\ndataset = AudioFolder(\"audio_data\")\\n\\nfor sample in dataset: process_data(sample.data) ```\\n\\nSimilarly, we can do something similar for all the image files in the image_data folder.\\n\\n```python from waggle.data.vision import ImageFolder\\n\\ndataset = ImageFolder(\"image_data\")\\n\\nfor sample in dataset: process_image_frame(sample.data) ```\\n\\nAdvanced: Choosing a color format\\n\\nBy default, the waggle.data.vision submodule uses an RGB color format. If you need more control, you can specify one of RGB or BGR to both the Camera and ImageFolder objects as follows:\\n\\n```python from waggle.data.vision import Camera, ImageFolder, RGB, BGR\\n\\nuse BGR data instead of RGB\\n\\ncamera = Camera(format=BGR)\\n\\nuse BGR data instead of RGB\\n\\ncamera = ImageFolder(format=BGR) ```\\n\\nAdvanced: Timing a block\\n\\nThe Plugin class provides a simple utility for timing how long a block of code takes.\\n\\nThe following example shows how we can instrument our code using a typical AI/ML example plugin.\\n\\n```python from waggle.plugin import Plugin\\n\\nwith Plugin() as plugin: # measures duration of input block and publishes to plugin.duration.input with plugin.timeit(\"plugin.duration.input\"): get_inputs(...)\\n\\n# measures duration of inference block and publishes to plugin.duration.inference\\nwith plugin.timeit(\"plugin.duration.inference\"):\\n    do_inference(...)\\n\\npublish_results(...)\\n\\n```\\n\\nIn the example above, the duration of the input and inference steps are measured and then the plugin publishes the duration in nanoseconds to the name provided to plugin.timeit as each block finishes.\\n\\nSeeing the internal details\\n\\nIf we run the basic example, the only thing we\\'ll see is the message \"publishing a value!\" every second. If you need to see more details, pywaggle is designed to easily interface with Python\\'s standard logging module. To enable debug logging, simply make the following additions:\\n\\n```python from waggle.plugin import Plugin from time import sleep\\n\\n1. import standard logging module\\n\\nimport logging\\n\\n2. enable debug logging\\n\\nlogging.basicConfig(level=logging.DEBUG)\\n\\nwith Plugin() as plugin: while True: sleep(1) print(\"publishing a value!\") plugin.publish(\"my.sensor.name\", 123) ```\\n\\nYou should see a lot of information like:\\n\\ntext DEBUG:waggle.plugin:starting plugin worker thread DEBUG:waggle.plugin:connecting to rabbitmq broker at rabbitmq:5672 with username \"plugin\" DEBUG:waggle.plugin:rabbitmq connection error: [Errno 8] nodename nor servname provided, or not known publishing a value! DEBUG:waggle.plugin:adding message to outgoing queue: Message(name=\\'my.sensor.name\\', value=123, timestamp=1619628240863845000, meta={}) DEBUG:waggle.plugin:connecting to rabbitmq broker at rabbitmq:5672 with username \"plugin\" DEBUG:waggle.plugin:rabbitmq connection error: [Errno 8] nodename nor servname provided, or not known publishing a value!\\n\\nThe most important lines are:\\n\\ntext publishing a value! DEBUG:waggle.plugin:adding message to outgoing queue: Message(name=\\'my.sensor.name\\', value=123, timestamp=1619628240863845000, meta={})\\n\\nThese are telling us that our messages are being queued up in an outgoing queue to be shipped.\\n\\nYou\\'ll also see a number of messages related to rabbitmq.\\n\\nThese are simply indicating the our plugin is waiting to connect to the Waggle ecosystem. This is normal when testing a standalone plugin without the rest of the Waggle stack. Plugins will simply queue up measurements in-memory until they exit.'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/README.md'}, page_content='Waggle Module\\n\\nThis page gives an overview of the core functionality provided by this module.\\n\\nPlugin Submodule\\n\\nProvides functionality for publishing sensor data and for processing messages.\\n\\nProtocol Submodule\\n\\nProvides functionality for packing and unpacking sensor and messaging data.')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load all Markdown files\n",
    "md_docs = repo_class_loader(paths, './*.md', UnstructuredMarkdownLoader)\n",
    "md_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.31 ms, sys: 12 ms, total: 21.3 ms\n",
      "Wall time: 37.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='import unittest\\nimport sage_data_client\\nfrom io import BytesIO\\nfrom datetime import datetime, timedelta\\nimport pandas as pd\\n\\n\\nclass TestQuery(unittest.TestCase):\\n    def assertValueResponse(self, df):\\n        self.assertIn(\"name\", df.columns)\\n        df.name.str\\n        self.assertIn(\"timestamp\", df.columns)\\n        df.timestamp.dt\\n        self.assertIn(\"value\", df.columns)\\n\\n    def test_empty_response(self):\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"-2000d\",\\n                filter={\\n                    \"name\": \"should.not.every.exist.XYZ\",\\n                },\\n            )\\n        )\\n\\n    def test_check_one_of_head_or_tail(self):\\n        with self.assertRaises(ValueError):\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00\",\\n                end=\"2021-01-01T10:31:00\",\\n                head=3,\\n                tail=3,\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n\\n    def test_queries(self):\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00\",\\n                end=\"2021-01-01T10:31:00\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00Z\",\\n                end=\"2021-01-01T10:31:00Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00.123Z\",\\n                end=\"2021-01-01T10:31:00.123Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00.123456Z\",\\n                end=\"2021-01-01T10:31:00.123456Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01 10:30:00\",\\n                end=\"2021-01-01 10:31:00\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=datetime(2021, 1, 1, 10, 31, 0),\\n                end=datetime(2021, 1, 1, 10, 32, 0),\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=pd.to_datetime(\"2021-01-01 10:30:00\"),\\n                end=pd.to_datetime(\"2021-01-01 10:31:00\"),\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        for dt in [\"-30s\", \"-3m\", \"-3min\", \"-1d\", \"-1w\"]:\\n            self.assertValueResponse(\\n                sage_data_client.query(\\n                    start=dt,\\n                    tail=1,\\n                    filter={\\n                        \"name\": \"env.temperature\",\\n                    },\\n                )\\n            )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"-4h\",\\n                end=\"-2h\",\\n                tail=1,\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        for dt in [\\n            timedelta(seconds=-30),\\n            timedelta(minutes=-1),\\n            timedelta(hours=-1),\\n            timedelta(days=-1),\\n        ]:\\n            self.assertValueResponse(\\n                sage_data_client.query(\\n                    start=dt,\\n                    tail=1,\\n                    filter={\\n                        \"name\": \"env.temperature\",\\n                    },\\n                )\\n            )\\n\\n    def test_load(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2021-10-14T21:42:21.149425156Z\",\"name\":\"env.temperature\",\"value\":21.74,\"meta\":{\"host\":\"0000dca632a3069f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31f\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"env.temperature\",\"value\":26.09,\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}\\n{\"timestamp\":\"2021-10-14T21:42:19.087014343Z\",\"name\":\"env.temperature\",\"value\":28.14,\"meta\":{\"host\":\"0000dca632a3074d.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc73\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W024\"}}\\n{\"timestamp\":\"2021-10-14T21:42:23.475857326Z\",\"name\":\"env.temperature\",\"value\":28.16,\"meta\":{\"host\":\"0000dca632a3076b.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc6d\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01B\"}}\\n{\"timestamp\":\"2021-10-14T21:42:34.995766556Z\",\"name\":\"env.temperature\",\"value\":33.27,\"meta\":{\"host\":\"0000dca632a3078f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bdc7\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W020\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.803472584Z\",\"name\":\"env.temperature\",\"value\":9.8,\"meta\":{\"host\":\"0000dca632a30792.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc42\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01D\"}}\\n{\"timestamp\":\"2021-10-14T21:42:30.9261079Z\",\"name\":\"env.temperature\",\"value\":25.63,\"meta\":{\"host\":\"0000dca632a307b6.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc8c\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W039\"}}\\n{\"timestamp\":\"2021-10-14T21:42:24.228048661Z\",\"name\":\"env.temperature\",\"value\":23.96,\"meta\":{\"host\":\"0000dca632a307bf.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc7d\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W028\"}}\\n{\"timestamp\":\"2021-10-14T21:42:13.914329997Z\",\"name\":\"env.temperature\",\"value\":21.84,\"meta\":{\"host\":\"0000dca632a307e6.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d05a1c2\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W02C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:17.300924641Z\",\"name\":\"env.temperature\",\"value\":30.12,\"meta\":{\"host\":\"0000dca632a307fb.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c328\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W016\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertEqual(len(df), 10)\\n        self.assertValueResponse(df)\\n\\n    def test_load_small_numbers(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2023-09-25T19:26:26.18944512Z\",\"name\":\"sensor_body_temperature\",\"value\":-655230609742226300000,\"meta\":{\"applicationId\":\"ac81e18b-1925-47f9-839a-27d999a8af55\",\"applicationName\":\"ATMOS test app\",\"devAddr\":\"00f06b4b\",\"devEui\":\"98208e0000032a15\",\"deviceName\":\"MFR Node\",\"deviceProfileId\":\"cf2aec2f-03e1-4a60-a32c-0faeef5730d8\",\"deviceProfileName\":\"MFR node\",\"host\":\"0000e45f014caee8.ws-rpi\",\"node\":\"000048b02d15bc8c\",\"plugin\":\"registry.sagecontinuum.org/flozano/lorawan-listener:0.0.6\",\"task\":\"lorawan-listener\",\"tenantId\":\"52f14cd4-c6f1-4fbd-8f87-4025e1d49242\",\"tenantName\":\"ChirpStack\",\"vsn\":\"W039\",\"zone\":\"shield\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertValueResponse(df)\\n        self.assertEqual(len(df), 1)\\n\\n    def test_load_empty_file(self):\\n        df = sage_data_client.load(\"tests/test-empty.ndjson\")\\n        self.assertEqual(len(df), 0)\\n        self.assertValueResponse(df)\\n\\n        df = sage_data_client.load(\"tests/test-empty.ndjson.gz\")\\n        self.assertEqual(len(df), 0)\\n        self.assertValueResponse(df)\\n\\n    def test_load_compressed_file(self):\\n        df1 = sage_data_client.load(\"tests/test-data.ndjson\")\\n        df2 = sage_data_client.load(\"tests/test-data.ndjson.gz\")\\n        self.assertEqual(len(df1), 1611)\\n        self.assertEqual(len(df2), 1611)\\n        # NOTE In Pandas Nones and NaNs do not equal themselves, so will fill them to make df1 == df2 work.\\n        self.assertTrue((df1.fillna(\"\") == df2.fillna(\"\")).all().all())\\n\\n    def test_mixed_types(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2021-10-14T21:42:21.149425156Z\",\"name\":\"test\",\"value\":21.74,\"meta\":{\"host\":\"0000dca632a3069f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31f\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"test\",\"value\":\"26.09\",\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"test\",\"value\":123,\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertEqual(len(df), 3)\\n        self.assertValueResponse(df)\\n        self.assertAlmostEqual(df.iloc[0].value, 21.74)\\n        self.assertEqual(df.iloc[1].value, \"26.09\")\\n        self.assertEqual(df.iloc[2].value, 123)\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/load_data_from_file.py'}, page_content='\"\"\"\\nThis example demonstrates loading a local data file containing 5 minutes of temperature data\\nand printing the mean value grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# load results from local file\\ndf = sage_data_client.load(\"data.json\")\\n\\n# print number of results of each name\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.mean())\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/trigger-stream.py'}, page_content='\"\"\"\\nThis is an example of a simple edge-to-cloud stream trigger which uses sage-data-client\\nto watch the latest internal temperature values and print records which exceed a threshold.\\n\\nAlthough it\\'s simple, this example could easily be extended in multiple ways. For example:\\n* Instead of just printing, an alert could be posted to Slack.\\n* Instead of a fixed threshold, you could learn a moving average per node and flag outliers.\\n\\nNote: In the future, this kind of streaming functionality *might* be provided by sage-data-client,\\nbut for now you can adapt this example to fit you use case.\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\nimport time\\n\\n\\ndef watch(start=None, filter=None):\\n    if start is None:\\n        start = pd.Timestamp.utcnow()\\n\\n    while True:\\n        df = sage_data_client.query(\\n            start=start,\\n            filter=filter,\\n        )\\n\\n        if len(df) > 0:\\n            start = df.timestamp.max()\\n            yield df\\n\\n        time.sleep(3.0)\\n\\n\\ndef main():\\n    filter = {\\n        \"name\": \"env.temperature\",\\n        \"sensor\": \"bme280\",\\n    }\\n\\n    threshold = 50.0\\n\\n    for df in watch(filter=filter):\\n        # print values which exceed threshold\\n        print(df[df.value > threshold].sort_values(\"timestamp\"))\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/print_rain_event_image_urls.py'}, page_content='\"\"\"\\nThis example demonstrates cross referencing rain gauge data to find rainy images. It outputs a list\\nof urls which can be saved and downloaded as follows:\\n\\npython3 print_rain_event_image_urls.py > urls.txt\\nwget -r -N -i urls.txt\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\n\\nvsn = \"W039\"\\n\\n# query raingauge data for the last week\\ndf = sage_data_client.query(\\n    start=\"2021-12-20\",\\n    end=\"2021-12-27\",\\n    filter={\\n        \"name\": \"env.raingauge.acc\",\\n        \"vsn\": vsn,\\n    }\\n)\\n\\n# compute mean rain in hour window\\nmean_acc = df.resample(\"1h\", on=\"timestamp\").value.mean()\\n\\n# find rain accumulation events\\nrain_events = mean_acc[mean_acc > 0]\\n\\n# collect uploads in each rain event window\\nuploads = pd.concat(sage_data_client.query(\\n        start=ts,\\n        end=ts + pd.to_timedelta(\"1h\"),\\n        filter={\\n            \"name\": \"upload\",\\n            \"vsn\": vsn,\\n            \"task\": \"imagesampler-top\",\\n        }\\n    ) for ts in rain_events.index)\\n\\n# print all urls found\\nfor url in uploads.value.values:\\n    print(url)\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/pressure_event_trigger.py'}, page_content='\"\"\"\\nThis example is a skeleton of how to poll the data system every minute for unusual\\npressure events.\\n\\nIn this case, events are determined windows with a stddev above an example\\nthreshold. For applications, you will need to provide your own criteria for\\nevents.\\n\\nAdditionally, you will need to provide a specific mechanism to carry out the\\nalerts (ex. email, Slack, dedicated alerting / ticketing system, etc).\\n\"\"\"\\nimport sage_data_client\\nimport time\\n\\nwhile True:\\n    # query pressure data in recent 10 minute window\\n    df = sage_data_client.query(\\n        start=\"-10m\",\\n        filter={\\n            \"name\": \"env.pressure\",\\n            \"sensor\": \"bme680\",\\n        }\\n    )\\n\\n    # compute stddev for nodes\\' pressure data in window\\n    std = df.groupby(\"meta.vsn\").value.std()\\n\\n    # find all pressure events exceeding an example threshold\\n    events = std[std > 8.0]\\n\\n    # \"post\" vsn to alert system\\n    for vsn in events.index:\\n        print(f\"post {vsn} to alert system\")\\n\\n    time.sleep(60)\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/temperature_stats.py'}, page_content='\"\"\"\\nThis example demonstrates querying all temperature data and printing basic stats grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# query and load data into pandas data frame\\ndf = sage_data_client.query(\\n    start=\"-1h\",\\n    filter={\\n        \"name\": \"env.temperature\",\\n    }\\n)\\n\\n# print stats of the temperature data grouped by node + sensor.\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"]))\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/join_queries.py'}, page_content='\"\"\"\\nThis example demonstrates one approach for combining multiple queries by resampling\\nresults into 30 minute windows and merging those into a new data frame.\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\n\\n\\ndef join_resampled_queries(start, end, window, filters):\\n    \"\"\"\\n    join_resampled_queries joins resampled data for a set of filters together\\n    into a single data frame\\n    \"\"\"\\n    return pd.DataFrame({\\n        name: sage_data_client.query(\\n            start=start,\\n            end=end,\\n            filter=filter,\\n        ).resample(window, on=\"timestamp\").value.mean()\\n        for name, filter in filters.items()\\n    })\\n\\n\\ndef main():\\n    start = \"2022-01-10T00:00:00Z\"\\n    end = \"2022-01-11T00:00:00Z\"\\n    vsn = \"W023\"\\n\\n    # combine lat, lon, temperature, pressure and humidity into data frame\\n    df = join_resampled_queries(start, end, \"30min\", {\\n        \"lat\": {\\n            \"name\": \"sys.gps.lat\",\\n            \"vsn\": vsn,\\n        },\\n        \"lon\": {\\n            \"name\": \"sys.gps.lon\",\\n            \"vsn\": vsn,\\n        },\\n        \"temperature\": {\\n            \"name\": \"env.temperature\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n        \"pressure\": {\\n            \"name\": \"env.pressure\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n        \"humidity\": {\\n            \"name\": \"env.relative_humidity\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n    })\\n\\n    # print out data for quick inspection\\n    print(df)\\n\\n    # save data to csv\\n    df.to_csv(\"combined.csv\")\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/raingauge_totals.py'}, page_content='\"\"\"\\nThis example demonstrates querying rain gauge data and printing the total\\nnumber of measurements grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# query and load data into pandas data frame\\ndf = sage_data_client.query(\\n    start=\"-1h\",\\n    filter={\\n        \"name\": \"env.raingauge.*\",\\n    }\\n)\\n\\n# print number of results of each name\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size())\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/trigger-batch.py'}, page_content='\"\"\"\\nThis is an example of a simple edge-to-cloud batch trigger which uses sage-data-client\\nto gather and aggregate internal temperature data every 5 minutes and prints all\\nnodes which exceed a threshold.\\n\\nAlthough it\\'s simple, this example could easily be extended in multiple ways. For example:\\n* Instead of just printing, an alert could be posted to Slack.\\n* Instead of a fixed threshold, the typical value across all nodes could be used to determine outliers.\\n\"\"\"\\nimport sage_data_client\\nimport time\\n\\n\\ndef main():\\n    filter = {\\n        \"name\": \"env.temperature\",\\n        \"sensor\": \"bme280\",\\n    }\\n\\n    threshold = 55.0\\n\\n    while True:\\n        # get the last 5m of temperature data\\n        df = sage_data_client.query(start=\"-5m\", filter=filter)\\n\\n        # get mean temperature by node in batch query\\n        mean_temps = df.groupby(\"meta.vsn\").value.mean()\\n\\n        # print values which exceed threshold\\n        print(mean_temps[mean_temps > threshold])\\n\\n        # wait 5m\\n        time.sleep(300)\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='from gzip import GzipFile\\nimport json\\nfrom pathlib import Path\\nfrom urllib.request import urlopen, Request\\nimport pandas as pd\\n\\n\\ndef resolve_time(t):\\n    try:\\n        return pd.to_datetime(t)\\n    except (TypeError, ValueError):\\n        pass\\n    return pd.to_datetime(\"now\", utc=True) + pd.to_timedelta(t)\\n\\n\\ndef timestr(t):\\n    return t.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\\n\\n\\ndef query(\\n    start,\\n    end=None,\\n    head: int = None,\\n    tail: int = None,\\n    experimental_func=None,\\n    bucket: str = None,\\n    filter: dict = None,\\n    endpoint: str = \"https://data.sagecontinuum.org/api/v1/query\",\\n) -> pd.DataFrame:\\n    \"\"\"\\n    query makes a query request to the data API and returns the results in a data frame.\\n\\n    Parameters\\n    ----------\\n    start : query start time, required\\n        Timestamps can be a relative like \"-1h\" or absolute like \"2021-05-01T10:30:00Z\".\\n\\n    end : query end time, default: None\\n        Timestamps can be a relative like \"-1h\" or absolute like \"2021-05-01T10:30:00Z\".\\n\\n    head : limit query response to earliest `head` records, default: None (only one of `head` or `tail` can be provided)\\n\\n    tail : limit query response to latest `tail` records, default: None (only one of `head` or `tail` can be provided)\\n\\n    experimental_func : aggregation function to apply to series.\\n\\n    bucket: name of bucket to query\\n\\n    filter : dictionary of query filters, default: None\\n\\n    endpoint : url of query api, default: \"https://data.sagecontinuum.org/api/v1/query\"\\n\\n    Returns\\n    -------\\n    result : pandas.DataFrame\\n        The data frame will contain the query response records.\\n\\n        See the Returns section for the `load` function for more details.\\n\\n    Examples\\n    --------\\n\\n    Querying and perform simple data aggregation\\n\\n    ```python\\n    import sage_data_client\\n\\n    # query and load data into pandas data frame\\n    df = sage_data_client.query(\\n        start=\"-1h\",\\n        filter={\\n            \"name\": \"env.temperature\",\\n        }\\n    )\\n\\n    # print results in data frame\\n    print(df)\\n\\n    # meta columns are expanded into meta.fieldname. for example, here we print the unique nodes\\n    print(df[\"meta.node\"].unique())\\n\\n    # print stats of the temperature data grouped by node + sensor.\\n    print(df.groupby([\"meta.node\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"]))\\n    ```\\n    \"\"\"\\n    # build query\\n    q = {\"start\": timestr(resolve_time(start))}\\n    if end is not None:\\n        q[\"end\"] = timestr(resolve_time(end))\\n    if filter is not None:\\n        q[\"filter\"] = filter\\n    if head is not None and tail is not None:\\n        raise ValueError(\"only one of `head` or `tail` can be provided\")\\n    elif head is not None:\\n        q[\"head\"] = head\\n    elif tail is not None:\\n        q[\"tail\"] = tail\\n    if experimental_func is not None:\\n        q[\"experimental_func\"] = experimental_func\\n    if bucket is not None:\\n        q[\"bucket\"] = bucket\\n\\n    data = json.dumps(q).encode()\\n    headers = {\"Accept-Encoding\": \"gzip\"}\\n    req = Request(endpoint, data, headers=headers)\\n\\n    with urlopen(req) as f:\\n        content_encoding = f.headers.get(\"Content-Encoding\", \"\")\\n        if \"gzip\" in content_encoding:\\n            f = GzipFile(fileobj=f, mode=\"rb\")\\n        return load(f)\\n\\n\\ndef load(path_or_buf) -> pd.DataFrame:\\n    \"\"\"\\n    load reads a path or file like object containing a response from the data api and returns the results in a data frame.\\n\\n    Parameters\\n    ----------\\n    path_or_buf : path like or file like object\\n\\n    Returns\\n    -------\\n    result : pandas.DataFrame\\n        The data frame will contain the query response records. Standard columns names are:\\n\\n        `name`: measurement name (ex. \"env.temperature\")\\n\\n        `timestamp`: measurement timestamp (nanoseconds since epoch resolution)\\n\\n        `value`: measurement value\\n\\n        Metadata fields like \"node\" and \"vsn\" are stored in columns named \"meta.node\" or \"meta.vsn\".\\n\\n    Examples\\n    --------\\n\\n    Loading saved query results from a file\\n\\n    Suppose we\\'ve saved the results of a query to a file `data.json`. We can load them using the following:\\n\\n    ```python\\n    import sage_data_client\\n\\n    # load results from local file\\n    df = sage_data_client.load(\"data.ndjson\")\\n\\n    # print number of results of each name\\n    print(df.groupby([\"meta.node\", \"name\"]).size())\\n    ```\\n    \"\"\"\\n    if isinstance(path_or_buf, str):\\n        if path_or_buf.endswith(\".gz\"):\\n            with GzipFile(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n        else:\\n            with open(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n\\n    if isinstance(path_or_buf, Path):\\n        if path_or_buf.suffix == \".gz\":\\n            with GzipFile(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n        else:\\n            with open(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n\\n    return _load(path_or_buf)\\n\\n\\ndef _load_row(r):\\n    input = json.loads(r)\\n    output = {}\\n    output[\"timestamp\"] = pd.to_datetime(input[\"timestamp\"], unit=\"ns\", utc=True)\\n    output[\"name\"] = input[\"name\"]\\n    output[\"value\"] = input[\"value\"]\\n    for k, v in input[\"meta\"].items():\\n        output[f\"meta.{k}\"] = v\\n    return output\\n\\n\\ndef _load(fileobj) -> pd.DataFrame:\\n    df = pd.DataFrame(map(_load_row, fileobj))\\n\\n    # if dataframe is empty, return empty with known columns\\n    if len(df) == 0:\\n        return pd.DataFrame(\\n            {\\n                \"timestamp\": pd.to_datetime([], utc=True),\\n                \"name\": pd.Series([], dtype=str),\\n                \"value\": [],\\n            }\\n        )\\n\\n    return df\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/__init__.py'}, page_content='\"\"\"\\nsage_data_client - Official Sage Python data API client.\\n========================================================\\n\\nsage_data_client goals are to make writing queries and working with the results easy. It does this by:\\n\\n* Providing a simple query function which talks to the data API.\\n* Providing the results in an easy to use [Pandas](https://pandas.pydata.org) data frame.\\n\"\"\"\\nfrom .query import query, load\\n'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='import unittest\\nfrom pathlib import Path\\nimport json\\nfrom tempfile import TemporaryDirectory\\nimport time\\nfrom datetime import datetime\\nimport os\\nimport pika\\nimport subprocess\\n\\nfrom waggle.plugin import Plugin, PluginConfig, Uploader, get_timestamp\\nimport wagglemsg\\n\\n# TODO(sean) add integration testing against rabbitmq\\n# TODO(sean) clean up the queue interface. it would be better to not know about the plugin.send / plugin.recv queues explicitly.\\n\\n\\nclass TestPlugin(unittest.TestCase):\\n    def test_publish(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test.int\", 1)\\n            plugin.publish(\"test.float\", 2.0)\\n            plugin.publish(\"test.str\", \"three\")\\n            plugin.publish(\\n                \"cows.total\",\\n                391,\\n                meta={\\n                    \"camera\": \"bottom_left\",\\n                },\\n            )\\n\\n    def test_publish_check_reserved(self):\\n        with Plugin() as plugin:\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"upload\", \"path/to/data\")\\n\\n    def test_get(self):\\n        with Plugin() as plugin:\\n            plugin.subscribe(\"raw.#\")\\n            with self.assertRaises(TimeoutError):\\n                plugin.get(timeout=0)\\n            with self.assertRaises(TimeoutError):\\n                plugin.get(timeout=0.001)\\n\\n            msg = wagglemsg.Message(\"test\", 1.0, 0, {})\\n            plugin.recv.put(msg)\\n            msg2 = plugin.get(timeout=0)\\n            self.assertEqual(msg, msg2)\\n\\n    def test_get_timestamp(self):\\n        ts = get_timestamp()\\n        self.assertIsInstance(ts, int)\\n\\n    def test_valid_values(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test\", 1)\\n            plugin.publish(\"test\", 1.3)\\n            plugin.publish(\"test\", \"some string\")\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", b\"some bytes\")\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", [1, 2, 3])\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", {1: 1, 2: 2, 3: 3})\\n\\n    def test_valid_meta(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test\", 1, meta={\"k\": \"v\"})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": 10})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": 12.3})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": []})\\n\\n    def test_valid_timestamp(self):\\n        with Plugin() as plugin:\\n            # valid int, nanosecond timestamp\\n            plugin.publish(\"test\", 1, timestamp=1649694687904754000)\\n\\n            # must prevent a float type timestamp\\n            ts = datetime(2022, 1, 1, 0, 0, 0).timestamp()\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, timestamp=ts)\\n\\n            # must prevent int timestamp in seconds from being loaded by flagging\\n            # timestamps that are too early.\\n            testcases = [\\n                datetime(2022, 1, 1, 0, 0, 0),\\n                datetime(3000, 1, 1, 0, 0, 0),\\n                datetime(5000, 1, 1, 0, 0, 0),\\n            ]\\n\\n            for dt in testcases:\\n                with self.assertRaises(ValueError):\\n                    plugin.publish(\"test\", 1, timestamp=int(dt.timestamp()))\\n\\n    def test_valid_publish_names(self):\\n        with Plugin() as plugin:\\n            with self.assertRaises(TypeError):\\n                plugin.publish(None, 0)\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"\", 0)\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\".\", 0)\\n\\n            # check for reserved names\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"upload\", 0)\\n\\n            # use _ instead of -\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"my-metric\", 0)\\n            # correct alternative\\n            plugin.publish(\"my_metric\", 0)\\n\\n            # assert len(name) <= 128\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"x\" * 129, 0)\\n\\n            # no empty parts allowed\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"vision.count..bird\", 0)\\n            # correct alternative\\n            plugin.publish(\"vision.count.bird\", 0)\\n\\n            # no spaces allowed\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"sys.cpu temp\", 0)\\n            # correct alternative\\n            plugin.publish(\"sys.cpu_temp\", 0)\\n\\n    # TODO(sean) refactor messaging part to make testing this cleaner\\n    def test_upload_file(self):\\n        with TemporaryDirectory() as tempdir:\\n            pl = Plugin(\\n                PluginConfig(\\n                    host=\"fake-rabbitmq-host\",\\n                    port=5672,\\n                    username=\"plugin\",\\n                    password=\"plugin\",\\n                    app_id=\"0668b12c-0c15-462c-9e06-7239282411e5\",\\n                ),\\n                uploader=Uploader(Path(tempdir, \"uploads\")),\\n            )\\n\\n            data = b\"here some data in a data\"\\n            upload_path = Path(tempdir, \"myfile.txt\")\\n            upload_path.write_bytes(data)\\n\\n            pl.upload_file(upload_path)\\n            item = pl.send.get_nowait()\\n            msg = wagglemsg.load(item.body)\\n            self.assertEqual(item.scope, \"all\")\\n            self.assertEqual(msg.name, \"upload\")\\n            self.assertIsNotNone(msg.timestamp)\\n            self.assertIsInstance(msg.value, str)\\n            self.assertIsNotNone(msg.meta)\\n            self.assertIn(\"filename\", msg.meta)\\n\\n    def test_timeit(self):\\n        with Plugin() as plugin:\\n            with plugin.timeit(\"dur\"):\\n                time.sleep(0.001)\\n            item = plugin.send.get(0.01)\\n            msg = wagglemsg.load(item.body)\\n            self.assertEqual(msg.name, \"dur\")\\n\\n\\nclass TestUploader(unittest.TestCase):\\n    def test_upload_file(self):\\n        with TemporaryDirectory() as tempdir:\\n            uploader = Uploader(Path(tempdir, \"uploads\"))\\n\\n            data = b\"here some data in a data\"\\n\\n            upload_path = Path(tempdir, \"myfile.txt\")\\n            upload_path.write_bytes(data)\\n\\n            path = uploader.upload_file(upload_path)\\n            self.assertFalse(upload_path.exists())\\n\\n            self.assertEqual(data, Path(path, \"data\").read_bytes())\\n            meta = json.loads(Path(path, \"meta\").read_text())\\n            self.assertIn(\"timestamp\", meta)\\n            self.assertIn(\"shasum\", meta)\\n            self.assertEqual(meta[\"labels\"][\"filename\"], upload_path.name)\\n\\n\\ndef rabbitmq_available():\\n    try:\\n        subprocess.check_output([\"docker-compose\", \"exec\", \"rabbitmq\", \"true\"])\\n        return True\\n    except subprocess.CalledProcessError:\\n        return False\\n\\n\\ndef get_admin_connection():\\n    params = pika.ConnectionParameters(\\n        credentials=pika.PlainCredentials(\"admin\", \"admin\")\\n    )\\n    return pika.BlockingConnection(params)\\n\\n\\n@unittest.skipUnless(rabbitmq_available(), \"rabbitmq not available\")\\nclass TestPluginWithRabbitMQ(unittest.TestCase):\\n    def setUp(self):\\n        os.environ[\"WAGGLE_PLUGIN_USERNAME\"] = \"plugin\"\\n        os.environ[\"WAGGLE_PLUGIN_PASSWORD\"] = \"plugin\"\\n        os.environ[\"WAGGLE_PLUGIN_HOST\"] = \"127.0.0.1\"\\n        os.environ[\"WAGGLE_PLUGIN_PORT\"] = \"5672\"\\n\\n        with get_admin_connection() as conn, conn.channel() as ch:\\n            ch.queue_purge(\"to-validator\")\\n\\n    def test_publish(self):\\n        now = time.time_ns()\\n\\n        with Plugin() as publisher:\\n            publisher.publish(\"test\", 123, meta={\"sensor\": \"bme680\"}, timestamp=now)\\n\\n        with get_admin_connection() as conn, conn.channel() as ch:\\n            _, _, body = ch.basic_get(\"to-validator\", auto_ack=True)\\n            msg = wagglemsg.load(body)\\n\\n        self.assertEqual(\\n            msg,\\n            wagglemsg.Message(\\n                name=\"test\",\\n                value=123,\\n                meta={\"sensor\": \"bme680\"},\\n                timestamp=now,\\n            ),\\n        )\\n\\n    def test_subscribe(self):\\n        msg = wagglemsg.Message(\\n            name=\"test\",\\n            value=123,\\n            meta={\"sensor\": \"bme680\"},\\n            timestamp=time.time_ns(),\\n        )\\n\\n        with Plugin() as subscriber:\\n            subscriber.subscribe(\"test\")\\n            time.sleep(1)\\n\\n            with get_admin_connection() as conn, conn.channel() as ch:\\n                ch.basic_publish(\"data.topic\", \"test\", wagglemsg.dump(msg))\\n\\n            msg2 = subscriber.get(1)\\n            self.assertEqual(msg, msg2)\\n\\n\\nclass TestPluginLogDir(unittest.TestCase):\\n    def test_log_dir(self):\\n        import sage_data_client\\n\\n        with TemporaryDirectory() as dir:\\n            dir = Path(dir)\\n\\n            # create dummy upload file\\n            upload_file = Path(dir, \"hello.txt\")\\n            upload_file.write_text(\"hello\")\\n\\n            timestamp = get_timestamp()\\n\\n            # set env var and run plugin\\n            try:\\n                os.environ[\"PYWAGGLE_LOG_DIR\"] = str(dir)\\n                with Plugin() as plugin:\\n                    plugin.publish(\"test\", 123, timestamp=timestamp)\\n                    plugin.publish(\\n                        \"test.with.meta\",\\n                        456,\\n                        meta={\"user\": \"data\"},\\n                        timestamp=timestamp + 10000,\\n                    )\\n                    plugin.upload_file(upload_file, timestamp=timestamp + 20000)\\n            finally:\\n                del os.environ[\"PYWAGGLE_LOG_DIR\"]\\n\\n            df = sage_data_client.load(Path(dir, \"data.ndjson\"))\\n\\n            # ensure records match what was published\\n            self.assertEqual(len(df), 3)\\n\\n            # TODO(sean) test timestamps\\n            self.assertEqual(df.loc[0, \"name\"], \"test\")\\n            self.assertEqual(df.loc[0, \"value\"], 123)\\n\\n            self.assertEqual(df.loc[1, \"name\"], \"test.with.meta\")\\n            self.assertEqual(df.loc[1, \"value\"], 456)\\n            self.assertEqual(df.loc[1, \"meta.user\"], \"data\")\\n\\n            self.assertEqual(df.loc[2, \"name\"], \"upload\")\\n            self.assertEqual(df.loc[2, \"meta.filename\"], \"hello.txt\")\\n\\n            # ensure all uploads exist\\n            for path in df[df.name == \"upload\"].value:\\n                self.assertTrue(Path(path).exists())\\n\\n\\ndef assertDictContainsSubset(t, a, b):\\n    t.assertLessEqual(a.items(), b.items())\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()\\n'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='import unittest\\nfrom waggle.data.audio import AudioFolder, AudioSample\\nfrom waggle.data.vision import RGB, BGR, ImageFolder, ImageSample, resolve_device\\nfrom waggle.data.timestamp import get_timestamp\\nimport numpy as np\\nfrom tempfile import TemporaryDirectory\\nfrom pathlib import Path\\nimport os.path\\nfrom itertools import product\\n\\n\\ndef generate_audio_data(samplerate, channels, dtype):\\n    if dtype == np.float32:\\n        return np.random.uniform(-1, 1, (samplerate, channels)).astype(dtype)\\n    if dtype == np.float64:\\n        return np.random.uniform(-1, 1, (samplerate, channels)).astype(dtype)\\n    if dtype == np.int16:\\n        return np.random.randint(\\n            -(2**15), 2**15, (samplerate, channels), dtype=dtype\\n        )\\n    if dtype == np.int32:\\n        return np.random.randint(\\n            -(2**31), 2**31, (samplerate, channels), dtype=dtype\\n        )\\n    raise ValueError(\"unsupported audio settings\")\\n\\n\\ndef generate_audio_sample(samplerate, channels, dtype):\\n    return AudioSample(generate_audio_data(samplerate, channels, dtype), samplerate, 0)\\n\\n\\nclass TestData(unittest.TestCase):\\n    def test_colors(self):\\n        for fmt in [RGB, BGR]:\\n            data = np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8)\\n            data2 = fmt.format_to_cv2(fmt.cv2_to_format(data))\\n            self.assertTrue(\\n                np.all(np.isclose(data, data2, 1.0)), f\"checking format {fmt}\"\\n            )\\n\\n    def test_resolve_device(self):\\n        self.assertEqual(\\n            resolve_device(Path(\"test.jpg\")), str(Path(\"test.jpg\").absolute())\\n        )\\n        self.assertEqual(\\n            resolve_device(\"file://path/to/test.jpg\"),\\n            str(Path(\"path/to/test.jpg\").absolute()),\\n        )\\n        self.assertEqual(\\n            resolve_device(\"http://camera-ip.org/image.jpg\"),\\n            \"http://camera-ip.org/image.jpg\",\\n        )\\n        self.assertEqual(\\n            resolve_device(\"rtsp://camera-ip.org/image.jpg\"),\\n            \"rtsp://camera-ip.org/image.jpg\",\\n        )\\n        self.assertEqual(resolve_device(0), 0)\\n\\n    def test_image_save(self):\\n        with TemporaryDirectory() as dir:\\n            sample = ImageSample(\\n                np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8), 0, RGB\\n            )\\n            for fmt in [\"jpg\", \"png\"]:\\n                name = f\"sample.{fmt}\"\\n                sample.save(os.path.join(dir, name))\\n                sample.save(Path(dir, name))\\n\\n    def test_image_save_load(self):\\n        with TemporaryDirectory() as dir:\\n            sample = ImageSample(\\n                np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8), 0, RGB\\n            )\\n            sample.save(Path(dir, \"sample.png\"))\\n            samples = ImageFolder(dir, RGB)\\n            self.assertTrue(np.allclose(sample.data, samples[0].data))\\n\\n    def test_audio_save(self):\\n        test_formats = [\"wav\", \"flac\"]\\n        test_samplerates = [22050, 44100, 48000]\\n        test_channels = [1, 2]\\n        test_dtypes = [np.float32, np.float64, np.int16, np.int32]\\n\\n        for format, samplerate, channels, dtype in product(\\n            test_formats, test_samplerates, test_channels, test_dtypes\\n        ):\\n            name = f\"sample.{format}\"\\n            with TemporaryDirectory() as dir:\\n                sample = generate_audio_sample(\\n                    samplerate, channels=channels, dtype=dtype\\n                )\\n                # test saving as any PathLike\\n                sample.save(os.path.join(dir, name))\\n                sample.save(Path(dir, name))\\n\\n    def test_audio_save_load(self):\\n        test_formats = [\"wav\", \"flac\"]\\n        test_samplerates = [22050, 44100, 48000]\\n        test_channels = [1, 2]\\n        test_dtypes = [np.float32, np.float64]\\n\\n        for format, samplerate, channels, dtype in product(\\n            test_formats, test_samplerates, test_channels, test_dtypes\\n        ):\\n            name = f\"sample.{format}\"\\n            with TemporaryDirectory() as dir:\\n                sample = generate_audio_sample(\\n                    samplerate, channels=channels, dtype=dtype\\n                )\\n                sample.save(Path(dir, name))\\n                samples = AudioFolder(dir)\\n                self.assertTrue(\\n                    np.allclose(sample.data, samples[0].data, atol=1e-4),\\n                    msg=f\"failed: format={format} samplerate={samplerate} channels={channels} dtypes={dtype}\",\\n                )\\n\\n    def test_get_timestamp(self):\\n        ts = get_timestamp()\\n        self.assertIsInstance(ts, int)\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/__init__.py'}, page_content=''),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/time.py'}, page_content=\"import time\\n\\n# BUG This *must* be addressed with the behavior written up in the plugin spec.\\n# We don't want any surprises in terms of accuraccy\\ntry:\\n    from time import time_ns as get_timestamp\\nexcept ImportError:\\n\\n    def get_timestamp():\\n        return int(time.time() * 1e9)\\n\\n\\n# NOTE to preserve the best accuracy, we implement the backwards compatible perf\\n# counter by only abstracting how to measure the duration between two times in\\n# nanoseconds\\ntry:\\n    from time import perf_counter_ns as timeit_perf_counter\\n\\n    def timeit_perf_counter_duration(start, finish):\\n        return finish - start\\n\\nexcept ImportError:\\n    from time import perf_counter as timeit_perf_counter\\n\\n    def timeit_perf_counter_duration(start, finish):\\n        return int((finish - start) * 1e9)\\n\"),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/config.py'}, page_content='from typing import NamedTuple\\n\\n\\nclass PluginConfig(NamedTuple):\\n    \"\"\"\\n    PluginConfig represents the config required to setup and run a Plugin.\\n    \"\"\"\\n\\n    # TODO generalize to support different backends\\n    username: str\\n    password: str\\n    host: str\\n    port: int\\n    app_id: str\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/uploader.py'}, page_content='import hashlib\\nimport json\\nfrom pathlib import Path\\nfrom shutil import copyfile\\nfrom .time import get_timestamp\\n\\n\\nclass Uploader:\\n    def __init__(self, root):\\n        self.root = Path(root)\\n\\n    # NOTE uploads are stored in the following directory structure:\\n    # root/\\n    #   timestamp-sha1sum/\\n    #     data\\n    #     meta\\n    def upload_file(self, path, meta={}, timestamp=None, keep=False):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n\\n        path = Path(path)\\n        checksum = sha1sum_for_file(path)\\n\\n        # create upload dir\\n        upload_dir = Path(self.root, f\"{timestamp}-{checksum}\")\\n        upload_dir.mkdir(parents=True, exist_ok=True)\\n\\n        # stage data file\\n        # NOTE we do a copy instead of move, as the upload dir may\\n        # be mounted from another disk.\\n        copyfile(path, Path(upload_dir, \"data\"))\\n        if not keep:\\n            path.unlink()\\n\\n        # stage meta file\\n        metafile = {\\n            \"timestamp\": timestamp,\\n            \"shasum\": checksum,\\n            \"labels\": {k: v for k, v in meta.items()},\\n        }\\n        metafile[\"labels\"][\"filename\"] = path.name\\n        write_json_file(Path(upload_dir, \"meta\"), metafile)\\n\\n        return upload_dir\\n\\n\\ndef sha1sum_for_file(path):\\n    h = hashlib.sha1()\\n    with open(path, \"rb\") as f:\\n        while True:\\n            chunk = f.read(32768)\\n            if chunk == b\"\":\\n                break\\n            h.update(chunk)\\n    return h.hexdigest()\\n\\n\\ndef write_json_file(path, obj):\\n    with open(path, \"w\") as f:\\n        json.dump(obj, f, separators=(\",\", \":\"), sort_keys=True)\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/__init__.py'}, page_content='from .config import PluginConfig\\nfrom .plugin import Plugin\\nfrom .uploader import Uploader\\nfrom .time import get_timestamp\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='import logging\\nfrom threading import Thread, Event\\nfrom queue import Queue, Empty\\nimport time\\nimport pika\\nimport pika.exceptions\\nimport wagglemsg\\nfrom .config import PluginConfig\\n\\n\\nlogger = logging.getLogger(__name__)\\n# pika is very verbose at DEBUG level. we turn it down here.\\nlogging.getLogger(\"pika\").setLevel(logging.CRITICAL)\\n\\n\\nclass RabbitMQPublisher:\\n    \"\"\"\\n    RabbitMQPublisher manages a connection to RabbitMQ and publishes messages from the provided queue.\\n\\n    This is done in a background thread which must be stopped by setting the provided stop Event.\\n    \"\"\"\\n\\n    def __init__(self, config: PluginConfig, messages: Queue, stop: Event):\\n        self.config = config\\n        self.params = get_connection_parameters_for_config(config)\\n        self.messages = messages\\n        self.stop = stop\\n        self.done = Event()\\n        Thread(target=self.__main).start()\\n\\n    def __main(self):\\n        logger.debug(\"publisher started.\")\\n        try:\\n            while not self.stop.is_set():\\n                try:\\n                    self.__connect_and_flush_messages()\\n                except Exception:\\n                    if logger.isEnabledFor(logging.DEBUG):\\n                        logger.exception(\"__connect_and_flush_messages exception\")\\n                    time.sleep(1)\\n        finally:\\n            self.done.set()\\n            logger.debug(\"publisher stopped.\")\\n\\n    def __connect_and_flush_messages(self):\\n        logger.debug(\"publisher connecting to rabbitmq...\")\\n        with pika.BlockingConnection(self.params) as conn, conn.channel() as ch:\\n            while not self.stop.is_set():\\n                self.__flush_messages(ch)\\n            logger.debug(\"publisher stopping...\")\\n            # attempt to flush any remaining messages\\n            self.__flush_messages(ch)\\n\\n    def __flush_messages(self, ch):\\n        while True:\\n            try:\\n                logger.debug(\"publisher checking for message...\")\\n                item = self.messages.get(timeout=1)\\n            except Empty:\\n                return\\n\\n            properties = pika.BasicProperties(\\n                delivery_mode=2, user_id=self.params.credentials.username\\n            )\\n\\n            # NOTE app_id is used by data service to validate and tag additional metadata provided by k3s scheduler.\\n            if self.config.app_id != \"\":\\n                properties.app_id = self.config.app_id\\n\\n            if logger.isEnabledFor(logging.DEBUG):\\n                logger.debug(\\n                    \"publishing message to rabbitmq: %s\", wagglemsg.load(item.body)\\n                )\\n\\n            try:\\n                ch.basic_publish(\\n                    exchange=\"to-validator\",\\n                    routing_key=item.scope,\\n                    properties=properties,\\n                    body=item.body,\\n                )\\n            except Exception:\\n                if logger.isEnabledFor(logging.DEBUG):\\n                    logger.exception(\\n                        \"basic_publish to rabbitmq failed. will requeue message...\"\\n                    )\\n                # requeue message so we can again later\\n                # NOTE(sean) this will reorder messages. if we realized we *must* preserve message\\n                # order, we must to change this to avoid subtle bugs!\\n                self.messages.put(item)\\n                # propagate error up to trigger reconnect\\n                raise\\n\\n\\nclass RabbitMQConsumer:\\n    \"\"\"\\n    RabbitMQConsumer manages a connection to RabbitMQ and puts received messages into the provided queue.\\n\\n    This is done in a background thread which must be stopped by setting the provided stop Event.\\n    \"\"\"\\n\\n    def __init__(self, topics, config: PluginConfig, messages: Queue, stop: Event):\\n        self.topics = topics\\n        self.config = config\\n        self.params = get_connection_parameters_for_config(config)\\n        self.messages = messages\\n        self.stop = stop\\n        self.done = Event()\\n        Thread(target=self.__main).start()\\n\\n    def __main(self):\\n        logger.debug(\"consumer started.\")\\n        try:\\n            while not self.stop.is_set():\\n                try:\\n                    self.__connect_and_consume_messages()\\n                except Exception:\\n                    if logger.isEnabledFor(logging.DEBUG):\\n                        logger.exception(\"__connect_and_consume_messages exception\")\\n                    time.sleep(1)\\n        finally:\\n            self.done.set()\\n            logger.debug(\"consumer stopped.\")\\n\\n    def __connect_and_consume_messages(self):\\n        logger.debug(\"consumer connecting to rabbitmq...\")\\n        with pika.BlockingConnection(self.params) as conn, conn.channel() as ch:\\n            # setup subscriber queue and bind to topics\\n            queue = ch.queue_declare(\"\", exclusive=True).method.queue\\n            ch.basic_consume(queue, self.__process_message, auto_ack=True)\\n\\n            for topic in self.topics:\\n                ch.queue_bind(queue, \"data.topic\", topic)\\n                logger.debug(\"consumer binding queue %s to topic %s\", queue, topic)\\n\\n            def check_stop():\\n                if self.stop.is_set():\\n                    logger.debug(\"consumer stopping...\")\\n                    ch.stop_consuming()\\n                else:\\n                    conn.call_later(1, check_stop)\\n\\n            conn.call_later(1, check_stop)\\n            logger.debug(\"consumer start processing messages...\")\\n            ch.start_consuming()\\n\\n    def __process_message(self, ch, method, properties, body):\\n        try:\\n            logger.debug(\"consumer processing message %s...\", body)\\n            msg = wagglemsg.load(body)\\n        except TypeError:\\n            logger.debug(\"unsupported message type: %s %s\", properties, body)\\n            return\\n        logger.debug(\"consumer putting message in waiting queue\")\\n        self.messages.put(msg)\\n\\n\\ndef get_connection_parameters_for_config(\\n    config: PluginConfig,\\n) -> pika.ConnectionParameters:\\n    return pika.ConnectionParameters(\\n        host=config.host,\\n        port=config.port,\\n        credentials=pika.PlainCredentials(\\n            username=config.username,\\n            password=config.password,\\n        ),\\n        connection_attempts=1,\\n        socket_timeout=1.0,\\n    )\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='import logging\\nimport re\\nimport wagglemsg\\n\\nfrom contextlib import contextmanager\\nfrom datetime import datetime\\nfrom os import getenv\\nfrom pathlib import Path\\nfrom queue import Queue, Empty\\nfrom threading import Event\\nfrom typing import NamedTuple\\n\\nfrom .config import PluginConfig\\nfrom .rabbitmq import RabbitMQPublisher, RabbitMQConsumer\\nfrom .time import get_timestamp, timeit_perf_counter, timeit_perf_counter_duration\\nfrom .uploader import Uploader\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass PublishData(NamedTuple):\\n    scope: str\\n    body: bytes\\n\\n\\n# Nanoseconds since epoch for 2000-01-01T00:00:00Z\\nMIN_TIMESTAMP_NS = 946706400000000000\\n\\n\\nclass FilesystemPublisher:\\n    def __init__(self, root):\\n        self.root = Path(root)\\n        self.root.mkdir(parents=True, exist_ok=True)\\n        self.datafile = Path(root, \"data.ndjson\").open(\"a\")\\n        self.uploads_dir = Path(root, \"uploads\")\\n        self.uploads_dir.mkdir(parents=True, exist_ok=True)\\n\\n    def close(self):\\n        self.datafile.close()\\n\\n    def publish(self, msg: wagglemsg.Message):\\n        import json\\n\\n        out = {\\n            \"name\": msg.name,\\n            \"value\": msg.value,\\n            \"meta\": msg.meta,\\n            # python doesn\\'t have builtin support for nanosecond\\n            \"timestamp\": isoformat_time_ns(msg.timestamp),\\n        }\\n        print(\\n            json.dumps(out, sort_keys=True, separators=(\",\", \":\")),\\n            file=self.datafile,\\n            flush=True,\\n        )\\n\\n    def upload_file(self, path, timestamp, meta):\\n        from shutil import copyfile\\n\\n        src = Path(path)\\n        dst = Path(self.uploads_dir, f\"{timestamp}-{src.name}\")\\n        copyfile(src, dst)\\n        meta = meta.copy()\\n        meta[\"filename\"] = Path(src).name\\n        self.publish(\\n            wagglemsg.Message(\\n                name=\"upload\",\\n                value=str(dst.absolute()),\\n                meta=meta,\\n                timestamp=timestamp,\\n            )\\n        )\\n\\n\\ndef isoformat_time_ns(ns: int) -> str:\\n    # python doesn\\'t have builtin support for nanosecond timestamps and formatting, so we provide\\n    # a backfill for it. this is only intended to be used in the run log for testing.\\n    nanostr = f\"{ns%1000:03d}\"\\n    return datetime.fromtimestamp(ns / 1e9).isoformat() + nanostr\\n\\n\\nclass Plugin:\\n    \"\"\"\\n    Plugin provides methods to publish and consume messages inside the Waggle ecosystem.\\n\\n    Examples\\n    --------\\n\\n    The simplest example is creating a Plugin and publishing a message. This can be done using:\\n\\n    ```python\\n    from waggle.plugin import Plugin\\n\\n    with Plugin() as plugin:\\n        plugin.publish(\"test_value\", 99)\\n    ```\\n    \"\"\"\\n\\n    def __init__(\\n        self, config=None, uploader=None, file_publisher: FilesystemPublisher = None\\n    ):\\n        self.config = config or get_default_plugin_config()\\n        self.uploader = uploader or get_default_plugin_uploader()\\n        self.send = Queue()\\n        self.recv = Queue()\\n        self.stop = Event()\\n        self.tasks = []\\n\\n        # TODO(sean) can we use ExitStack to clean up???\\n\\n        self.file_publisher = file_publisher\\n\\n        if self.file_publisher is None and getenv(\"PYWAGGLE_LOG_DIR\") is not None:\\n            self.file_publisher = FilesystemPublisher(getenv(\"PYWAGGLE_LOG_DIR\"))\\n\\n    def __enter__(self):\\n        self.tasks.append(RabbitMQPublisher(self.config, self.send, self.stop))\\n        return self\\n\\n    def __exit__(self, exc_type, exc_value, exc_traceback):\\n        self.stop.set()\\n\\n        if self.file_publisher is not None:\\n            self.file_publisher.close()\\n\\n        for task in self.tasks:\\n            task.done.wait()\\n\\n    def subscribe(self, *topics):\\n        self.tasks.append(RabbitMQConsumer(topics, self.config, self.recv, self.stop))\\n        # TODO(sean) add mock or integration testing against rabbitmq to actually test this\\n\\n    def get(self, timeout=None):\\n        try:\\n            return self.recv.get(timeout=timeout)\\n        except Empty:\\n            pass\\n        raise TimeoutError(\"plugin get timed out\")\\n\\n    def publish(self, name, value, meta={}, timestamp=None, scope=\"all\", timeout=None):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n        raise_for_invalid_publish_name(name)\\n        self.__publish(name, value, meta, timestamp, scope, timeout)\\n\\n    # NOTE __publish is used internally by publish and upload_file to do an unchecked\\n    # message publish. the main reason this exists is to guard against reserved names\\n    # like \"upload\" in publish but still allow upload_file to use it.\\n    def __publish(self, name, value, meta, timestamp, scope=\"all\", timeout=None):\\n        if not isinstance(value, (int, float, str)):\\n            raise TypeError(\"Value must be an int, float or str.\")\\n        if not isinstance(timestamp, int):\\n            raise TypeError(\\n                \"Timestamp must be an int and have units of nanoseconds since epoch. Please see the documentation for more information on setting timestamps.\"\\n            )\\n        if timestamp < MIN_TIMESTAMP_NS:\\n            raise ValueError(\\n                \"Timestamp probably has wrong units and is being processed as before 2000-01-01T00:00:00Z. Timestamp must have units of nanoseconds since epoch. Please see the documentation for more information on setting timestamps.\"\\n            )\\n        if not valid_meta(meta):\\n            raise TypeError(\"Meta must be a dictionary of strings to strings.\")\\n        msg = wagglemsg.Message(name=name, value=value, timestamp=timestamp, meta=meta)\\n\\n        # hack to use file publisher for everything except uploads\\n        if self.file_publisher is not None and name != \"upload\":\\n            self.file_publisher.publish(msg)\\n\\n        logger.debug(\"adding message to outgoing queue: %s\", msg)\\n        self.send.put(PublishData(scope, wagglemsg.dump(msg)), timeout=timeout)\\n\\n    def upload_file(self, path, meta={}, timestamp=None, keep=False):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n\\n        if self.file_publisher is not None:\\n            self.file_publisher.upload_file(path, meta=meta, timestamp=timestamp)\\n\\n        if self.uploader is not None:\\n            meta = meta.copy()\\n            meta[\"filename\"] = Path(path).name\\n            upload_path = self.uploader.upload_file(\\n                path=path, meta=meta, timestamp=timestamp, keep=keep\\n            )\\n            self.__publish(\"upload\", upload_path.name, meta, timestamp)\\n\\n    @contextmanager\\n    def timeit(self, name):\\n        logger.debug(\"starting timeit block %s\", name)\\n        start = timeit_perf_counter()\\n        yield\\n        finish = timeit_perf_counter()\\n        duration = timeit_perf_counter_duration(start, finish)\\n        self.publish(name, duration)\\n        logger.debug(\"finished timeit block %s\", name)\\n\\n\\ndef get_default_plugin_config() -> PluginConfig:\\n    return PluginConfig(\\n        username=getenv(\"WAGGLE_PLUGIN_USERNAME\", \"plugin\"),\\n        password=getenv(\"WAGGLE_PLUGIN_PASSWORD\", \"plugin\"),\\n        host=getenv(\"WAGGLE_PLUGIN_HOST\", \"rabbitmq\"),\\n        port=int(getenv(\"WAGGLE_PLUGIN_PORT\", 5672)),\\n        app_id=getenv(\"WAGGLE_APP_ID\", \"\"),\\n    )\\n\\n\\ndef valid_meta(meta):\\n    return isinstance(meta, dict) and all(isinstance(v, str) for v in meta.values())\\n\\n\\ndef get_default_plugin_uploader():\\n    if (\\n        getenv(\"WAGGLE_PLUGIN_UPLOAD_PATH\") is None\\n        and getenv(\"PYWAGGLE_LOG_DIR\") is not None\\n    ):\\n        return None\\n    return Uploader(Path(getenv(\"WAGGLE_PLUGIN_UPLOAD_PATH\", \"/run/waggle/uploads\")))\\n\\n\\npublish_name_part_pattern = re.compile(\"^[a-z0-9_]+$\")\\n\\n\\ndef raise_for_invalid_publish_name(s: str):\\n    if not isinstance(s, str):\\n        raise TypeError(f\"publish name must be a string: {s!r}\")\\n    if len(s) > 128:\\n        raise ValueError(f\"publish must be at most 128 characters: {s!r}\")\\n    if s == \"upload\":\\n        raise ValueError(f\"name {s!r} is reserved for system use only\")\\n    parts = s.split(\".\")\\n    for p in parts:\\n        if not publish_name_part_pattern.match(p):\\n            raise ValueError(\\n                f\"publish name invalid: {s!r} part: {p!r} (names must consist of [a-z0-9_] and may be joined by .)\"\\n            )\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='import cv2\\nfrom pathlib import Path\\nimport numpy\\nfrom typing import Union\\nimport os\\nfrom os import PathLike\\nimport random\\nimport json\\nimport re\\nimport threading\\nimport time\\nfrom base64 import b64encode\\nfrom .timestamp import get_timestamp\\nfrom shutil import which\\nimport ffmpeg\\nimport logging\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass BGR:\\n    @classmethod\\n    def cv2_to_format(cls, data):\\n        return data\\n\\n    @classmethod\\n    def format_to_cv2(cls, data):\\n        return data\\n\\n\\nclass RGB:\\n    @classmethod\\n    def cv2_to_format(cls, data):\\n        return cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\\n\\n    @classmethod\\n    def format_to_cv2(cls, data):\\n        return cv2.cvtColor(data, cv2.COLOR_RGB2BGR)\\n\\n\\nWAGGLE_DATA_CONFIG_PATH = Path(\\n    os.environ.get(\"WAGGLE_DATA_CONFIG_PATH\", \"/run/waggle/data-config.json\")\\n)\\n\\n\\ndef read_device_config(path):\\n    config = json.loads(Path(path).read_text())\\n    return {\\n        section[\"match\"][\"id\"]: section\\n        for section in config\\n        if \"id\" in section[\"match\"]\\n    }\\n\\n\\n# TODO use format spec like rgb vs bgr in config file\\nclass ImageSample:\\n    data: numpy.ndarray\\n    timestamp: int\\n    format: Union[BGR, RGB]\\n\\n    def __init__(self, data, timestamp, format):\\n        self.format = format\\n        self.data = self.format.cv2_to_format(data)\\n        self.timestamp = timestamp\\n\\n    def save(self, path: PathLike):\\n        path = Path(path)\\n        data = self.format.format_to_cv2(self.data)\\n        cv2.imwrite(str(path), data)\\n\\n    def _repr_html_(self):\\n        data = self.format.format_to_cv2(self.data)\\n        ok, buf = cv2.imencode(\".png\", data)\\n        if not ok:\\n            raise RuntimeError(\"could not encode image\")\\n        b64data = b64encode(buf.ravel()).decode()\\n        return f\\'<img src=\"data:image/png;base64,{b64data}\" />\\'\\n\\n\\nclass VideoSample:\\n    path: str\\n    timestamp: int\\n\\n    def __init__(self, path, timestamp, format=RGB):\\n        self.format = format\\n        self.path = path\\n        self.timestamp = timestamp\\n        self.capture = None\\n\\n    def __enter__(self):\\n        self.capture = cv2.VideoCapture(self.path)\\n        if not self.capture.isOpened():\\n            raise RuntimeError(\\n                f\"unable to open video capture for file {self.path!r}\"\\n            )\\n        self.fps = self.capture.get(cv2.CAP_PROP_FPS)\\n        if self.fps > 100.:\\n            self.fps = 0.\\n            logger.debug(f\\'pywaggle cannot calculate timestamp because the fps ({self.fps}) is too high.\\')\\n            self.timestamp_delta = 0\\n        else:\\n            self.timestamp_delta = 1 / self.fps\\n        self._frame_count = 0\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        if self.capture.isOpened():\\n            self.capture.release()\\n\\n    def __iter__(self):\\n        self._frame_count = 0\\n        return self\\n\\n    def __next__(self):\\n        if self.capture == None or not self.capture.isOpened():\\n            raise RuntimeError(\"video is not opened. use the Python WITH statement to open the video\")\\n        ok, data = self.capture.read()\\n        if not ok or data is None:\\n            raise StopIteration\\n        # timestamp must be an integer in nanoseconds\\n        approx_timestamp = self.timestamp + int(self.timestamp_delta * self._frame_count * 1e9)\\n        self._frame_count += 1\\n        return ImageSample(data=data, timestamp=approx_timestamp, format=self.format)\\n\\n\\ndef resolve_device(device):\\n    if isinstance(device, Path):\\n        return resolve_device_from_path(device)\\n    # objects that are not paths or strings are considered already resolved\\n    if not isinstance(device, str):\\n        return device\\n    match = re.match(r\"([A-Za-z0-9]+)://(.*)$\", device)\\n    # non-url like paths refer to data shim devices\\n    if match is None:\\n        return resolve_device_from_data_config(device)\\n    # return file:// urls as path\\n    if match.group(1) == \"file\":\\n        return resolve_device_from_path(Path(match.group(2)))\\n    # return other urls as-is\\n    return device\\n\\n\\ndef resolve_device_from_path(path):\\n    return str(path.absolute())\\n\\n\\ndef resolve_device_from_data_config(device):\\n    config = read_device_config(WAGGLE_DATA_CONFIG_PATH)\\n    section = config.get(device)\\n    if section is None:\\n        raise KeyError(f\"no device found {device!r}\")\\n    try:\\n        return section[\"handler\"][\"args\"][\"url\"]\\n    except KeyError:\\n        raise KeyError(f\"missing .handler.args.url field for device {device!r}.\")\\n\\nclass Camera:\\n    INPUT_TYPE_FILE = \"file\"\\n    INPUT_TYPE_OTHER = \"other\"\\n\\n    def __init__(self, device=0, format=RGB):\\n        self.capture = _Capture(resolve_device(device), format)\\n        match = re.match(r\"([A-Za-z0-9]+)://(.*)$\", device)\\n        if match is not None and match.group(1) == \"file\":\\n            self.input_type = self.INPUT_TYPE_FILE\\n        else:\\n            self.input_type = self.INPUT_TYPE_OTHER\\n\\n    def __enter__(self):\\n        if self.input_type == self.INPUT_TYPE_FILE:\\n            logger.info(f\\'input is a file. the background thread disabled for grabbing frames\\')\\n            self.capture.enable_daemon = False\\n        else:\\n            self.capture.enable_daemon = True\\n        self.capture.__enter__()\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.capture.__exit__(exc_type, exc_val, exc_tb)\\n\\n    def snapshot(self):\\n        with self.capture:\\n            return self.capture.snapshot()\\n\\n    def stream(self):\\n        with self.capture:\\n            yield from self.capture.stream()\\n\\n    def record(self, duration, file_path=\"./sample.mp4\", skip_second=1):\\n        return self.capture.record(duration, file_path, skip_second)\\n\\n\\nclass _Capture:\\n    def __init__(self, device, format):\\n        self.device = device\\n        self.format = format\\n        self.context_depth = 0\\n        self.enable_daemon = False\\n        self.daemon_need_to_stop = threading.Event()\\n        self._ready_for_next_frame = threading.Event()\\n        self.daemon = threading.Thread(target=self._run, daemon=True)\\n        self.lock = threading.Lock()\\n\\n    def __enter__(self):\\n        if self.context_depth == 0:\\n            self.capture = cv2.VideoCapture(self.device)\\n            if not self.capture.isOpened():\\n                raise RuntimeError(\\n                    f\"unable to open video capture for device {self.device!r}\"\\n                )\\n            # spin up a thread to keep up with the camera frame rate\\n            if self.enable_daemon:\\n                self.daemon_need_to_stop.clear()\\n                self.daemon.start()\\n        self.context_depth += 1\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.context_depth -= 1\\n        if self.context_depth == 0:\\n            if self.enable_daemon:\\n                self.daemon_need_to_stop.set()\\n            self.capture.release()\\n    \\n    def _run(self):\\n        # we sleep slighly shorter than FPS to drain the buffer efficiently\\n        # NOTE: OpenCV\\'s FPS get function is inaccurate as a USB webcam gives 1 FPS while\\n        #       a RTSP stream returns 180000. none of them are correct. therefore, we cannot\\n        #       decide the sleep time based on obtained FPS\\n        # fps = self.capture.get(cv2.CAP_PROP_FPS)\\n        sleep = 0.01\\n        # if fps > 0 and fps < 100:\\n        #    sleep = 1 / (fps + 1)\\n        # logging.debug(f\\'camera FPS is {fps}. the background thread sleeps {sleep} seconds in between grab()\\')\\n        while not self.daemon_need_to_stop.is_set():\\n            try:\\n                self.lock.acquire()\\n                ok = self.capture.grab()\\n                if not ok:\\n                    raise RuntimeError(\"failed to grab a frame\")\\n                self.timestamp = get_timestamp()\\n            finally:\\n                self.lock.release()\\n            self._ready_for_next_frame.set()\\n            time.sleep(sleep)\\n\\n    def grab_frame(self):\\n        if self.daemon.is_alive():\\n            if not self._ready_for_next_frame.wait(timeout=10.):\\n                raise RuntimeError(\"failed to grab a frame from the background thread: timed out\")\\n            self._ready_for_next_frame.clear()\\n            try:\\n                self.lock.acquire(timeout=1)\\n                timestamp = self.timestamp\\n                ok, data = self.capture.retrieve()\\n                if not ok:\\n                    raise RuntimeError(\"failed to retrieve the taken snapshot\")\\n            finally:\\n                self.lock.release()\\n            return ImageSample(data=data, timestamp=timestamp, format=self.format)\\n        else:\\n            ok = self.capture.grab()\\n            if not ok:\\n                raise RuntimeError(\"failed to take a snapshot\")\\n            timestamp = get_timestamp()\\n            ok, data = self.capture.retrieve()\\n            if not ok:\\n                raise RuntimeError(\"failed to retrieve the taken snapshot\")\\n            return ImageSample(data=data, timestamp=timestamp, format=self.format)\\n\\n    def snapshot(self):\\n        return self.grab_frame()\\n\\n    def stream(self):\\n        try:\\n            while True:\\n                yield self.grab_frame()\\n        except:\\n            pass\\n\\n    def record(self, duration, file_path=\"./sample.mp4\", skip_second=1):\\n        if which(\"ffmpeg\") == None:\\n            raise RuntimeError(\"ffmpeg does not exist to record video. please install ffmpeg\")\\n        if self.context_depth > 0:\\n            raise RuntimeError(f\\'the stream {self.device} is already open. please close first or use without the Python\\\\\\'s WITH statement\\')\\n        if isinstance(self.device, str) and self.device.startswith(\"rtsp\"):\\n            c = ffmpeg.input(self.device, rtsp_transport=\"tcp\", ss=skip_second)\\n        else:\\n            c = ffmpeg.input(self.device, ss=skip_second)\\n        c = ffmpeg.output(c, file_path, codec=\"copy\", f=\\'mp4\\', t=duration).overwrite_output()\\n        timestamp = get_timestamp()\\n        _, stderr = ffmpeg.run(c, quiet=True)\\n        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\\n            return VideoSample(path=file_path, timestamp=timestamp)\\n        else:\\n            raise RuntimeError(f\\'error while recording: {stderr}\\')\\n        \\n\\n\\nclass ImageFolder:\\n    available_formats = {\".jpg\", \".jpeg\", \".png\"}\\n\\n    def __init__(self, root, format=RGB, shuffle=False):\\n        self.files = sorted(\\n            p.absolute()\\n            for p in Path(root).glob(\"*\")\\n            if p.suffix in self.available_formats\\n        )\\n        self.format = format\\n        if shuffle:\\n            random.shuffle(self.files)\\n\\n    def __len__(self):\\n        return len(self.files)\\n\\n    def __getitem__(self, i):\\n        data = cv2.imread(str(self.files[i]))\\n        timestamp = Path(self.files[i]).stat().st_mtime_ns\\n        return ImageSample(data=data, timestamp=timestamp, format=self.format)\\n\\n    def __repr__(self):\\n        return f\"ImageFolder{self.files!r}\"\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/__init__.py'}, page_content='# Maintaining backwards compatibility for now.\\nfrom .data_shim import open_data_source\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/audio.py'}, page_content='from os import PathLike\\nfrom pathlib import Path\\nimport numpy\\nimport soundfile\\nfrom typing import NamedTuple\\nimport random\\nfrom base64 import b64encode\\nfrom io import BytesIO\\nfrom .timestamp import get_timestamp\\n\\n\\nclass AudioSample(NamedTuple):\\n    data: numpy.ndarray\\n    samplerate: int\\n    timestamp: int\\n\\n    def save(self, path: PathLike):\\n        path = Path(path)\\n        soundfile.write(str(path), self.data, self.samplerate)\\n\\n    def _repr_html_(self):\\n        with BytesIO() as buf:\\n            soundfile.write(\\n                buf, self.data, self.samplerate, format=\"flac\", closefd=False\\n            )\\n            b64data = b64encode(buf.getvalue()).decode()\\n        return f\"\"\"\\n<audio controls=\"controls\" autobuffer=\"autobuffer\">\\n<source src=\"data:audio/wav;base64,{b64data}\" />\\n</audio>\\n\"\"\"\\n\\n\\nclass Microphone:\\n    def __init__(self, samplerate=48000, channels=1, name=None):\\n        import soundcard\\n\\n        self.microphone = soundcard.default_microphone()\\n        self.samplerate = samplerate\\n        self.channels = channels\\n        self.name = name\\n\\n    def record(self, duration):\\n        timestamp = get_timestamp()\\n        data = self.microphone.record(\\n            samplerate=self.samplerate,\\n            numframes=int(duration * self.samplerate),\\n            channels=self.channels,\\n        )\\n        return AudioSample(data, self.samplerate, timestamp=timestamp)\\n\\n\\nclass AudioFolder:\\n    available_formats = {\".\" + s.lower() for s in soundfile.available_formats().keys()}\\n\\n    def __init__(self, root, shuffle=False):\\n        self.files = sorted(\\n            p.absolute()\\n            for p in Path(root).glob(\"*\")\\n            if p.suffix in self.available_formats\\n        )\\n        if shuffle:\\n            random.shuffle(self.files)\\n\\n    def __len__(self):\\n        return len(self.files)\\n\\n    def __getitem__(self, i):\\n        data, samplerate = soundfile.read(str(self.files[i]), always_2d=True)\\n        timestamp = Path(self.files[i]).stat().st_mtime_ns\\n        return AudioSample(data, samplerate, timestamp=timestamp)\\n\\n    def __repr__(self):\\n        return f\"AudioFolder{self.files!r}\"\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/timestamp.py'}, page_content='try:\\n    from time import time_ns as get_timestamp\\nexcept ImportError:\\n    from time import time\\n\\n    def get_timestamp():\\n        return int(time() * 1e9)\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='import logging\\nimport numpy as np\\nfrom urllib.request import urlopen\\nfrom threading import Thread, Event\\nfrom queue import Queue, Empty, Full\\nimport time\\nimport os\\nimport socket\\nfrom pathlib import Path\\nimport json\\nimport random\\nimport re\\n\\nlogger = logging.getLogger(__name__)\\n\\ntry:\\n    import cv2\\nexcept ImportError:\\n    logger.warning(\\n        \"cv2 module not found. pywaggle requires this to capture image and video data.\"\\n    )\\n\\n\\n# BUG This *must* be addressed with the behavior written up in the plugin spec.\\n# We don\\'t want any surprises in terms of accuraccy\\ntry:\\n    from time import time_ns\\nexcept ImportError:\\n    logger.warning(\"using backwards compatible implementation of time_ns\")\\n\\n    def time_ns():\\n        return int(time.time() * 1e9)\\n\\n\\ndef cvtColor(bgr_img, pixel_format=\"rgb\"):\\n    if pixel_format == \"rgb\":\\n        return cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\\n    return bgr_img\\n\\n\\nclass ImageHandler:\\n    def __init__(self, query, url, pixel_format=\"rgb\"):\\n        self.url = url\\n        self.pixel_format = pixel_format\\n\\n    def get(self, timeout=None):\\n        try:\\n            with urlopen(self.url, timeout=timeout) as f:\\n                data = f.read()\\n                ts = time_ns()\\n                arr = np.frombuffer(data, np.uint8)\\n                bgr_img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\\n                return ts, cvtColor(bgr_img, self.pixel_format)\\n        except socket.timeout:\\n            raise TimeoutError(\"get timed out\")\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, *exc):\\n        pass\\n\\n\\ndef video_worker(handler):\\n    try:\\n        while not handler.quit.is_set():\\n            ok, bgr_img = handler.cap.read()\\n            if not ok:\\n                break\\n            img = cvtColor(bgr_img, handler.pixel_format)\\n            item = (time_ns(), img)\\n\\n            # attempt to add an item to the queue\\n            try:\\n                handler.queue.put_nowait(item)\\n                continue\\n            except Full:\\n                logger.debug(\"video frame queue full. evicting oldest frame...\")\\n            # evict an item from queue\\n            try:\\n                handler.queue.get_nowait()\\n            except Empty:\\n                pass\\n            # queue should have space to add now. (assuming this\\n            # is the only producer adding to this queue)\\n            handler.queue.put_nowait(item)\\n    finally:\\n        handler.cap.release()\\n        handler.released.set()\\n\\n\\n# TODO We need to use a flexible model where the data returned is\\n# extensible. For example, serial data won\\'t really have a good\\n# notion of \"timestamp\". Maybe it\\'s better to not include that.\\n\\n\\nclass VideoHandler:\\n    def __init__(self, query, url, pixel_format=\"rgb\"):\\n        self.pixel_format = pixel_format\\n        self.cap = cv2.VideoCapture(url)\\n        if not self.cap.isOpened():\\n            raise RuntimeError(f\\'could not open camera at \"{url}\".\\')\\n        self.queue = Queue(8)\\n        self.quit = Event()\\n        self.released = Event()\\n        # NOTE(sean) no further mutation can be done on VideoHandler state. all\\n        # interaction with cap *must* be done in the worker thread or via queue\\n        # and quit primitives\\n        worker = Thread(target=video_worker, args=(self,), daemon=True)\\n        worker.start()\\n\\n    def get(self, timeout=None):\\n        try:\\n            return self.queue.get(timeout=timeout)\\n        except Empty:\\n            raise TimeoutError(\"get timed out\")\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, *exc):\\n        self.quit.set()\\n        self.released.wait()  # <- wait for cleanup in worker thread\\n\\n\\nWAGGLE_DATA_CONFIG_PATH = Path(\\n    os.environ.get(\"WAGGLE_DATA_CONFIG_PATH\", \"/run/waggle/data-config.json\")\\n)\\n\\ntry:\\n    config = json.loads(WAGGLE_DATA_CONFIG_PATH.read_text())\\nexcept FileNotFoundError:\\n    logger.debug(\\n        \"could not find data config file %s. using empty resource list.\",\\n        WAGGLE_DATA_CONFIG_PATH,\\n    )\\n    config = []\\n\\n\\ndef dict_is_subset(a, b):\\n    return all(k in b and re.match(b[k], a[k]) for k in a.keys())\\n\\n\\ndef find_all_matches(query):\\n    return [c for c in config if dict_is_subset(query, c[\"match\"])]\\n\\n\\ndef find_match(query):\\n    matches = find_all_matches(query)\\n    if len(matches) == 0:\\n        raise RuntimeError(\"no matches found\")\\n    if len(matches) > 1:\\n        raise RuntimeError(\"multiple devices found\")\\n    return matches[0]\\n\\n\\nhandlers = {\\n    \"image\": ImageHandler,\\n    \"video\": VideoHandler,\\n}\\n\\n\\n# optimizations *could* happen here, on demand...\\ndef open_data_source(**query):\\n    match = find_match(query)\\n    handler = handlers[match[\"handler\"][\"type\"]]\\n    args = match[\"handler\"][\"args\"]\\n    return handler(query, **args)\\n'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/measurements.py'}, page_content='from datetime import datetime\\nimport json\\nimport time\\n\\n\\nclass MeasurementsFile:\\n    def __init__(self, filename):\\n        self.records = []\\n\\n        with open(filename, \"r\") as f:\\n            for r in map(json.loads, f):\\n                # 2021-06-25T18:52:15.404690128Z\\n                r[\"timestamp\"] = datetime.strptime(\\n                    r[\"timestamp\"][:26], \"%Y-%m-%dT%H:%M:%S.%f\"\\n                )\\n                self.records.append(r)\\n        self.records.sort(key=lambda r: r[\"timestamp\"])\\n\\n    def play(self, nodelay=False):\\n        if len(self.records) == 0:\\n            return\\n        last_record = self.records[0]\\n        for r in self.records:\\n            delta = r[\"timestamp\"] - last_record[\"timestamp\"]\\n            if not nodelay:\\n                time.sleep(delta.total_seconds())\\n            yield r\\n            last_record = r\\n\\n\\n# MessagePlayer can take a SDR format file and replay the contents\\n# this will help support use cases where someone wants to inject known\\n# data into their plugin from a file.\\n# (think about name?)\\n# other features:\\n# should sort by timestamp / or leave no sort as flag?\\n# should be able to decide starting time\\n')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load all python files\n",
    "py_docs = repo_class_loader(paths, './*.py', PythonLoader)\n",
    "py_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.9 ms, sys: 49.8 ms, total: 121 ms\n",
      "Wall time: 166 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sage-data-client/examples/plotting_example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Basic Plotting Example\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import sage_data_client\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"First, we\\'ll query the last 7 days of temperature data from W022\\'s BME680 sensor.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df = sage_data_client.query(\\\\n\\', \\'    start=\"-7d\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\",\\\\n\\', \\'        \"vsn\": \"W022\",\\\\n\\', \\'        \"sensor\": \"bme680\",\\\\n\\', \\'    }\\\\n\\', \\')\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"Next, we\\'ll plot a simple line chart.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df.set_index(\"timestamp\").value.plot()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"Finally, we\\'ll plot the temperature distribution.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.hist(bins=100)\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Geospatial Mapping Example\\\\n\\', \\'Within this example, we walk through how to query for SAGE data, filter our values, and plot maps of the data using Cartopy, Matplotlib, and hvPlot!\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'## Imports\\\\n\\', \\'We import our sage_data_client, along with the plotting libraries matplotlib, Cartopy, hvPlot and holoviews.\\\\n\\', \\'\\\\n\\', \\'If you have not installed these packages already, make sure to run this line!\\']\\'\\n\\n\\'code\\' cell: \\'[\\'!pip3 install matplotlib bokeh holoviews hvplot cartopy pandas metpy\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import sage_data_client\\\\n\\', \\'from bokeh.models.formatters import DatetimeTickFormatter\\\\n\\', \\'import hvplot.pandas\\\\n\\', \\'import holoviews as hv\\\\n\\', \\'import cartopy.crs as ccrs\\\\n\\', \\'import cartopy.feature as cfeature\\\\n\\', \\'import matplotlib.pyplot as plt\\\\n\\', \\'from metpy.plots import USCOUNTIES\\\\n\\', \\'import pandas as pd\\\\n\\', \\'import warnings\\\\n\\', \\'\\\\n\\', \\'warnings.filterwarnings(\"ignore\")\\\\n\\', \\'hv.extension(\"bokeh\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Query and load data into pandas data frame\\\\n\\', \\'We have two queries we are interested in:\\\\n\\', \\'- Temperature\\\\n\\', \\'- Location data (latitude and longitude of the sensor)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'temperature_df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\",\\\\n\\', \\'    }\\\\n\\', \\')\\\\n\\', \\'\\\\n\\', \\'location_df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"sys.gps.*\",\\\\n\\', \\'    }\\\\n\\', \\')\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Investigate the Temperature Dataframe\\\\n\\', \\'Notice how the dataframe containing temperature data stores the temperature value as `value`, along with several `meta.` fields.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'temperature_df\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Investigate the Location Dataframe\\\\n\\', \\'This dataframe does not have as many metadata fields... but we do have enough to join our two dataframes. Another issue with this dataframe is that the location values are stored as individual rows, when we would ideally like these to be their own columns (ex. latitude and longitude for a given location)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'location_df\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Clean Up the Data\\\\n\\', \\'\\\\n\\', \"Let\\'s start the data cleaning process!\\\\n\", \\'\\\\n\\', \\'### Join Latitude and Longitude into the Same Rows\\\\n\\', \\'Our first step is to join our latitude and longitude values into the same row. We start by\\\\n\\', \\'- Subsetting for latitude and longitude in the dataframe\\\\n\\', \\'- Rename the fields accordingly\\\\n\\', \\'- Join the columns based on the timestamp, host, and node\\\\n\\', \\'- Drop any extra columns\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Subset the latitude values, and rename to latitude\\\\n\\', \"lat = location_df.loc[location_df.name == \\'sys.gps.lat\\']\\\\n\", \"lat_df = lat.rename(columns={\\'value\\':\\'latitude\\'})\\\\n\", \\'\\\\n\\', \\'# Subset the longitude values, and rename to latitude\\\\n\\', \"lon = location_df.loc[location_df.name == \\'sys.gps.lon\\']\\\\n\", \"lon_df = lon.rename(columns={\\'value\\':\\'longitude\\'})\\\\n\", \\'\\\\n\\', \\'# Join the latitude and longitude dataframes, returning a dataframe with shared latitude and longitude information\\\\n\\', \"joined_lats_lons = pd.merge(lat_df, lon_df, on=[\\'timestamp\\', \\'meta.host\\', \\'meta.node\\'],  how=\\'outer\\', )\\\\n\", \\'\\\\n\\', \\'# Filter out unwanted columns\\\\n\\', \"joined_lats_lons = joined_lats_lons[[x for x in joined_lats_lons.columns if ((\\'x\\' not in x) and (\\'y\\' not in x) and (\\'timestamp\\' not in x))]]\\\\n\", \\'\\\\n\\', \\'# Return our dataframe\\\\n\\', \\'joined_lats_lons\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Join our Latitude and Longitude Information with the Temperature Dataframe\\\\n\\', \\'Now that we have our location dataframe cleaned up, we can join this with the temperature dataframe, so we know where our temperature values are collected!\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Merge the dataframes using the host and node as the shared fields to join on\\\\n\\', \"df = pd.merge(temperature_df, joined_lats_lons,  on=[\\'meta.host\\', \\'meta.node\\'], how=\\'right\\')\\\\n\", \\'\\\\n\\', \\'# Drop any duplicates, based on the timestamp, host, and node\\\\n\\', \"df_filtered = df.drop_duplicates([\\'timestamp\\', \\'meta.host\\', \\'meta.node\\'])\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Run Statistics on our Dataframe\\\\n\\', \"Let\\'s say we are interested in hourly mean temperature... we can calculate that!\"]\\'\\n\\n\\'code\\' cell: \\'[\\'hours = df_filtered.timestamp.dt.hour.unique()\\']\\'\\n\\n\\'code\\' cell: \\'[\\'hourly_mean = df_filtered.groupby([df_filtered.timestamp.dt.hour,\\\\n\\', \"                                   \\'meta.node\\']).mean()\"]\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \\'    fig = plt.figure(figsize=(14,8))\\\\n\\', \\'    ax = plt.subplot(projection=ccrs.PlateCarree())\\\\n\\', \"    s = hourly_mean.loc[hour].plot.scatter(ax=ax, x=\\'longitude\\', y=\\'latitude\\', c=\\'value\\', cmap=\\'Spectral_r\\')\\\\n\", \\'    ax.add_feature(cfeature.LAND)\\\\n\\', \\'    ax.add_feature(cfeature.STATES)\\\\n\\', \\'    ax.add_feature(cfeature.LAKES)\\\\n\\', \\'    ax.set_extent([-125, -66.5, 20, 50])\\\\n\\', \\'    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\\\\n\\', \"                  linewidth=2, color=\\'gray\\', alpha=0.5, linestyle=\\'--\\')\\\\n\", \\'    gl.top_labels = False\\\\n\\', \\'    gl.right_labels = False\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    plt.title(f\\'Average Temperature (degF) at \\\\\\\\n {time_label}\\', fontsize=16);\\\\n\", \\'    plt.show()\\\\n\\', \\'    plt.close()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Zoom in on the Chicago Area\\\\n\\', \"It\\'s nice having a national map, but let\\'s zoom into Chicago for a higher resolution view of the sensors around the city and surrounding suburbs.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \\'    fig = plt.figure(figsize=(14,8))\\\\n\\', \\'    ax = plt.subplot(projection=ccrs.PlateCarree())\\\\n\\', \"    s = hourly_mean.loc[hour].plot.scatter(ax=ax, x=\\'longitude\\', y=\\'latitude\\', c=\\'value\\', cmap=\\'Spectral_r\\', vmin=28, vmax=38)\\\\n\", \\'    ax.add_feature(cfeature.LAND)\\\\n\\', \\'    ax.add_feature(cfeature.STATES)\\\\n\\', \\'    ax.add_feature(cfeature.LAKES)\\\\n\\', \\'    ax.add_feature(USCOUNTIES)\\\\n\\', \\'    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\\\\n\\', \"                  linewidth=2, color=\\'gray\\', alpha=0.5, linestyle=\\':\\')\\\\n\", \\'    gl.top_labels = False\\\\n\\', \\'    gl.right_labels = False\\\\n\\', \\'    ax.set_extent([-89, -87, 41, 43])\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    plt.title(f\\'Average Temperature (degF) at \\\\\\\\n {time_label}\\', fontsize=16);\\\\n\", \\'    plt.show()\\\\n\\', \\'    plt.close()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Plot Interactive National Maps\\\\n\\', \\'We can use hvPlot here to plot interactive national maps\\']\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    display(hourly_mean.loc[hour].hvplot.points(x=\\'longitude\\',\\\\n\", \"                                                y=\\'latitude\\',\\\\n\", \"                                                color=\\'value\\',\\\\n\", \"                                                cmap=\\'Spectral_r\\',\\\\n\", \\'                                                geo=True,\\\\n\\', \"                                                tiles=\\'CartoLight\\',\\\\n\", \"                                                title=f\\'Average Temperature (degF) at {time_label}\\',\\\\n\", \"                                                clabel=\\'Temperature (degF)\\',\\\\n\", \\'                                                crs=ccrs.PlateCarree()))\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Plot an Interactive Regional Map of Chicago\\\\n\\', \\'And the same for the region around Chicago\\\\n\\', \\'\\\\n\\', \\'We start first by subsetting points out of our dataframe around the Northern Illinois area.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'illinois_points = hourly_mean.loc[(hourly_mean.latitude > 41.) & (hourly_mean.longitude > -89)]\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'Now that we have this, we loop through and plot our data!\\']\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    display(illinois_points.loc[hour].hvplot.points(x=\\'longitude\\',\\\\n\", \"                                                    y=\\'latitude\\',\\\\n\", \"                                                    color=\\'value\\',\\\\n\", \"                                                    cmap=\\'Spectral_r\\',\\\\n\", \"                                                    tiles=\\'CartoLight\\',\\\\n\", \\'                                                    geo=True,\\\\n\\', \"                                                    title=f\\'Average Temperature (degF) at {time_label}\\',\\\\n\", \"                                                    clabel=\\'Temperature (degF)\\'))\"]\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/sage-interactive-plot.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Interactive Plotting Example\\\\n\\', \\'Within this example, we walk through how to query for SAGE data, filter our values, and plot using the hvPlot interactive plotting library!\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Imports and Query\\\\n\\', \\'We import our `sage_data_client`, along with the plotting libraries `hvPlot` and `holoviews`\\\\n\\', \\'\\\\n\\', \\'**If you have not installed these packages already, make sure to run this line!**\\']\\'\\n\\n\\'code\\' cell: \\'[\\'!pip3 install matplotlib bokeh holoviews hvplot\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import sage_data_client\\\\n\\', \\'from bokeh.models.formatters import DatetimeTickFormatter\\\\n\\', \\'import hvplot.pandas\\\\n\\', \\'import holoviews as hv\\\\n\\', \\'import warnings\\\\n\\', \\'\\\\n\\', \\'# query and load data into pandas data frame\\\\n\\', \\'df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\"\\\\n\\', \\'    }\\\\n\\', \\')\\\\n\\', \\'\\\\n\\', \\'warnings.filterwarnings(\"ignore\")\\\\n\\', \\'hv.extension(\"bokeh\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Clean our Data\\\\n\\', \\'When we first visualize our dataset (temperature), notice how we have some **very** low values (< -100 degrees Fahrenheit).\\']\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.plot();\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'We can flag these values as bad data using the following:\\']\\'\\n\\n\\'code\\' cell: \\'[\\'df = df[df.value > -100]\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'Now, if we visualize our data again, notice how we do not have these abnormally low values.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.plot();\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Create an Interactive Plot\\\\n\\', \\'We can use hvPlot to plot our data! Instead of using `.plot()` like we did before, which creates a matplotlib static plot, we can use `.hvplot()` which creates an interactive plot. \\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Set\\\\n\\', \\'formatter = DatetimeTickFormatter(days=\"%d %b %Y \\\\\\\\n %H:%M UTC\",\\\\n\\', \\'                                  hours=\"%d %b %Y \\\\\\\\n %H:%M UTC\",\\\\n\\', \\'                                  minutes=\"%d %b %Y \\\\\\\\n %H:%M UTC\",)\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \"df.hvplot.line(x=\\'timestamp\\',\\\\n\", \"               y=\\'value\\',\\\\n\", \"               ylabel=\\'Temperature (degF)\\',\\\\n\", \"               xlabel=\\'Time\\',\\\\n\", \"               by=\\'meta.vsn\\',\\\\n\", \\'               groupby=[\"meta.sensor\"],\\\\n\\', \\'               xformatter=formatter, \\\\n\\', \"               color=hv.Palette(\\'Category20\\'),\\\\n\", \\'               height=400,\\\\n\\', \\'               width=600)\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load all notebook files\n",
    "ipynb_docs = repo_class_loader(paths, './*.ipynb', NotebookLoader)\n",
    "ipynb_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting\n",
    "Using specific text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 2000\n",
    "chunk_overlap = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.06 ms, sys: 769 μs, total: 4.83 ms\n",
      "Wall time: 4.88 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sage-website/docs/contact-us.md'}, page_content='sidebar_label: Contact us\\n\\nContact us\\n\\nEmail\\n\\nFor support, general questions, or comments, you can always reach us at:\\n\\nsupport@waggle-edge.ai\\n\\nMessage Board\\n\\nWe also encourage developers and users to start a new topic or issue on the Waggle sensor message board:\\n\\nGitHub Discussions'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/sesctl.md'}, page_content=\"sidebar_label: sesctl sidebar_position: 2\\n\\nsesctl: a tool to schedule jobs in Waggle edge computing\\n\\nThe tool sesctl is a command-line tool that communicates with an Edge scheduler in the cloud to manage user jobs. Users can create, edit, submit, suspend, and remove jobs via the tool.\\n\\nInstallation\\n\\nThe tool can be downloaded from the edge scheduler repository and be run on person's desktop or laptop.\\n\\n:::note Please make sure to download the correct version of the tool based on the system architecture. For example, if you run it on a Mac download sesctl-darwin-amd64. :::\\n\\nbash chmod +x sesctl-<system>-<arch> ln sesctl-<system>-<arch> sesctl sesctl\\n\\nSubmit a job\\n\\nYou can follow the tutorial to submit an example job to understand how to design your own job.\\n\\nFor more tutorials\\n\\nThe in-depth tutorials on the functionalities that sesctl offers can be found in the README.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/pluginctl.md'}, page_content='sidebar_label: pluginctl sidebar_position: 1\\n\\npluginctl: a tool to develop and test plugins on a node\\n\\nWe developed the tool pluginctl to help end users develop and test their edge application (i.e., plugin) on a node before registering the plugin in Edge code repository. The tool helps on simplifying the process of testing the edge code and making changes as needed for development, by buildig the code into a container, running the container inside the node, and checking the result from the container.\\n\\nAll of Waggle nodes have the tool already installed. For plugin developers who have access to nodes, they can simply type the following to start with once they are logged into a node, bash sudo pluginctl\\n\\nThe in-depth tutorials on the functionalities that pluginctl offers can be found in the README.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/triggers.md'}, page_content='sidebar_label: Trigger examples sidebar_position: 3\\n\\nTrigger Examples\\n\\nThis page provides a few examples of triggers within Sage. Triggers are programs which generally use data and events from the edge or cloud to automatically drive or notify other behavior in the system.\\n\\nCloud-to-Edge Examples\\n\\nCloud-to-edge triggers are programs running in the cloud which monitor events or external data sources and then, in response, change some behavior on the nodes.\\n\\nSevere Weather Trigger\\n\\nThis example starts and stops jobs in response to severe weather events scraped from the National Weather Service API.\\n\\nWildfire Trigger\\n\\nThis example looks at results from the smoke detector job and modify its own scheduling interval in response. The concept is that as smoke is detected, we want to run more frequent detections.\\n\\nEdge-to-Cloud Examples\\n\\nEdge-to-cloud triggers are programs which monitor data published from the nodes and use it, potentially along with additional data sources, to perform some computation or actions.\\n\\nSage Data Client Batch Trigger\\n\\nThis is a simple batch trigger example of using Sage Data Client to print nodes where the internal mean temperature exceeds a threshold every 5 minutes.\\n\\nSage Data Client Stream Trigger\\n\\nThis is an example of using Sage Data Client to watch the data stream and print nodes where the internal temperature exceeds a threshold.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=\"Developer quick reference\\n\\nDisclaimer\\n\\n:warning: This is a quick-reference guide, not a complete guide to making a plugin. Use this to copy-paste commands while working on plugins and to troubleshoot them in the testing and scheduling stages. Please consult the official :green_book:Plugin Tutorials for detailed guidance.\\n\\nTips\\n\\n:information_source: Plugin=App\\n\\n:green_book: = recommended code docs and tutorials from Sage.\\n\\n:point_right: First make a minimalistic app with a core functionality to test on the node. Later you may add all the options you want.\\n\\n:point_up: Avoid making a plugin from scratch. Use another plugin or this template for your first plugin or use :new: Cookiecutter Template.\\n\\n:warning: Repository names should be all in small alphanumeric letters and - (Do not use _)\\n\\nRequirements : Install Docker, git, and Python\\n\\nComponents of a plugin\\n\\nTypical components of a Sage plugin are described below:\\n\\n1. An application\\n\\nThis is just your usual Python program, either a single .py script or a set of directories with many components (e.g. ML models, unit tests, test data, etc).\\n\\n:point_right: First do this step on your machine and perfect it until you are happy with the core functionality.\\n\\napp/app.py* : the main Python file (sometimes also named main.py) contains the code that defines the functionality of the plugin or calls other scripts to do tasks. It usually has from waggle.plugin import Plugin call to get the data from in-built sensors and publishes the output.\\n\\nNote: Variable names in plugin.publish should be descriptive and specific.\\n\\nInstall pywaggle pip3 install -U 'pywaggle[all]'\\n\\napp/test.py : optional but recommended file, contains the unit tests for the plugin.\\n\\n2. Dockerizing the app\\n\\n:point_right: Put everything in a Docker container using a waggle base image and make it work. This may require some work if libraries are not compatible. Always use the latest base images from Dockerhub\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Dockerfile* : contains instructions for building a Docker image for the plugin. It specifies the waggle base image from dockerhub, sets up the environment, installs dependencies, and sets the entrypoint for the container.\\n\\n:warning: Keep it simple ENTRYPOINT [\"python3\", \"/app/app.py\"]\\n\\nrequirements.txt* : lists the Python dependencies for the plugin. It is used by the Dockerfile to install the required packages using pip.\\n\\nbuild.sh : is an optional shell script to automate building the complicated Docker image with tags etc.\\n\\nMakefile : optional but the recommended file includes commands for building the Docker image, running tests, and deploying the plugin.\\n\\n3. ECR configs and docs\\n\\nYou can do this step (except sage.yaml) after testing on the node but before the ERC submission. :smile:\\n\\nsage.yaml* : is the configuration file useful for ECR and job submission? Most importantly it specifies the version and input arguments.\\n\\nREADME.md and ecr-meta/ecr-science-description.md* : a Markdown file describing the scientific rationale of the plugin as an extended abstract. This includes a description of the plugin, installation instructions, usage examples, data downloading code snippets, and other relevant information.\\n\\n:bulb: Keep the same text in both files and follow the template of ecr-science-description.md.\\n\\necr-meta/ecr-icon.jpg : is an icon (512px x 512px or smaller) for the plugin in the Sage portal.\\n\\necr-meta/ecr-science-image.jpg : is a key image or figure plot that best represents the scientific output of the plugin.\\n\\n:::info :green_book: Check Sage Tuorial Part1 and Part2 :::\\n\\nGetting access to the node\\n\\nFollow this page: https://portal.sagecontinuum.org/account/access to access the nodes.\\n\\nTo test your connection the first time, execute ssh waggle-dev-sshd and enter your ssh key passphrase. You should get the following output,\\n\\nEnter passphrase for key /Users/bhupendra/.ssh/id_rsa: no command provided Connection to 192.5.86.5 closed.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Enter the passphrase to continue.\\n\\nTo connect to the node, execute ssh waggle-dev-node-V032 and enter your passphrase (required twice).\\n\\nYou should see the following message,\\n\\nWe are connecting you to node V032\\n\\n:::info :green_book: See Sage Tuorial: Part 3 for details on this topic. :::\\n\\nTesting plugins on the nodes\\n\\n:::danger :warning: Do not run any app or install packages directly on the node. Use Docker container or pluginctl commands. :::\\n\\n1. Download and run it\\n\\nDownload\\n\\nIf you have not already done it, you need your plugin in a public GitHub repository at this stage.\\n\\nTo test the app on a node, go to nodes W0xx (e.g. W023) and clone your repo there using the command git clone.\\n\\nAt this stage, you can play with your plugin in the docker container until you are happy. Then if there are changes made to the plugin, I reccomend replicating the same in your local repository and pushing it to the github and node.\\n\\nor do git commit -am \\'changes from node\\' and git push -u origin main.\\n\\nHowever, before commiting from node, you must run following commands at least once in your git repository on the node. git config [--locale] user.name \"Full Name\" git config [--locale] user.email \"email@address.com\"\\n\\n:::danger :warning: Make sure your Dockerfile has a proper entrypoint or the pluginctl run will fail. :::\\n\\nTesting with Pluginctl\\n\\n:::info :green_book: For more details on this topic check pluginctl docs. :::\\n\\nThen to test execute the command sudo pluginctl build .. This will output the plugin-image registry address at the end of the build. Example: 10.31.81.1:5000/local/my-plugin-name\\n\\nTo run the plugin without input argument, use sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name>\\n\\nExecute the command with input arguments. sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name> -- -i top_camera.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='If you need GPU, use the selector sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name> -- -i top_camera.\\n\\n:exclamation: -- is a separator. After the -- all arguments are for your entrypoint i.e. app.py.\\n\\nTo check running plugins, execute sudo pluginctl ps.\\n\\nTo stop the plugin, execute sudo pluginctl rm cloud-motion.\\n\\nTo check the log pluginctl logs cloud-motion :warning:Do not forget to stop the plugins after testing or it will run forever.\\n\\nTesting USBSerial devices\\n\\n:point_right:The USBserial device template is in Cookiecutter Template. Also check wxt536 plugin.\\n\\nSteps for working with a USB serial device\\n\\nFirst, you need to confirm which computing unit the USB device is connected to, RPi or nxcore.\\n\\nThen, you add the --selector and --privileged options to the pluginctl command during testing and specifying the same in the job.yaml for scheduling.\\n\\nTo test the plugin on nxcore, which has the USB device, use the command sudo pluginctl run -n testname --selector zone=core --privileged 10.31.81.1:5000/local/plugin-name.\\n\\nThe --selector and --privileged attributes should be added to the pluginSpec in the job submission script as shown in the example YAML code.\\n\\nYou can check which computing unit is being used by the edge scheduler by running the kubectl describe pod command and checking the output.\\n\\n:warning: Re/Check that you are using the correct USB port for the device if getting empty output or folder not found error.\\n\\n2. Check if it worked?\\n\\nLogin to the Sage portal and follow the instructions from the section See Your Data on Sage Portal\\n\\n3. Check why it failed?\\n\\nWhen you encounter a failing/long pending job with an error, you can use the following steps to help you diagnose the issue:\\n\\nFirst check the Dockerfile entrypoint.\\n\\nUse the command sudo kubectl get pod to get the name of the pod associated with the failing job.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Use the command sudo kubectl logs <<POD_NAME>> to display the logs for the pod associated with the failing job. These logs will provide you with information on why the job failed.\\n\\nUse the command sudo kubectl describe pod POD_NAME to display detailed information about the pod associated with the failing job.\\n\\nThis information can help you identify any issues with the pod itself, such as issues with its configuration or resources.\\n\\nBy following these steps, you can better understand why the job is failing and take steps to resolve the issue.\\n\\n4. Troubleshooting inside the container using pluginctl\\n\\nFollow this tutorial to get in an already running container to troubleshoot the issue. If the plugin fails instantly and your are not able to get inside the container use following commands to override the entrypoint\\n\\nFirst Deploy with Custom Entrypoint --entrypoint /bin/bash: sudo pluginctl deploy -n testnc --entrypoint /bin/bash 10.31.81.1:5000/local/plugin-mobotix-scan -- -c \\'while true; do date; sleep 1; done\\' Note the -c \\'while true; do date; sleep 1; done\\' instead of your usual plugin arguments. Now if you do sudo pluginctl logs testnc you will see the logs i.e. date.\\n\\nAccess the Plugin Container: sudo pluginctl exec -ti testnc -- /bin/bash\\n\\nEdge Code Repository\\n\\nHow to get your plugin on ECR\\n\\nTo publish your Plugin on ECR, follow these steps: 1. Go to https://portal.sagecontinuum.org/apps/. 2. Click on \"Explore the Apps Portal\". 3. Click on \"My Apps\". You must be logged in to continue. 4. Click \"Create App\" and enter your Github Repo URL. 5. \\'Click \"Register and Build App\". 6. On Your app page click on the \"Tags\" tab to get the registry link when you need to run the job on the node either using pluginctl or job script. This will look like:docker pull registry.sagecontinuum.org/bhupendraraut/mobotix-move:1.23.3.2 7. Repeat the above process for updating the plugin.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=':::warning After the build process is complete, you need to make the plugin public to schedule it. :::\\n\\n:point_right: If you have skipped step 3. ECR Configs and Docs, do it before submitting it to the ECR. Ensure that your ecr-meta/ecr-science-description.md and sage.yaml files are properly configured for this process.\\n\\nVersioning your code\\n\\n:::danger You can not resubmit the plugin to ECR with the same version number again. ::: - So think about how you change it every time you resubmit to ERC and make your style of versioning. :thinking_face: - I use \\'vx.y.m.d\\' e.g. \\'v0.23.3.4\\' but then I can only have 1 version a day, so now I am thinking of adding an incremental integer to it.\\n\\nAfter ECR registry test (generally not required)\\n\\nGenerally successfully tested plugins just work. However, in case they are failing in the scheduled jobs after running for a while or after successfully running in the above tests, do the following.\\n\\nTo test a plugin on a node after it has been built on the ECR, follow these steps: sudo pluginctl run --name test-run registry.sagecontinuum.org/bhupendraraut/cloud-motion:1.23.01.24 -- -input top\\n\\nThis command will execute the plugin with the specified ECR image (version 1.23.01.24), passing the \"-input top\" argument to the plugin (Note -- after the image telling pluginctl that these arguments are for the plugin).\\n\\n:point_right: Note the use of sudo in all pluginctl and docker commands on the node.\\n\\nAssuming that the plugin has been installed correctly and the ECR image is available, running this command should test the \"test-motion\" plugin on the node.\\n\\nYou may also have to call the kubectl <POD> commands as in the testing section if this fails.\\n\\nScheduling the job\\n\\n:::warning :exclamation: If you get an error like registry does not exist in ECR, then check that your plugin is made public. :::\\n\\nFollow this link to get an understanding of how to submit a job\\n\\nHere are the parameters we set for the Mobotix sampler plugin,'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='less= -name thermalimaging registry.sagecontinuum.org/bhupendraraut/mobotix-sampler:1.22.4.13 \\\\ --ip 10.31.81.14 \\\\ -u userid \\\\ -p password \\\\ --frames 1 \\\\ --timeout 5 \\\\ --loopsleep 60 - Your science rule can be a cronjob (More information can be found here - This runs every 15 minutes \"thermalimaging\": cronjob(\"thermalimaging\", \"*/15 * * * *\"). - Use Crontab Guru. - You can also make it triggered by a value. Please read this for supported functions.\\n\\nScheduling scripts\\n\\n:sparkles: Check user friendly job submission UI.\\n\\n:green_book: Check sesctl docs for command line tool.\\n\\n:point_up: Do not use _, upper case letters or . in the job name. Use only lowercase letters, numbers and -.\\n\\n:point_up: Ensure that the plugin is set to \\'public\\' in the Sage app portal.\\n\\njob.yaml example for USB device\\n\\nyaml= name: atmoswxt plugins: - name: waggle-wxt536 pluginSpec: image: registry.sagecontinuum.org/jrobrien/waggle-wxt536:0.23.4.13 privileged: true selector: zone: core nodeTags: [] nodes: W057: true W039: true scienceRules: - \\'schedule(\"waggle-wxt536\"): cronjob(\"waggle-wxt536\", \"1/10 * * * *\")\\' successCriteria: - WallClock(\\'1day\\')\\n\\nMultiple jobs example\\n\\nIf you want to run your plugins not all at the same time. Use this example.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='```yaml= name: w030-k3s-upgrade-test plugins: - name: object-counter-bottom pluginSpec: image: registry.sagecontinuum.org/yonghokim/object-counter:0.5.1 args: - -stream - bottom_camera - -all-objects selector: resource.gpu: \"true\" - name: cloud-cover-bottom pluginSpec: image: registry.sagecontinuum.org/seonghapark/cloud-cover:0.1.3 args: - -stream - bottom_camera selector: resource.gpu: \"true\" - name: surfacewater-classifier pluginSpec: image: registry.sagecontinuum.org/seonghapark/surface_water_classifier:0.0.1 args: - -stream - bottom_camera - -model - /app/model.pth - name: avian-diversity-monitoring pluginSpec: image: registry.sagecontinuum.org/dariodematties/avian-diversity-monitoring:0.2.5 args: - --num_rec - \"1\" - --silence_int - \"1\" - --sound_int - \"20\" - name: cloud-motion-v1 pluginSpec: image: registry.sagecontinuum.org/bhupendraraut/cloud-motion:1.23.02.20 args: - --input - bottom_camera - name: imagesampler-bottom pluginSpec: image: registry.sagecontinuum.org/theone/imagesampler:0.3.1 args: - -stream - bottom_camera - name: audio-sampler pluginSpec: image: registry.sagecontinuum.org/seanshahkarami/audio-sampler:0.4.1 nodeTags: [] nodes: W030: true scienceRules: - \\'schedule(object-counter-bottom): cronjob(\"object-counter-bottom\", \"/5 * * * \")\\' - \\'schedule(cloud-cover-bottom): cronjob(\"cloud-cover-bottom\", \"01-59/5 * * * \")\\' - \\'schedule(surfacewater-classifier): cronjob(\"surfacewater-classifier\", \"02-59/5 * * * \")\\' - \\'schedule(\"avian-diversity-monitoring\"): cronjob(\"avian-diversity-monitoring\", \" * * * \")\\' - \\'schedule(\"cloud-motion-v1\"): cronjob(\"cloud-motion-v1\", \"03-59/5 * * * \")\\' - \\'schedule(imagesampler-bottom): cronjob(\"imagesampler-bottom\", \"04-59/5 * * * \")\\' - \\'schedule(audio-sampler): cronjob(\"audio-sampler\", \"/5 * * * \")\\' successCriteria: - Walltime(1day)'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='```\\n\\nhere objecct-counter runs at 0, 5, 10, etc cloud-cover: 1, 6, 11, etc. surface water: 2, 7, 12, etc. cloud-motion: 3, 8, 13, etc. image-sampl: 4, 9, 14, etc.\\n\\nDebugging failed jobs\\n\\nDo you know how to identify why a job is failing\\n\\n:sparkles: When the job failures are seen as red markers on your job page, you can click them to see the error.\\n\\nOr detail errors can be found using using sage_data_client\\n\\nRequirements: sage_data_client and utils.py\\n\\nBy specifying the plugin name and node, the following code will print out the reasons for job failure within the last 60 minutes.\\n\\n```python= from utils import *\\n\\nmynode = \"w030\"\\n\\nmyplugin = \"water\" df = fill_completion_failure(parse_events(get_data(mynode, start=\"-60m\"))) for _, p in df[df[\"plugin_name\"].str.contains(myplugin)].iterrows(): print(p[\"error_log\"])'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='```\\n\\nDownloading the data\\n\\nSage docs for accessing-data\\n\\nSee Your Data on Sage Portal\\n\\nTo check your data on Sage Portal, follow these steps: 1. Click on the Data tab at the top of the portal page. 2. Select Data Query Browser from the dropdown menu. 3. Then, select your app in the filter. This will show all the data that is uploaded by your app using the plugin.publish() and plugin.upload() methods.\\n\\nIn addition, you can data visualize as a time series and select multiple variables to visualize together in a chart, which can be useful for identifying trends or patterns.\\n\\nDownload all images with wget\\n\\nVisit https://training-data.sagecontinuum.org/\\n\\nselect the node and period for data.\\n\\nSelect the required data and download the text file urls-xxxxxxx.txt with urls\\n\\nTo select only the top camera images, use the vim command: g/^\\\\(.*top\\\\)\\\\@!.*$/d. This will delete URLs that do not contain the word \\'top\\'\\n\\nCopy the following command from the website and run it in your terminal. wget -r -N -i urls-xxxxxxx.txt\\n\\nSage data client for text data\\n\\nSage data client python Notebook Example\\n\\npypi link pip install sage-data-client\\n\\n:::info :green_book: Documentation for accessing the data. :::\\n\\nQuerying data example\\n\\nThe sage_data_client provides query() function which takes the parameters:\\n\\n```python import sage_data_client import pandas as pd\\n\\ndf = sage_data_client.query( start=\"2023-01-08T00:00:09Z\", # Start time in \"YYYY-MM-DDTHH:MM:SSZ\" or \"YYYYMMDD-HH:MM:SS\" format end=\"2024-01-08T23:23:24Z\", # End time in the same format as start time filter={ \"plugin\": \".mobotix-scan.\", # Regex for filtering by plugin name \"vsn\": \"W056\", # Specific node identifier \"name\": \"upload\", # Specific data field \"filename\": \".*_position1.nc\" # Regex for filtering filenames } )\\n\\ndf.sort_values(\\'timestamp\\') df'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='```\\n\\nFilter Criteria\\n\\nstart and end: Time should be specified in UTC, using the format YYYY-MM-DDTHH:MM:SSZ or YYYYMMDD-HH:MM:SS.\\n\\nfilter: A dictionary for additional filtering criteria. Each key is a column name in the df.\\n\\nUse regular expressions (denoted as .*pattern.*) for flexible matching within text fields like plugin or filename.\\n\\nDownloading Files\\n\\nUse additional pandas operations on df to to include only the records of interest and download the files using a function like the one provided below, which gets the URLs in the value column, using authentication.\\n\\n```python import requests import os from requests.auth import HTTPBasicAuth\\n\\nuname = \\'username\\' upass = \\'token_as_password\\'\\n\\ndef download_files(df, download_path, uname, upass): # check download directory if not os.path.exists(download_path): os.makedirs(download_path)\\n\\nfor index, row in df.iterrows(): # \\'value\\' column has url url = row[\\'value\\']\\n\\n  filename = url.split(\\'/\\')[-1]\\n\\n  # Download using credentials\\n  response = requests.get(url, auth=HTTPBasicAuth(uname, upass))\\n  if response.status_code == 200:\\n     # make the downloads path\\n     file_path = os.path.join(download_path, filename)\\n     # Write a new file\\n     with open(file_path, \\'wb\\') as file:\\n     file.write(response.content)\\n     print(f\"Downloaded {filename} to {file_path}\")\\n  else:\\n     print(f\"Failed to download {url}, status code: {response.status_code}\")\\n\\nusage\\n\\ndownload_files(df, \\'/Users/bhupendra/projects/epcape_pier/data/downloaded/nc_pos1\\', uname, upass)'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=\"```\\n\\nMore data analysis resources\\n\\nSAGE Examples\\n\\nCROCUS Cookbooks\\n\\nMiscellaneous\\n\\nFind PT Mobotix thermal camera ip on the node\\n\\nLogin to the node where the PTmobotix camera is connected. 1. run nmap -sP 10.31.81.1/24\\n\\nNmap scan report for ws-nxcore-000048B02D3AF49F (10.31.81.1) Host is up (0.0012s latency). Nmap scan report for switch (10.31.81.2) Host is up (0.0058s latency). Nmap scan report for ws-rpi (10.31.81.4) Host is up (0.00081s latency). Nmap scan report for 10.31.81.10 Host is up (0.0010s latency). Nmap scan report for 10.31.81.15 Host is up (0.00092s latency). Nmap scan report for 10.31.81.17 Host is up (0.0014s latency). Nmap done: 256 IP addresses (6 hosts up) scanned in 2.42 seconds\\n\\nFrom the output run any command for each ip e.g. curl -u admin:meinsm -X POST http://10.31.81.15/control/rcontrol?action=putrs232&rs232outtext=%FF%01%00%0F%00%00%10\\n\\nThe ip for which output is OK is the Mobotix.\\n\\nSSH 'Broken Pipe' Issue and Solution\\n\\nA 'Broken pipe' occurs when the SSH session to waggle-dev-node is inactive for longer than 10/15 minutes, resulting in a closed connection.\\n\\nclient_loop: send disconnect: Broken pipe Connection to waggle-dev-node-w021 closed by remote host. Connection to waggle-dev-node-w021 closed.\\n\\nSolution\\n\\nTo prevent the SSH session from timing out and to maintain the connection, the following configuration options can be added to the SSH config file: ```ssh\\n\\nKeep the SSH connection alive by sending a message to the server every 60 seconds\\n\\nHost * TCPKeepAlive yes ServerAliveInterval 60 ServerAliveCountMax 999 ```\"),\n",
       " Document(metadata={'source': 'sage-website/docs/installation-manuals/wsn-manual.md'}, page_content='Wild Sage Node manual\\n\\nThe Wild Sage Node \"Getting Started\" manual is a complete overview of getting started with your new WSN.\\n\\nDownload WSN manual'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='sidebar_label: Architecture sidebar_position: 2\\n\\nArchitecture\\n\\nThe cyberinfrastructure consists of coordinating hardware and software services enabling AI at the edge. Below is a quick summary of the different infrastructure pieces, starting at the highest-level and zooming into each component to understand the relationships and role each plays.\\n\\nHigh-Level Infrastructure\\n\\nThere are 2 main components of the cyberinfrastructure: - Nodes that exist at the edge - The cloud that hosts services and storage systems to facilitate running “science goals” @ the edge\\n\\nEvery edge node maintains connections to 2 core cloud components: one to a Beehive and one to a Beekeeper\\n\\nBeekeeper\\n\\nThe Beekeeper is an administrative server that allows system administrators to perform actions on the nodes such as gather health metrics and perform software updates. All nodes \"phone home\" to their Beekeeper and maintain this \"life-line\" connection.\\n\\nDetails & source code: https://github.com/waggle-sensor/beekeeper\\n\\nBeehive\\n\\nThe Node-to-Beehive connection is the pipeline for the science. It is over this connection that instructions for the node will be sent, in addition to how data is published into the Beehive storage systems from applications (plugins) running on the nodes.\\n\\nThe overall infrastructure supports multiple Beehives, where each node is associated with a single Beehive. The set of nodes associated with a Beehive creates a \"project\" where each \"project\" is separate, having its own data store, web services, etc.\\n\\nIn the example above, there are 2 nodes associated with Beehive 1, while a single node is associated with Beehive 2. With all nodes, in this example, being administered by a single Beekeeper.\\n\\nNote: the example above shows a single Beekeeper, but a second Beekeeper could have been used for administrative isolation.\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-beehive-v2\\n\\nBeehive Infrastructure'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='Looking deeper into the Beehive infrastructure, it contains 2 main components: - software services such as the Edge Scheduler (ES), Lambda Triggers (LT), data APIs, and websites/portals - data storage systems such as the Data Repository (DR) and the Edge Code Repository (ECR)\\n\\nThe Beehive is the “command center” for interacting with the Waggle nodes at the edge. Hosting websites and interfaces allowing scientists to create science goals to run plugins at the edge & browse the data produced by those plugins.\\n\\nThe software services and data storage systems are deployed within a kubernetes environment to allow for easy administration and to support running in a multiple server architecture, supporting redundancy and service replication.\\n\\nWhile the services running within Beehive are many (both graphical and REST style API interfaces), the following is an outline of the most vital.\\n\\nData Repository (DR)\\n\\nThe Data Repository is the data store for housing all the edge produced plugin data. It consists of different storage technologies (i.e. influxdb) and techniques to store simple textual data (i.e. key-value pairs) in addition to large blobular data (i.e. audio, images, video). The Data Repository additionally has an API interface for easy access to this data.\\n\\nThe data store is a time-series database of key-value pairs with each entry containing metadata about how and when the data originated @ the edge. Included in this metadata is the data collection timestamp, plugin version used to collect the data, the node the plugin was run on, and the specific compute unit within the node that the plugin was running on.\\n\\njson { \"timestamp\":\"2022-06-10T22:37:47.369013647Z\", \"name\":\"iio.in_temp_input\", \"value\":25050, \"meta\":{ \"host\":\"0000dca632ed6d06.ws-rpi\", \"job\":\"sage\", \"node\":\"000048b02d35a97c\", \"plugin\":\"plugin-iio:0.6.0\", \"sensor\":\"bme680\", \"task\":\"iio-rpi\", \"vsn\":\"W08C\" } }'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='In the above example, the value of 25050 was collected @ 2022-06-10T22:37:47.369013647Z from the bme680 sensor on node 000048b02d35a97c via the plugin-iio:0.6.0 plugin.\\n\\nNote: see the Access and use data site for more details and data access examples.\\n\\nDetails & source code: https://github.com/waggle-sensor/data-repository\\n\\nEdge Scheduler (ES)\\n\\nThe Edge Scheduler is defined as the suite of services running in Beehive that facilitate running plugins @ the edge. Included here are user interfaces and APIs for scientists to create and manage their science goals. The Edge Scheduler continuously analyzes node workloads against all the science goals to determine how the science goals are deployed to the Beehive nodes. When it is determined that a node\\'s science goals are to be updated, the Edge Scheduler interfaces with WES running on those nodes to update the node\\'s local copy of the science goals. Essentially, the Edge Scheduler is the overseer of all the Beehive\\'s nodes, deploying science goals to them to meet the scientists plugin execution objectives.\\n\\nDetails & source code: https://github.com/waggle-sensor/edge-scheduler\\n\\nEdge Code Repository (ECR)\\n\\nThe Edge Code Repository is the \"app store\" that hosts all the tested and benchmarked edge plugins that can be deployed to the nodes. This is the interface allowing users to discover existing plugins (for potential inclusion in their science goals) in addition to submitting their own. At it\\'s core, the ECR provides a verified and versioned repository of plugin Docker images that are pulled by the nodes when a plugin is to be downloaded as run-time component of a science goal.\\n\\nDetails & source code: https://github.com/waggle-sensor/edge-code-repository\\n\\nLambda Triggers (LT)\\n\\nThe Lambda Triggers service provides a framework for running reactive code within the Beehive. There are two kinds of reaction triggers considered here: From-Edge and To-Edge.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='From-Edge triggers, or messages that originate from an edge node, can be used to trigger lambda functions -- for example, if high wind velocity is detected, a function could be triggered to determine how to reconfigure sensors or launch a computation or send an alert.\\n\\nTo-Edge triggers are messages that are to change a node\\'s behavior. For example an HPC calculation or cloud-based data analysis could trigger an Edge Scheduler API call to request a science goal to be run on a particular set of edge nodes.\\n\\nDetails & source code: https://github.com/waggle-sensor/lambda-triggers\\n\\nNodes\\n\\nNodes are the edge computing component of the cyberinfrastructure. All nodes consist of 3 items: 1. Persisent storage for housing downloaded plugins and caching published data before it is transferred to the node\\'s Beehive 2. CPU and GPU compute modules where plugins are executed and perform the accelerated inferences 3. Sensors such as environment sensors, cameras and LiDAR systems\\n\\nEdge nodes enable fast computation @ the edge, leveraging the large non-volatile storage to handle caching of high frequency data (including images, audio and video) in the event the node is \"offline\" from its Beehive. Through expansion ports the nodes support the adding and removing of sensors to fully customize the node deployments for the particular deployment environment.\\n\\nOverall, even though the nodes may use different CPU architectures and different sensor configurations, they all leverage the same Waggle Edge Stack (WES) to run plugins.\\n\\nWild Sage Node (Wild Waggle Node)\\n\\nThe Wild Sage Node (or Wild Waggle Node) is a custom built weather-proof enclosure intended for remote outdoor installation. The node features software and hardware resilience via a custom operating system and custom circuit board. Internal to the node is a power supply and PoE network switch supporting the addition of sensors through standard Ethernet (PoE), USB and other embedded protocols via the node expansion ports.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content=\"The technical capabilities of these nodes consists of: - NVidia Xavier NX ARM64 Node Controller w/ 8GB of shared CPU/GPU RAM - 1 TB of NVMe storage - 4x PoE expansion ports - 1x USB2 expansion port - optional Stevenson Shield housing a RPi 4 w/ environmental sensors & microphone - optional 2nd NVidia Xavier NX ARM64 Edge Processor\\n\\nNode installation manual: https://sagecontinuum.org/docs/installation-manuals/wsn-manual\\n\\nDetails & source code: https://github.com/waggle-sensor/wild-waggle-node\\n\\nBlade Nodes\\n\\nA Blade Node is a standard commercially available server intended for use in a climate controlled machine room, or extended temperature range telecom-grade blades for harsher environments. The AMD64 based operating system supports these types of nodes, enabling the services needed to support WES.\\n\\nThe above diagram shows the basic technical configuration of a Blade Node: - Multi-core ARM64 - 32GB of RAM - Dedicated NVida T4 GPU - 1 TB of SSD storage\\n\\nNote: it is possible to add the same optional Stevenson Shield housing that is available to the Wild Sage Nodes\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-blade\\n\\nRunning plugins @ the Edge\\n\\nIncluded in the Waggle operating systems are the core components necessary to enable running plugins @ the edge. At the heart of this is k3s, which creates a protected & isolated run-time environment. This environment combined with the tools and services provided by WES enable plugin access to the node's CPU, GPU, sensors and cameras.\\n\\nWaggle Edge Stack (WES)\"),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The Waggle Edge Stack is the set of core services running within the edge node\\'s k3s run-time environment that supports all the features that plugins need to run on the Waggle nodes. The WES services coordinate with the core Beehive services to download & run scheduled plugins (including load balancing) and facilitate uploading plugin published data to the Beehive data repository. Through abstraction technologies and WES provided tools, plugins have access to sensor and camera data.\\n\\nThe above diagram demonstrates 2 plugins running on a Waggle node. Plugin 1 (\"neon-kafka\") is an example plugin that is running alongside Plugin 2 (\"data-smooth\"). In this example, \"neon-kafka\" (via the WES tools) is reading metrics from the node\\'s sensors and then publishing that data within the WES run-time environment (internal to the node). At the same time, the \"data-smooth\" plugin is subscribing to this data stream, performing some sort of inference and then publishing the inference results (via WES tools) to Beehive.\\n\\nNote: see the Edge apps guide on how to create a Waggle plugin.\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-edge-stack\\n\\nWhat is a plugin?\\n\\nPlugins are the user-developed modules that the cyberinfrastructure is designed around. At it\\'s simplest definition a \"plugin\" is code that runs @ the edge to perform some task. That task may be simply collecting sample camera images or a complex inference combining sensor data and results published from other plugins. A plugin\\'s code will interface with the edge node\\'s sensor(s) and then publish resulting data via the tools provided by WES. All developed plugins are hosted by the Beehive Edge Code Repository.\\n\\nSee how to create plugins for details.\\n\\nScience Goals'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='A \"science goal\" is a rule-set for how and when plugins are run on edge nodes. These science goals are created by scientist to accomplish a science objective through the execution of plugins in a specific manner. Goals are created, in a human language, and managed within the Beehive Edge Scheduler. It is then the cyberinfrastucture responsibility to deploy the science goals to the edge nodes and execute the goal\\'s plugins. The tutorial walks through running a science goal.\\n\\nLoRaWAN\\n\\nThe Waggle Edge Stack includes the ChirpStack software stack and other services to facilitate communication between Nodes and LoRaWAN devices. This empowers Nodes to effortlessly establish connections with wireless sensors, enabling your plugins to seamlessly access and harness valuable data from these sensors.\\n\\nTo get started using LoRaWAN, head over to the Contact Us page. A tutorial will be available soon showing you how to get started with LoRaWAN.\\n\\nThe above diagram demonstrates the hardware in Nodes and services in WES that enable Nodes to use LoRaWAN and publish the measurements to a Beehive. The following sections will explain each componenent and service.\\n\\nsource code: - wes-chirpstack - wes-chirpstack-server - wes-rabbitmq - Tracker - Lorawan Listener Plugin\\n\\nWhat is LoRaWAN?\\n\\nLoRaWAN, short for \"Long Range Wide Area Network,\" is a wireless communication protocol designed for low-power, long-range communication between IoT (Internet of Things) devices. It employs a low-power wide-area network (LPWAN) technology, making it ideal for connecting remote sensors and devices. For more information view the documentation here.\\n\\nChirpstack'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='ChirpStack is a robust and open-source LoRaWAN Network Server that enables efficient management of LoRaWAN devices, gateways, and data. Its architecture consists of several crucial components, each serving a distinct role in LoRaWAN network operations. Below, we provide a brief overview of these components along with links to ChirpStack documentation for further insights.\\n\\nChirpstack documentation\\n\\nUDP Packet Forwarder\\n\\nThe UDP Packet Forwarder is an essential component that acts as a bridge between LoRa gateways and the ChirpStack Network Server. It receives incoming packets from LoRa gateways and forwards them to the ChirpStack Gateway Bridge for further processing. To learn more about the UDP Packet Forwarder, refer to the documentation here.\\n\\nChirpStack Gateway Bridge\\n\\nThe ChirpStack Gateway Bridge is responsible for translating gateway-specific protocols into a standard format for the ChirpStack Network Server. It connects to a UDP Packet Forwader, ensuring that data is properly formatted and can be seamlessly processed by the network server. For in-depth information on the ChirpStack Gateway Bridge, explore the documentation here.\\n\\nMQTT Broker\\n\\nWES includes a MQTT (Message Queuing Telemetry Transport) broker to handle communication between various services. MQTT provides a lightweight and efficient messaging system. This service ensures that data flows smoothly between the network server, gateways, and applications. You can find detailed information about the MQTT broker integration in the ChirpStack documentation here.\\n\\nChirpStack Server'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content=\"The ChirpStack Server serves as the core component, managing device sessions, data, and application integrations. It utilizes Redis for device sessions, metrics, and caching, ensuring efficient data handling and retrieval. For persistent data storage, ChirpStack uses PostgreSQL, accommodating records for tenants, applications, devices, and more. For a comprehensive understanding of the ChirpStack Server and its associated database technologies, consult the ChirpStack documentation here.\\n\\nNOTE: Chirpstack v4 combined the application and network server into one component.\\n\\nTracker\\n\\nThe Tracker is a service designed to record the connectivity of LoRaWAN devices to the Nodes. This service uses the information received from the MQTT broker to call ChirpStack's gRPC API. The information received from the API is then used to keep the Node's manifest up-to-date. Subsequently, it forwards this updated manifest to the Beehive. For more information, view the documentation here.\\n\\nLorawan Listener Plugin\\n\\nThe LoRaWAN Listener is a plugin designed to publish measurements collected from LoRaWAN devices. It simplifies the process of extracting and publishing valuable data from these devices. For more information about the plugin view the plugin page here.\\n\\nLorawan Device Compatibility\\n\\nThe Wild Sage Node is designed to support a wide range of Lorawan devices, ensuring flexibility and adaptability for various applications. If you are wondering which Lorawan devices can be connected to a Wild Sage Node, the device must have the following tech specs:\\n\\ndesigned for US915 (902–928 MHz) frequency region.\\n\\ncompatible with Lorawan Mac versions 1.0.0 - 1.1.0\\n\\ncompatible with Chirpstack's Lorawan Network Server\\n\\nThe device supports Over-The-Air Activation (OTAA) or Activation By Personalization (ABP)\\n\\nThe device has a Lorawan device class of A, B, or C\"),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='It is important to note that all channels within the US915 frequency band are enabled in a Wild Sage Node. If you wish to learn more about our Lorawan Gateway, please visit our portal. For inquiries about supporting Lorawan regions other than US915, please Contact Us.\\n\\nDevice Examples\\n\\nWhether you are designing your own Lorawan sensor, looking for a Lorawan data logger, or seeking an off-the-shelf Lorawan device the Wild Sage Node will support it, we have examples for you:\\n\\nDesigning your own Lorawan sensor?\\n\\nArduino MKR WAN 1310\\n\\nLooking for a Lorawan data logger?\\n\\nICT International MFR Node\\n\\nLooking for an off-the-shelf Lorawan device?\\n\\nICT International SFM1X Sap Flow Meter\\n\\nSeeking Lorawan device manufacturers?\\n\\nICT International\\n\\nRAKwireless\\n\\nThe Things Network Device Marketplace\\n\\nDecentLab'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='sidebar_label: Overview sidebar_position: 1\\n\\nSage: A distributed software-defined sensor network.\\n\\nWhat is Sage?\\n\\nGeographically distributed sensor systems that include cameras, microphones, and weather and air quality stations can generate such large volumes of data that fast and efficient analysis is best performed by an embedded computer connected directly to the sensor. Sage is exploring new techniques for applying machine learning algorithms to data from such intelligent sensors and then building reusable software that can run programs within the embedded computer and transmit the results over the network to central computer servers. Distributed, intelligent sensor networks that can collect and analyze data are an essential tool for scientists seeking to understand the impacts of global urbanization, natural disasters such as flooding and wildfires, and climate change on natural ecosystems and city infrastructure.\\n\\nSage is deploying sensor nodes that support machine learning frameworks in environmental testbeds in California, Colorado, and Kansas and in urban environments in Illinois and Texas. The reusable cyberinfrastructure running on these testbeds will give climate, traffic, and ecosystem scientists new data for building models to study these coupled systems. The software components developed are open source and provide an open architecture to enable scientists from a wide range of fields to build their own intelligent sensor networks.\\n\\nPartners are deploying testbeds in Australia, Japan, UK, and Taiwan, providing scientists with even more data for analysis. The toolkit is also extending the current educational curriculum used in Chicago to inspire young people – with an emphasis on women and minorities, to pursue science, technology, and mathematics careers – by providing a platform for students to explore measurement-based science questions related to the natural and built environments.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='The data from sensors and applications is hosted in the cloud to facilitate easy data analysis.\\n\\nWho are the users?\\n\\nThe most common users have included:\\n\\nDomain scientists interested in developing edge AI applications.\\n\\nUsers interested in sensor and application-produced datasets.\\n\\nCyberinfrastructure researchers interested in platform research.\\n\\nDomain scientists interested in adding new sensors and deploying nodes to answer specific science questions.\\n\\nHow do I use the platform?\\n\\nThis depends on your desired interaction interest. The platform consists of edge compute applications which process data (ex. sensor readings, camera images, audio recordings, etc). These edge applications then produce their own data (ex. inferences) and upload the results to a cloud database. This cloud database can be accessed directly and/or additional compute can be performed on the cloud data.\\n\\nThe entry-point into learning about your interaction with the system might be best directed by getting answers (by following the links) to the question(s) you are most interested in.\\n\\nHow do I access sensors? - Want to learn about existing, supported sensors? - Do you have a new sensor that you want to write an edge application for?\\n\\nHow do I run edge apps? - Want to know how to create an edge app? - Want to know how your edge app can get access to edge sensor data? - Want to share your edge app data with other edge applications? - Want to know how to upload data to the cloud?\\n\\nHow do I access and use data? - Want to learn about how data is stored/organized? - Do you have data that is up in the cloud and want to know how to access it?\\n\\nHow do I compute in the cloud? - Want to know how to autonomously react to edge produced data? - Want to know how to trigger an HPC event? - Want to get a text message when your edge application does something cool?\\n\\nHow do I build my own device? - Want to set up your own device for local edge app development? - Want to teach AI to a classroom of students?'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='How is the cyberinfrastructure architected?\\n\\nIf you are interested in learning more about how the cyberinfrastructure works you can head on over to the Architecture Overview page.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='sidebar_position: 3\\n\\nSubmit your job\\n\\nAre you ready to deploy your plugins to measure the world? We will use edge scheduler to submit a job and demonstrate how you can deploy plugins to field-deployed Waggle nodes.\\n\\n:::caution If you have not created your account, please go to https://portal.sagecontinuum.org and sign in to create a new account with your email. Once signed in, you will be able to create and edit your jobs, but will need a permission to submit jobs to the scheduler. Please contact-us to request for the job submission permission. :::\\n\\nJobs are an instance of a science goal. They detail what needs to be accomplished on Waggle nodes. A science goal may have multiple jobs to fill the missing data to answer scientific questions of the goal. A job describes, - plugins that are registered and built in edge code repository with specification including any plugin arguments, - a list of Waggle nodes on which the plugins will be scheduled and run, - science rules describing a condition-action set that includes when the plugins should be scheduled, - conditions to determine when the job is considered as completed\\n\\nCreating and submitting jobs are an important step for successful science mission using Waggle nodes.\\n\\nCreate a job\\n\\nWe create a job file in YAML format (JSON format is also supported. Please check out details of job attributes.)\\n\\n```bash cat << EOF > myjob.yaml\\n\\nname: myjob plugins: - name: image-sampler pluginSpec: image: registry.sagecontinuum.org/theone/imagesampler:0.3.0 args: - -stream - bottom_camera nodes: W023: scienceRules: - \"schedule(image-sampler): cronjob(\\'image-sampler\\', \\' * * * \\')\" successcriteria: - WallClock(1d) EOF'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='```\\n\\nIn this example, we want to schedule a plugin named image-sampler to collect an image from the camera named bottom_camera on W023 node. As a result of the job execution, we will get images from the node\\'s camera. The job also specifies that the plugin needs to be scheduled every minute (i.e., * * * * * in crontab expression). The job completes 24 hours after the job started to run on the node.\\n\\n:::info We support human-friendly names for the sensors we host. The \"bottom_camear\" is named based on the orientation the camera is attached to the node. The full list of sensors including cameras for the W023 node can be found here :::\\n\\n:::note We currently do not check job\\'s success criteria. This means that once a job is submitted it is served forever. We will update our system to support different conditions for the success criteria attribute. :::\\n\\nUpload your job to the scheduler\\n\\nsesctl is a command-line tool to manage jobs in the scheduler. You can download the latest version from our Github repository. Please make sure you download the tool supported for your machine. For example, on Linux desktop or laptop you would download linux-amd64 version of the tool. Please see the sesctl document for more details.\\n\\n:::note Once you have contacted us for access permissions, you will need a token provided from the access page. Replace the <<user token>> below with the access token provided on this page. :::\\n\\nYou can set the SES host and user token as an environmental variable to your terminal. Please follow your shell\\'s guidance to set them properly. In Bash shell, bash export SES_HOST=https://es.sagecontinuum.org export SES_USER_TOKEN=<<user token>>\\n\\nLet\\'s ping the scheduler in the cloud, bash sesctl ping\\n\\nYou will get a response \"pong\" from the scheduler, { \"id\": \"Cloud Scheduler (cloudscheduler-sage)\", \"version\": \"0.18.0\" }\\n\\nTo create a job using the job file, bash sesctl create --file-path myjob.yaml'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='The scheduler will return a job id and the state for the job creation, bash { \"job_id\": \"56\", \"job_name\": \"myjob\", \"state\": \"Created\" }\\n\\nTo verify that we have uploaded the job, bash sesctl stat\\n\\nYou will see the job entry from the response of the command, bash JOB_ID NAME USER STATUS AGE ==================================================================== ... 56 myjob theone Created - ...\\n\\nSubmit the job\\n\\nTo submit the job,\\n\\nbash sesctl submit --job-id 56\\n\\nThe response should indicate that the job state is changed to \"Submitted\", bash { \"job_id\": \"56\", \"state\": \"Submitted\" }\\n\\n:::note You may receive a list of errors from the scheduler if the job fails to be validated. For instance, your account may not have scheduling permission on the node W023. Please consult with us for any error, especially errors related to scheduling permission on nodes in the job. :::\\n\\nCheck status of jobs\\n\\nWe check status of the job we submitted, bash sesctl stat --job-id 56\\n\\nThe tool will print details of the job, ```bash ===== JOB STATUS ===== Job ID: 56 Job Name: myjob Job Owner: Job Status: Submitted Job Starttime: 2022-10-10 02:21:37.373437 +0000 UTC\\n\\n===== SCHEDULING DETAILS ===== Science Goal ID: 45afe963-5b8b-4e15-654c-54e2946f2ddb Total number of nodes 1'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='```\\n\\nThe job status can be also shown in job status page.\\n\\nAccess to data\\n\\nA few minutes later, the W023 Waggle node would start collecting images by scheduling the plugin on the node. Collected images are transferred to Beehive for users to download.\\n\\nconsole curl -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-5m\", \"filter\": { \"task\": \"image-sampler\", \"vsn\": \"W023\", \"name\": \"upload\" } } \\'\\n\\nClean it up\\n\\nAs we approach to the end of this tutorial, we need to clean up the job because otherwise it will be served forever. To remove the job from the scheduler, ```bash\\n\\nsince the job is running, we remove the job forcefully\\n\\nsesctl rm --force 56 ```\\n\\nYou should see output that looks like, bash { \"job_id\": \"56\", \"state\": \"Removed\" }\\n\\nMore tutorials using sesctl\\n\\nMore tutorials can be found in our Github repository.\\n\\nCreating job description with advanced science rules for supporting realistic science mission\\n\\nThe science rule used in the tutorial asked the scheduler to schedule the image sampler plugin every minute. For collecting training images from a set of Waggle nodes this makes total sense with the science rule. However, users in Waggle should want more complex behaviors at the node to not only schedule plugins, but enable cloud computation triggered by sending local events to the cloud. The events and triggers can be captured by creating science rules that monitor local sensor measurement on nodes. Please visit the science rules to know more complex science rules that user can create.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-waggle.md'}, page_content='sidebar_position: 7\\n\\nBuilding your own Waggle device\\n\\nAre you a professor that wants to use affordable Waggle devices to teach students interested in AI? Are you someone interested in developing a new edge app using a local development platform? Are you a Waggle user interested in using a new sensor (i.e. a new camera, a bat signal detector, a custom sensor they built)? If you would like to build, design and deploy software that could answer your questions above, then Waggle is the right choice for you.\\n\\nThis tutorial will guide you in preparing your own Waggle device and (optionally) registering it to upload data to a shared development Beehive. This Waggle device is a fully unlocked development platform running the same WES infrastructure that runs in production Waggle edge devices (ex. the Wild Waggle Node). This is an ideal platform for users interested in developing a new edge app and/or experimenting with a new sensor.\\n\\nGetting Started\\n\\nTo get started in boot-strapping your Waggle Edge Computing kit you can follow the instructions for the various supported platforms on the node-platforms GitHub page.\\n\\nWe currently support a limited set of hardware platform because making edge devices into Waggle requires some hardware specific instructions. Check out the platforms we support as of now. More platforms will be added in the future. However, if you would like to add support for other platforms go ahead and submit a pull request to node-platforms.\\n\\nRegistering your Waggle device\\n\\nDuring the bootstrapping process you will have the option to register your device within the web portal here. It is highly recommended to register your device, as this enables all the core WES tools to be automatically downloaded, enabling the edge app development and run-time environment. Additionally, this enables your edge apps to publish data to the development Beehive, accessible to cloud-based analysis tools and workflow frameworks.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-waggle.md'}, page_content='To register your device, use the dev devices form. Enter your device ID (which you will obtain through the hardware boot-strapping process) then click \"Get Keys\" button. A \"registration zip\" file will be generated and available for download. Then follow the instructions for your device to load the registration keys.\\n\\nYou may register as many times as you want. But note that each registration key has a short expiration time and should be used shortly after generation.\\n\\nNow you are ready to develop your edge apps and/or introduce new sensors to the Waggle platform. Head over to the overview to find the instructions you need for development.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='sidebar_position: 4\\n\\nAccess and use data\\n\\nRaw sensor data is collected by edge code. This edge code can either talk to sensor hardware directly or may obtain data from an abstraction layer (not show in image above). Edge code may forward unprocessed sensor data, do light processing to convert raw sensor values into final data products, or may use CPU/GPU-intensive workloads (e.g. AI application) to extract information from data-intensive sensors such as cameras, microphone or LIDAR.\\n\\nSensor data from nodes that comes in numerical or textual form (e.g. temperature) is stored natively in our time series database. Sensor data in form of large files (images, audio, movies..) is stored in the Waggle object store, but is referenced in the time series data (thus the dashed arrow in the figure above). Thus, the primary way to find all data (sensor and large files) is via the Waggle sensor query API described below.\\n\\nCurrently the Waggle sensor database contains data such as:\\n\\nRelative humidity, barometric pressure, ambient temperature and gas (VOC) BME680.\\n\\nRainfall measurements (Hydreon RG-15).\\n\\nAI-based cloud coverage estimation from camera images.\\n\\nAI-based object counts from camera images.\\n\\nSystem data such as uptime, cpu and memory.\\n\\nData can be accessed in realtime via our data API or in bulk via data bundles.\\n\\nData API\\n\\nWaggle provides a data API for immediate and flexible access to sensor data via search over time and metadata tags. It is primarily intended to support exploratory and near real time use cases.\\n\\nDue to the wide variety of possible queries, we do not attempt to provide DOIs for results from the data API. Instead, we leave it up to users to organize and curate datasets for their own applications. Long term, curated data is instead provided via data bundles.\\n\\nThere are two recommended approaches to working with the Data API:\\n\\nUsing the Python Sage Data Client.\\n\\nUsing the HTTP API.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='Each is appropriate for different use cases and integrations, but generally the following rule applies:\\n\\nIf you just want to get data into a Pandas dataframe for analysis and plotting, use the sage-data-client, otherwise use the HTTP API.\\n\\nUsing Sage data client\\n\\nThe Sage data client is a Python library which streamlines querying the data API and getting the results into a Pandas dataframe. For details on installation and usage, please see the Python package.\\n\\nUsing HTTP API\\n\\nThis example shows how to retrieve data the latest data from a specific sensor (you can adjust the start field if you do not get any recent data):\\n\\n```console curl -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-10s\", \"filter\": { \"sensor\": \"bme680\" } } \\'\\n\\nExample results:json {\"timestamp\":\"2021-08-09T19:26:03.880781217Z\",\"name\":\"iio.in_humidityrelative_input\",\"value\":70.905,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.878659392Z\",\"name\":\"iio.in_pressure_input\",\"value\":975.78,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.872652127Z\",\"name\":\"iio.in_resistance_input\",\"value\":93952,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.874998057Z\",\"name\":\"iio.in_temp_input\",\"value\":27330,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}}'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='```\\n\\n:::tip More details of using the data API and the data model can be found here and here. :::\\n\\nData bundles\\n\\nData bundles provide sensor data and associated metadata in a single, large, downloadable file. Soon, each Data Bundle available for download will have a DOI that can be used for publication citations.\\n\\nData Bundles are compiled nightly and may be downloaded in this archive.\\n\\nAccessing file uploads\\n\\nUser applications can upload files for AI training purposes. These files stored in an S3 bucket hosted by the Open Storage Network.\\n\\nTo find these files use the filter \"name\":\"upload\" and specify additional filters to limit search results, for example:\\n\\nconsole curl -s -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\'{ \"start\": \"2021-09-10T12:51:36.246454082Z\", \"end\":\"2021-09-10T13:51:36.246454082Z\", \"filter\": { \"name\":\"upload\", \"plugin\":\"imagesampler-left:0.2.3\" } }\\''),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='Output: json {\"timestamp\":\"2021-09-10T13:19:27.237651354Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d05a0a4/1631279967237651354-2021-09-10T13:19:26+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d05a0a4\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T13:50:32.29028603Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bc3d/1631281832290286030-2021-09-10T13:50:32+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bc3d\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T12:52:59.782262376Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdc2/1631278379782262376-2021-09-10T12:52:59+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T13:49:49.084350086Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdd2/1631281789084350086-2021-09-10T13:49:48+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bdd2\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}}\\n\\nFor a quick way to only extract the urls from the json objects above, a tool like jq can be used:\\n\\nconsole curl -s -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\'{ \"start\": \"2021-09-10T12:51:36.246454082Z\", \"end\":\"2021-09-10T13:51:36.246454082Z\", \"filter\": { \"name\":\"upload\", \"plugin\":\"imagesampler-left:0.2.3\" } }\\' | jq -r .value > urls.txt'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content=\"The resulting file urls.txt will look like this: text https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d05a0a4/1631279967237651354-2021-09-10T13:19:26+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bc3d/1631281832290286030-2021-09-10T13:50:32+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdc2/1631278379782262376-2021-09-10T12:52:59+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdd2/1631281789084350086-2021-09-10T13:49:48+0000.jpg\\n\\nTo download the files: console wget -i urls.txt\\n\\nIf many files are downloaded, it is better to preserve the directory tree structure to prevent filename collision: console wget -r -i urls.txt\\n\\nProtected data\\n\\nWhile most Waggle data is open and public - some types of data, such as raw images and audio from sensitive locations, may require additional steps:\\n\\nYou will need a Sage account.\\n\\nYou will need to sign our Data Use Agreement for access.\\n\\nYou will need to provide authentication to tools you are using to download files. (ex. wget, curl)\\n\\nAttempting to download protected files without meeting these criteria will yield a 401 Unauthorized response.\\n\\nIf you've identified protected data you are interested in, please contact us so we can help get you access.\\n\\nIn the case of protected files, you'll need to provide authentication to your tool of choice. These will be your portal username and access token which can be found in the Access Credentials section of the site.\\n\\nThese can be provided to tools like wget and curl as follows:\\n\\n```console\\n\\nexample using wget\\n\\nwget --user=\\n\\nexample using curl\\n\\ncurl -u\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-an-account.md'}, page_content='sidebar_label: Create an account sidebar_position: 1\\n\\nOverview\\n\\nWhile some Sage features are open for public use, you\\'ll need an approved account to perform tasks such as:\\n\\nGet access to protected data.\\n\\nPublish apps to the ECR.\\n\\nSchedule app on nodes.\\n\\nIn this document, we\\'ll walk though creating an account.\\n\\nCreating an account\\n\\nClick on the Portal button in the upper right corner.\\n\\nClick on the Sign In button in the upper right corner.\\n\\nThis will take you to the Globus login page where you\\'ll need to provide your organization credentials. If you do not see your organization, please see the \"Didn\\'t find your organization?\" note at the bottom of the Globus login page.\\n\\nFinally, if this is your first time signing in, you\\'ll need to choose a username which will complete your account creation.\\n\\nAt this point, our team will need to review and approve your account before you\\'ll have permission to perform certain tasks. If you your account is not approved within 72 hours or you have special requirements, please Contact us so that we can help perform any account configuration.\\n\\nNext steps\\n\\nOnce your account is approved, you will have scheduling access and protected data browsing in the portal for nodes we\\'ve assigned to your account.\\n\\nFor CLI tools and SSH access to nodes, please go to Portal → Your Account → Access Creds and follow the Update SSH Public Keys and Finish Setup for Node Access instructions.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content='sidebar_position: 5\\n\\nAccess Waggle sensors\\n\\nA Waggle sensor is an entity that produces measurements of a phenomenon and that helps users analyze what is happening in the environment. There are sensors already hosted by Waggle and also sensors that are being integrated into Waggle as a user-hosted sensor. A sensor does not necessarily mean a physical device, but can be a program producing measurements from data -- we call it software-defined sensor. Once those sensors become available in Waggle nodes edge applications running inside the nodes can pull measurements from the sensors to process them.\\n\\nIn general, Waggle sensors are desinged to be accessible from any edge applications running on the Waggle node that hosts the sensors, but can be limited their access to groups and personnel. For example, a pan-tilt-zoom featured camera may be only accessed from authorized applications in order to prevent other applications from operating the camera. Ideally, Waggle sensors can form and support the Waggle ecosystem where sensor measurements are integrated and used by edge applications for higher level computation and complex decision making.\\n\\nWaggle physical sensors\\n\\nThe Waggle node is designed to accommodate sensors commonly used to support environmental science, but not limited to host other sensors. The currently supported sensors are,\\n\\nNOTE: not all Waggle nodes have the same set of sensors, and the sensor configuration depends on what to capture from the environment where the node is deployed\\n\\nBME680 temperature, humidity, pressure, and gas preview RG-15 rainfall preview ETS ML1-WS 20-16 kHz microphone recording sound XNV-8080R 5 MP camera with 92.1 degree horizontal and 67.2 degree vertical angle view XNV-8082R 6 MP camera with 114 degree horizontal and 62 degree vertical angle view XNF-8010RV 6 MP fisheye camera with 192 degree horizontal and vertical angle view XNV-8081Z 5 MP digital pan-tilt-rotate-zoom camera'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content=\"Any collaborators and user communities can bring up their sensors to Waggle node. The node can easily host sensor devices that support serial interface as well as network interface (e.g., http, rtsp, etc). Other currently supported user sensors include:\\n\\nSoftware-defined Radio: detecting raindrops and snow flakes\\n\\nRadiation detector: radiation detector\\n\\nLIDAR: distance of nearby objects\\n\\nMobotix: infrared camera\\n\\n[view more...]\\n\\nWaggle software-defined sensors\\n\\nSoftware-defined sensors are limitless as edge applications define them. You can start building your edge application that publishes outputs using PyWaggle's basic example that can become a software-defined sensor. Later, such outputs can be consumed by other edge applications to produce higher level information about the measurements. A few example of Waggle software-defined sensors are,\\n\\nObject Counter: env.count.OBJECT counts objects from an image, where OBJECT is the object name that is recognized\\n\\nCloud Coverage Estimator: env.coverage.cloud provides a percentage of cloud covered in an image\\n\\nAccess to Waggle sensors\\n\\nWaggle sensors are integrated into Waggle using the PyWaggle library. PyWaggle utilizes AMQP, the message publishing and subscribing mechanism, to support exchanging sensor measurements between device plugins and edge applications. An edge application can subscribe and process those measurements using PyWaggle's subscriber. The application then produces its output and publishes it as a measurement back to the system using PyWaggle publisher.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content=\"PyWaggle often provides edge applications direct access to physical sensors. For sensors that support realtime protocols like RTSP and RTP and others, PyWaggle exposes those protocols to edge applications, and it is up to the applications to process data using given protocol. For example, RTSP protocol can be handled by OpenCV's VideoCapture class inside an application. If any physical sensor device that requires a special interfacing to the device, an edge application that supports the interfacing need to run in order to publish sensor measurements to the system, and later those measurements are used by other edge applications.\\n\\nExample: sampling images from camera\\n\\nIt is often important to sample images from cameras in the field to create initial dataset for a machine learning algorithm. The example describes how to access to a video stream from a camera sensor using PyWaggle.\\n\\nBring your own sensor to Waggle\\n\\nUsers may need to develop their own device plugin to expose the sensor to the system, or to publish measurement data from the sensor to the cloud. Unlike an edge application or software-defined sensors, device plugins communicating with a physical sensor may need special access, e.g. serial port, in order to talk to the sensor attached to Waggle node. Such device plugin may need to be verified by the Waggle team. Visit the Building your own Waggle device page for the guide to set up your Waggle device.\\n\\nTo integrate your sensor device into Waggle, head over to the Contact Us page\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/cloud-compute.md'}, page_content='sidebar_position: 6\\n\\nCloud compute & HPC on edge data\\n\\nWaggle provides a number of interfaces which other computing and HPC systems can build on top of. In this section, we explore some of the most common applications of Waggle.\\n\\nTriggering on data from the edge\\n\\nA common application is monitoring data from the edge and triggering actions when values exceed a threshold or an unusual event is detected.\\n\\nAs a getting started example, we demonstrate an outline of how this can be done in Waggle using the Sage data client.\\n\\n```python import sage_data_client import time\\n\\nwhile True: # query pressure data in recent 10 minute window df = sage_data_client.query( start=\"-10m\", filter={ \"name\": \"env.pressure\", \"sensor\": \"bme680\", } )\\n\\n# compute stddev for nodes\\' pressure data in window\\nstd = df.groupby(\"meta.vsn\").value.std()\\n\\n# find all pressure events exceeding an example threshold\\nevents = std[std > 8.0]\\n\\n# \"post\" vsn to alert system\\nfor vsn in events.index:\\n    print(f\"post {vsn} to alert system\")\\n\\ntime.sleep(60)\\n\\n```\\n\\nThe above code queries a 10 minute window of atmospheric pressure data every minute and \"posts\" alerts for nodes exceeding a predefined standard deviation threshold.\\n\\nThis example and more can be found in the Sage data client examples directory.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/4-publishing-to-ecr.md'}, page_content='sidebar_position: 4\\n\\nPart 4: Publishing to ECR\\n\\nNow that we\\'ve finished preparing our code and testing it, we\\'re almost ready to publish it to the Edge Code Repository!\\n\\nPreparing our app\\n\\nBefore publishing an app to the Edge Code Repository, we need to add a few packaging items to it.\\n\\nFirst, update the homepage in your sage.yaml to point to your app-tutorial Github repo and verify that it matches the following:\\n\\nyaml name: \"app-tutorial\" version: \"0.1.0\" description: \"My really amazing app!\" keywords: \"\" authors: \"Your name\" collaborators: \"\" funding: \"\" license: \"\" homepage: \"https://github.com/username/app-tutorial\" source: architectures: - \"linux/amd64\" - \"linux/arm64\"\\n\\nNext, create an ecr-meta directory in your repo and populate it with the following text and media:\\n\\necr-science-description.md - Markdown with in depth description of the science being done here (1 page of text).\\n\\necr-icon.jpg - An icon for the project/work 512x512px.\\n\\necr-science-image.jpg - A science image for the project with a minimum size of 1920x1080px.\\n\\nOnce we\\'ve commited and pushed those files to your repo, we\\'re ready to publish our app!\\n\\nPublishing our app\\n\\nPlease visit the Edge Code Repository and complete the following steps:\\n\\nGo to \"Sign In\" and follow the instructions.\\n\\nGo to \"My Apps\".\\n\\nGo to \"Create app\" and follow the instructions.\\n\\nIf everything is successful, your plugin will appeared and be marked as \"Built\".\\n\\nConclusion\\n\\nCongratulation! You\\'ve successfully written, tested and published an app to ECR!\\n\\nWe encourage you to check out other apps in the ECR and explore additional functionality provided by pywaggle.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/1-intro-to-edge-apps.md'}, page_content=\"sidebar_position: 1\\n\\nPart 1: Intro to edge apps\\n\\nWhat are edge apps?\\n\\nEdge apps are programs which read data (ex. sensors, audio, video), process it and then publish information derived from that data.\\n\\nA basic example of an app is one which reads and publishes a value from a sensor every minute. A more complex example could publish the number of birds in a scene using a deep learning model.\\n\\nEdge apps are composed of code, dependencies and models which are packaged so they can be scheduled on Waggle nodes. At a high level, the typical app lifecycle is:\\n\\nExploring existing edge apps\\n\\nOne of the major goals of Waggle is to provide the science community with a diverse catalog of edge apps to enable the sharing of new research. This catalog is maintained as part of the Edge Code Repository where you can find more background information and links to their source repos.\\n\\nWe encourage users to explore the ECR to get familiar with existing apps as well a references if you develop your own edge app.\\n\\nNext steps\\n\\nIf this sounds exciting and you'd like to write you own edge app, please continue to part 2!\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='sidebar_position: 2\\n\\nPart 2: Creating an edge app\\n\\nIn part 1, we showed an overview of what edge apps are and how they fit into the Waggle ecosystem. Now, we\\'ll dive right in and start writing our very own edge app!\\n\\nPrerequisites\\n\\nFor this part of the tutorial, we\\'ll assume you are developing directly on a laptop or machine with a camera or webcam available. You should have some basic development experience in Python and with git for version control.\\n\\nDevelopment workflow\\n\\nIn the next few parts of this tutorial, we\\'ll deep dive into the following app development workflow:\\n\\nFirst, data and model selection is where you scope the problem and identify a new or existing model for your application. This typically happens outside of our ecosystem.\\n\\nSecond, develop and test is where you begin to integrate your initial code with our ecosystem, test and finally build your application in ECR.\\n\\nFinally, deploy and iterate is where you schedule your application for deployment and look at the results.\\n\\nA driving example\\n\\nIn order to illustrate progress through each of these stages, we\\'ll start with a concrete code example and iterate on it over the next few sections.\\n\\nIn practice, lots of work goes into the data and model selection step. For now, we\\'ll assume that groundwork has already been done and we\\'ve settled on the following code snippit to start with.\\n\\n```python import numpy as np import cv2\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): # read example image from file image = cv2.imread(\"example.jpg\")\\n\\n# compute mean color\\nmean_color = compute_mean_color(image)\\n\\n# print mean color\\nprint(mean_color)\\n\\nif name == \"main\": main()'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='```\\n\\nBootstrapping our app from a template\\n\\nWe\\'ll start our by using a cookiecutter template to bootstrap our app.\\n\\nFirst, ensure the latest cookiecutter is installed:\\n\\nsh pip3 install --upgrade cookiecutter\\n\\nNow, run the following command:\\n\\nsh cookiecutter gh:waggle-sensor/cookiecutter-sage-app\\n\\nYou should be prompted to fill in the following fields:\\n\\ntxt [1/5] name (my-amazing-app-name): my-amazing-app-name [2/5] description (My really amazing app!): [3/5] author (My name): [4/5] version (0.1.0): [5/5] Select kind 1 - vision 2 - usbserial_sensor 3 - minimal 4 - tutorial <<< use 4 for tutorial Choose from [1/2/3/4] (1): 4\\n\\nIf this succeeds, a new app-tutorial directory will be created with the following files:\\n\\nName Description main.py Main code requirements.txt Code dependencies Dockerfile App build instructions sage.yaml App metadata\\n\\nInstalling the dependencies\\n\\nThe first step in preparing our example for the edge is to install pywaggle in our local development environment.\\n\\npywaggle is our Python SDK which provides edge apps access to devices (ex. cameras and microphones) and messaging within a node.\\n\\nFor this tutorial, we\\'ll install the latest version of the requirements included in the template:\\n\\nsh pip3 install --upgrade --requirement requirements.txt\\n\\nAccessing a camera\\n\\nNow that we have pywaggle, the first change we\\'ll make is to use a camera as input rather than a static image file. We\\'ll use the following shapshot() function to take an RGB snapshot from the camera.\\n\\n```python import numpy as np\\n\\nfrom waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n# compute mean color\\nmean_color = compute_mean_color(snapshot.data)\\n\\n# print mean color\\nprint(mean_color)\\n\\nif name == \"main\": main()'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='```\\n\\nNow, we can try this out by running:\\n\\nsh python3 main.py\\n\\nYou should see output like:\\n\\ntxt [51.43575738 51.83611871 54.64226671]\\n\\nYou\\'re exact numbers may differ as this is computed using your default camera.\\n\\nPublishing results\\n\\nThe next change we\\'ll make is to publish our data to the Beehive Data Repository instead of just print it. This will allow it to be sent to a Beehive once it\\'s scheduled on a node.\\n\\n```python import numpy as np\\n\\nfrom waggle.plugin import Plugin from waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): with Plugin() as plugin: # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n    # compute mean color\\n    mean_color = compute_mean_color(snapshot.data)\\n\\n    # publish mean color\\n    plugin.publish(\"color.mean.r\", mean_color[0], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.g\", mean_color[1], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.b\", mean_color[2], timestamp=snapshot.timestamp)\\n\\nif name == \"main\": main()'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='```\\n\\nNow, we\\'ll run this using:\\n\\nsh python3 main.py\\n\\nYou may notice something... there\\'s no output! Usually, published data is sent to a beehive where it can be viewed later. However, because we\\'re developing locally and have not configured a beehive, the data isn\\'t going anywhere. In the next section, we\\'ll see how we can tap into our published data.\\n\\nViewing run logs\\n\\nIn order to make developing and debugging apps easier, pywaggle can write out a log directory as follows:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nThis will create a new directory named test-run and will contain a file named data.ndjson which contains something like:\\n\\njson {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":32.67932074652778} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":19.087491319444446} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":10.337491319444444}\\n\\nIf we run python3 main.py again, then we\\'ll see new data appended to that file:\\n\\njson {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":32.67932074652778} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":19.087491319444446} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":10.337491319444444} {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":30.90709743923611} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":16.61302517361111} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":8.565154079861111}\\n\\nThis provides a convenient way to understand the behavior of an app, particularly one with a more complicated flow.\\n\\nUploading a snapshot\\n\\nFinally, the last change we\\'ll make is to upload our snapshots after publishing the mean color.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='We\\'ll upload every snapshot for demonstration purposes, but you wouldn\\'t want to do this in a real app. Instead, you\\'d typically upload in response to detecting an event such as an anomalous object or loud noise.\\n\\n```python import numpy as np\\n\\nfrom waggle.plugin import Plugin from waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): with Plugin() as plugin: # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n    # compute mean color\\n    mean_color = compute_mean_color(snapshot.data)\\n\\n    # publish mean color\\n    plugin.publish(\"color.mean.r\", mean_color[0], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.g\", mean_color[1], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.b\", mean_color[2], timestamp=snapshot.timestamp)\\n\\n    # save and upload image\\n    snapshot.save(\"snapshot.jpg\")\\n    plugin.upload_file(\"snapshot.jpg\", timestamp=snapshot.timestamp)\\n\\nif name == \"main\": main()'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='```\\n\\nLet\\'s run our app again using:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nIf you take a look in the test-run/uploads directory, you should now see an image.\\n\\nUploads are added to the run log directory using the format nstimestamp-filename.\\n\\nYou should also see a corresponding item in the data.ndjson file.\\n\\njson {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":29.601871744791666} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":16.004838324652777} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":8.217218967013888} {\"meta\":{\"filename\":\"snapshot.jpg\"},\"name\":\"upload\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":\"/Users/sean/dev/pw-example/test-run/uploads/1661279974985679000-snapshot.jpg\"}\\n\\nTools for analyzing run logs (Optional)\\n\\nIf you find yourself working with run logs frequently, we recommend the Sage data client which provides convenient functionality for loading and doing analysis on the data.ndjson file. See the \"Load results from file\" example for more info.\\n\\nNext steps\\n\\nCongratulations! You\\'ve finished preparing our example code for the edge!\\n\\nIn the part 3, we\\'ll look at how we can build and test our app on a real node!'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='sidebar_position: 3\\n\\nPart 3: Testing an edge app\\n\\nIn the previous part, we took a code snippit and iterated on it until it was ready for the edge. By the end, we had basic camera access and publishing working!\\n\\nNow, we\\'re ready to start testing it on a development node and describing our build steps.\\n\\nAccessing development nodes\\n\\nThe first thing we need to do is get access to a development node. Unfortunately, we are still developing the infrastructure to open this up to general users.\\n\\nFor now, please contact us to request access to a development node and we\\'ll work with you to setup access.\\n\\nCreating a repo for our app\\n\\nBefore connecting to our node, let\\'s take a moment to organize our code into a repo we will later use on the node.\\n\\nGo ahead and create a new Github repo named app-tutorial and commit the files from previous part.\\n\\nBuilding our app\\n\\nNow that we\\'ve setup node access, ssh to the node then clone and cd into your app-tutorial repo:\\n\\nsh git clone https://github.com/username/app-tutorial cd app-tutorial\\n\\nThe first thing we\\'ll do is build our app on the node:\\n\\nsh sudo pluginctl build .\\n\\nThis may take some time, but once it completes you should see something like:\\n\\n```txt Sending build context to Docker daemon 59.39kB Step 1/6 : FROM waggle/plugin-base:1.1.1-base ... Step 2/6 : WORKDIR /app ... Step 3/6 : COPY requirements.txt . ... Step 4/6 : RUN pip3 install --no-cache-dir -r requirements.txt ... Step 5/6 : COPY . . ... Step 6/6 : ENTRYPOINT [\"python3\", \"main.py\"] ... b38bc0a208d0: Pushed 1101ffccd70a: Pushed latest: digest: sha256:7bee2a62fbcc9913f1c53bbdab79e973e70947618ffe4db90cae6a8f0ff6c8d7 size: 2407 Successfully built plugin\\n\\n10.31.81.1:5000/local/app-tutorial'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='```\\n\\nOnce we see Successfully built plugin, we can continue to running our app.\\n\\nRunning our app\\n\\nWhen we successfully built our app, the last line of output was 10.31.81.1:5000/local/app-tutorial. We will now use this reference to run our app.\\n\\nsh sudo pluginctl run --name app-tutorial 10.31.81.1:5000/local/app-tutorial\\n\\nWhen you run this, you\\'ll see that there\\'s a bug in the code:\\n\\n```sh Launched the plugin app-tutorial-1659971085 successfully INFO: 2022/08/08 15:04:45 run.go:63: Plugin is in \"Pending\" state. Waiting...\\n\\n[ WARN:0@0.032] global /io/opencv/modules/videoio/src/cap_v4l.cpp (902) open VIDEOIO(V4L2:/dev/video0): can\\'t open camera by index Traceback (most recent call last): File \"main.py\", line 32, in\\n\\nThis was caused by the fact that most nodes have multiple cameras, so we need to be more specific about which camera to use.\\n\\nTo address this, we\\'ll change the following line in main.py from:\\n\\npython with Camera() as camera:\\n\\nto:\\n\\npython with Camera(\"left\") as camera:\\n\\nThe specific camera name will depend on your specific node. If you are having problems accessing a camera, please contact us for more details.\\n\\nAfter rebuilding and running this again, the plugin should run and exit cleanly:\\n\\n```txt Launched the plugin app-tutorial-1659971085 successfully INFO: 2022/08/08 15:04:45 run.go:63: Plugin is in \"Pending\" state. Waiting...\\n\\nshould exit cleanly with no output'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='```\\n\\nNow that we know this works, please commit and push the change to the repo from your machine.\\n\\nFinally, if you are rebuilding and running code frequently, you can combine the build and run into a single step as follows:\\n\\nsh sudo pluginctl run --name app-tutorial $(sudo pluginctl build .)\\n\\nViewing our output\\n\\nWe\\'ll close this part, by looking at the data we just published. To do this, we\\'ll query the Beehive Data Repository:\\n\\nsh curl -s \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-5m\", \"filter\": { \"task\": \"app-tutorial\" } }\\'\\n\\nYou should see some results like:\\n\\njson {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.b\",\"value\":133.61671793619792,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.g\",\"value\":136.46639404296874,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.r\",\"value\":134.48696818033855,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/Pluginctl/sage-app-tutorial-app-tutorial/000048b02d15bdc2/1659971088820981933-snapshot.jpg\",\"meta\":{\"filename\":\"snapshot.jpg\",\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}}\\n\\nThese are exactly the mean color values we computed and published!\\n\\nThis is intended to be a quick preview of how to access data to help get you started. If you are interested, we cover this topic in much depth here.\\n\\nNext steps'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content=\"Now we've been able to build, run and even fix a bug in our code! In part 4, we'll see how to publish a first release of our code to the Edge Code Repository!\"),\n",
       " Document(metadata={'source': 'sage-website/docs/events/past/hackathon-2023.md'}, page_content='sidebar_label: August 2023 Hackathon\\n\\nAugust 2023 Sage Hackathon\\n\\nWe are hosting a Hackathon for Sage in late August ~~with preliminary dates August 30-31~~!\\n\\nUpdate: Based on user feedback, we will run afternoon sessions on both August 30 and 31 starting at 1pm CST. We will email out an agenda and invite to Slack to participants who have signed up a few days before the event.\\n\\nThe goal of the Hackathon is to set aside a few hours dedicated to working through user applications, code and science examples in depth. If this interests you, please fill out the signup form as soon as possible!\\n\\nPrior to the Hackathon, we request that you do a few things: 1. Read the Sage Overview in the Sage docs to understand what Sage is and how it might connect to your work. 2. Start assembling the people on your team likely to participate. 3. Ensure they have accounts in the Sage Portal. 4. Start working through the Edge apps tutorial in the Sage docs.\\n\\nDuring the Hackathon, we will review how to use the portal and parts of the Edge app tutorial. However, already having done some preliminary work will allow more time for our team to provide support for your unique application.'),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content='Sage Data Client\\n\\nThis is the official Sage Python data API client. Its main goal is to make writing queries and working with the results easy. It does this by:\\n\\nProviding a simple query function which talks to the data API.\\n\\nProviding the results in an easy to use Pandas data frame.\\n\\nInstallation\\n\\nSage Data Client can be installed with pip using:\\n\\nsh pip3 install sage-data-client\\n\\nIf you prefer to install this package into a Python virtual environment or are unable to install it system wide, you can use the venv module as follows:\\n\\n```sh\\n\\n1. Create a new virtual environment called my-venv.\\n\\npython3 -m venv my-venv\\n\\n2. Activate the virtual environment\\n\\nsource my-venv/bin/activate\\n\\n3. Install sage data client in the virtual environment\\n\\npip3 install sage-data-client ```\\n\\nNote: If you are using Linux, you may need to install the python3-venv package which is outside of the scope of this document.\\n\\nNote: You will need to activate this virtual environment when opening a new terminal before running any Python scripts using Sage Data Client.\\n\\nUsage Examples\\n\\nQuery API\\n\\n```python import sage_data_client\\n\\nquery and load data into pandas data frame\\n\\ndf = sage_data_client.query( start=\"-1h\", filter={ \"name\": \"env.temperature\", } )\\n\\nprint results in data frame\\n\\nprint(df)\\n\\nmeta columns are expanded into meta.fieldname. for example, here we print the unique nodes\\n\\nprint(df[\"meta.vsn\"].unique())\\n\\nprint stats of the temperature data grouped by node + sensor.\\n\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"])) ```\\n\\n```python import sage_data_client\\n\\nquery and load data into pandas data frame\\n\\ndf = sage_data_client.query( start=\"-1h\", filter={ \"name\": \"env.raingauge.*\", } )\\n\\nprint number of results of each name\\n\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size())'),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content='```\\n\\nLoad results from file\\n\\nIf we have saved the results of a query to a file data.json, we can also load using the load function as follows:\\n\\n```python import sage_data_client\\n\\nload results from local file\\n\\ndf = sage_data_client.load(\"data.json\")\\n\\nprint number of results of each name\\n\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size()) ```\\n\\nIntegration with Notebooks\\n\\nSince we leverage the fantastic work provided by the Pandas library, performing things like looking at dataframes or creating plots is easy.\\n\\nA basic example of querying and plotting data can be found here.\\n\\nAdditional Examples\\n\\nAdditional code examples can be found in the examples directory.\\n\\nIf you\\'re interested in contributing your own examples, feel free to add them to examples/contrib and open a PR!\\n\\nReference\\n\\nThe query function accepts the following arguments:\\n\\nstart. Absolute or relative start timestamp. (required)\\n\\nend. Absolute or relative end timestamp.\\n\\nhead. Limit results to head earliest values per series. (Only one of head or tail can be provided.)\\n\\ntail. Limit results to tail latest values per series. (Only one of head or tail can be provided.)\\n\\nfilter. Key-value patterns to filter data on.'),\n",
       " Document(metadata={'source': 'pywaggle/README.md'}, page_content='Waggle Python Module\\n\\npywaggle is a Python module for implementing Waggle plugins and system services.\\n\\nInstallation Guides\\n\\nMost users getting started with pywaggle will want to install latest version with all optional dependencies using:\\n\\nsh pip install -U pywaggle[all]\\n\\nAdvanced users can install specific subsets of functionality using the following extras flags:\\n\\naudio - Audio and microphone support for plugins.\\n\\nvision - Image, video and camera support for plugins.\\n\\n```sh\\n\\ninstall only core plugin features\\n\\npip install pywaggle\\n\\ninstall only audio features\\n\\npip install pywaggle[audio]\\n\\ninstall only vision features\\n\\npip install pywaggle[vision]\\n\\ninstall both audio and vision features\\n\\npip install pywaggle[audio,vision] ```\\n\\nUsage Guides\\n\\nWriting a plugin'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='Writing a plugin\\n\\nAlthough this doc is still useful as a reference for more advanced pywaggle features, we have migrated the getting started portions to their own tutorial on our central docs site. The pywaggle docs will become more of a reference guide in the near future.\\n\\nIn this guide, we\\'ll walk through writing a basic plugin and exploring some of the functionality provided by pywaggle.\\n\\nThat being said, we do want to emphasize that pywaggle is designed to make it easy to interface existing Python code with the Waggle stack. To a first approximation, pywaggle aims to augment print statements with publish statements.\\n\\nIf you\\'d like to jump ahead to real code, please see the following examples:\\n\\nMinimal Numpy Example\\n\\nHello World ML Example\\n\\nThese repos can be used as starter templates for your own plugin development.\\n\\nWhat is a plugin?\\n\\nA plugin is a self-contained program which typically reads sensors, audio or video data, does some processing and finally publishes results derived from that data.\\n\\nThe most basic example of a plugin is one which simply reads and publishes a value from a sensor. A more complex plugin could publish the number of cars seen in a video stream using a deep learning model.\\n\\nPlugins fit into the wider Waggle infrastructure by being tracked in the Edge Code Repository, deployed to nodes and publishing data to our data repository.\\n\\nWriting \"Hello World\" plugin code\\n\\nNote: In this guide, we currently only cover writing the plugin __code__. We still are updating the docs on building and running a plugin inside Virtual Waggle and natively. As such, this guide will help you structure and run your code locally but not against the rest of platform.\\n\\nWe\\'ll walk through writing a \"hello world\" plugin which simply publishes a increasing counter as measurement hello.world.counter every second.\\n\\n1. Install pywaggle\\n\\nFirst, we\\'ll install the latest version of pywaggle.\\n\\nsh pip install -U pywaggle[all]'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='This will install the core pywaggle modules along with the extra developer modules.\\n\\n2. Create empty plugin directory\\n\\nCreate a new empty directory which we\\'ll write our plugin in.\\n\\nsh mkdir plugin-hello-world cd plugin-hello-world\\n\\n3. Create a requirements.txt dependency file\\n\\nCreate a new requirements.txt file and add this following:\\n\\nsh pywaggle[all]\\n\\nThis will be used when building our plugin to ensure all dependencies are available. Right now, it only contains pywaggle but you can add your own custom dependencies here.\\n\\n4. Create main.py file\\n\\nCreate a new file called main.py with the following code:\\n\\n```python from waggle.plugin import Plugin import time\\n\\nwith Plugin() as plugin: for i in range(10): print(\"publishing value\", i) plugin.publish(\"hello.world.value\", i) time.sleep(1)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\n5. Run plugin\\n\\nThe plugin can now be run using:\\n\\nsh python3 main.py\\n\\nYou should see the output:\\n\\ntxt publishing value 0 publishing value 1 publishing value 2 ...\\n\\n6. Access run logs (Optional)\\n\\nAs you\\'re developing and debugging a plugin, it can be very helpful to see the run log of published messages and uploads.\\n\\nYou can enable this by defining the PYWAGGLE_LOG_DIR=path/to/run/logs environment variable as follows:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nNow, you should see a new directory named test-run with the following contents:\\n\\ntxt test-run/ data.ndjson <- measurements published uploads/ <- timestamped files uploaded nstimestamp1-filename1 nstimestamp2-filename2 ...\\n\\nThe data.ndjson file is a newline delimited JSON file containing raw measurement messages.\\n\\nHere\\'s an example from a more complete plugin:\\n\\njson {\"name\":\"env.temperature\",\"timestamp\":\"2022-08-23T13:27:10.562104000\",\"meta\":{\"sensor\":\"bme280\"},\"value\":23.0} {\"name\":\"upload\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{\"camera\":\"top\",\"filename\":\"test.png.webp\"},\"value\":\"/Users/sean/git/pywaggle-log-dir-example/testrun/uploads/1661279233561615000-test.png.webp\"} {\"name\":\"image.cats\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{},\"value\":0} {\"name\":\"image.birds\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{\"camera\":\"left\"},\"value\":8} {\"name\":\"timeit.inference\",\"timestamp\":\"2022-08-23T13:27:11.562104000\",\"meta\":{},\"value\":1005408000}\\n\\nThe contents of the log directory operates in an append mode, so you may safely run the plugin multiple times without losing previous data.\\n\\nAdding \"Hello World\" plugin packaging info\\n\\nNow that we have the basic plugin code working, let\\'s prepare this code to be submitted to the Edge Code Repository.\\n\\n1. Create a Github repo for plugin\\n\\nFirst, we need to create a Github repo for our plugin. Go ahead a create one called \"plugin-hello-world\" and add the contents from our plugin-hello-world directory.'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='For the purposes of this example, we\\'ll assume our plugin URL is https://github.com/username/plugin-hello-world.\\n\\n2. Add Dockerfile\\n\\nCreate and add a new file called Dockerfile with the following contents:\\n\\ndockerfile FROM waggle/plugin-base:1.1.1-ml COPY requirements.txt /app/ RUN pip3 install --no-cache-dir --upgrade -r /app/requirements.txt COPY . /app/ WORKDIR /app ENTRYPOINT [\"python3\", \"/app/main.py\"]\\n\\nThis file defines what base image should be used by a plugin and how it should be run. In more complex examples, additional dependencies may be specified here.\\n\\n3. Add sage.yaml\\n\\nCreate and add a new file called sage.yaml with the following contents:\\n\\nyaml name: \"hello-world\" description: \"My hello world plugin\" keywords: \"hello, testing\" authors: \"Your Name <your.email@somewhere.org>, A Coworker <your.coworker@somewhere.org>\" collaborators: \"Helpful Collaborator <our.collaborator@otherplace.edu>\" funding: \"\" license: \"\" homepage: \"https://github.com/username/plugin-hello-world/blob/main/README.md\" source: architectures: - \"linux/amd64\" - \"linux/arm64\"\\n\\nThis file contains metadata about what your plugin is called and what it\\'s supposed to do. It is used by the Edge Code Repository when submitting plugins.\\n\\n4. Add ECR media\\n\\nCreate a ecr-meta directory and populate it with the following text and media:\\n\\necr-science-description.md - Markdown with in depth description of the science being done here (1 page of text).\\n\\necr-icon.jpg - An icon for the project/work 512x512px.\\n\\necr-science-image.jpg - A science image for the project with a minimum size of 1920x1080px.\\n\\nBeyond the basics\\n\\nMore about the publish function\\n\\nIn the previous example, we saw the most basic usage of the publish function. Now, we want to talk about a couple additional features available to you.'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='First, metadata can be added to measurements to provide context to how a measurement was created. For example, suppose we had a left and a right facing camera on a node and wanted to track which one was used.\\n\\npython plugin.publish(\"my.sensor.name\", 123, meta={\"camera\": \"left\"})\\n\\nThis will bind the meta data together with the measurement and will be available throughout the rest of the data pipeline.\\n\\nSecond, you can explicitly provide a timestamp for situations where you have more information on when a measurement was taken. For example:\\n\\npython plugin.publish(\"my.sensor.name\", 123, timestamp=my_timestamp_in_ns)\\n\\nNote: Timestamps are expected to be in nanoseconds since epoch. In Python 3.7+, this is available through the standard time.time_ns() function.\\n\\nSubscribing to other measurements\\n\\nPlugins can subscribe to measurements published by other plugins running on the same node. This allows users to leverage existing work or compose a larger application of multiple independent components.\\n\\nThe followng basic example simply waits for measurements named \"my.sensor.name\" and prints the value it received.\\n\\n```python from waggle.plugin import Plugin from random import random\\n\\nwith Plugin() as plugin: plugin.subscribe(\"my.sensor.name\")\\n\\nwhile True:\\n    msg = plugin.get()\\n    print(\"Another plugin published my.sensor.name value\", msg.value)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nIn the case you need multiple multiple measurements, you can simply use:\\n\\npython plugin.subscribe(\"my.sensor.name1\", \"my.sensor.name2\", \"my.sensor.name3\")\\n\\nTo differentiate the results, you can use the message name:\\n\\npython msg = plugin.get() if msg.name == \"my.sensor.name1\": # do something elif msg.name == \"my.sensor.name2\": # do something else\\n\\nIn more complex examples, the full message metadata can also be used to differentiate behavior:\\n\\n```python plugin.subscribe(\"env.temperature\")\\n\\nwhile True: msg = plugin.get() if msg.meta.get(\"sensor\") == \"bme280\": # do something elif msg.meta.get(\"sensor\") == \"bme680\": # do something else'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nMore about the subscribe function\\n\\nThe subscribe function can match two kinds of wildcard patterns. Measurement names are treated as \"segments\" broken up by a dot and we can match various segments using the star and hash operators.\\n\\nFirst, we can match a single wildcard segment using the \"my.sensor.*\" pattern. This will match all measurements with exactly three segments and whose first segment is \"my\", second segment is \"sensor\" and third segment can be anything.\\n\\nSecond, we can match zero or more segments using the \"my.#\" pattern. This will match all measurements whose first segment is \"my\" like \"my.sensor\", \"my.sensor.name\" or \"my.sensor.name.is.cool\".\\n\\nWorking with camera and microphone data\\n\\npywaggle provides a simple abstraction to cameras and microphones.\\n\\nAccessing a video stream\\n\\n```python from waggle.plugin import Plugin from waggle.data.vision import Camera import time\\n\\nuse case 1: take a snapshot and process\\n\\nwith Plugin() as plugin: sample = Camera().snapshot() # do processing result = process(sample.data) plugin.publish(\"my.measurement\", result, timestampe=sample.timestamp)\\n\\nuser case 2: process camera frames\\n\\nwith Plugin() as plugin, Camera() as camera: # process samples from video stream for sample in camera.stream(): count = count_cars_in_image(sample.data) if count > 10: sample.save(\"cars.jpg\") plugin.upload_file(\"cars.jpg\")'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nThe camera.snapshot() function returns a camera frame. The function provides a convenient way to capture a frame and process it.\\n\\nThe camera.stream() function yields a sequence of ImageSample with the following properties:\\n\\nsample.data. captured image\\'s numpy data array.\\n\\nsample.timestamp. captured image\\'s nanosecond timestamp.\\n\\nAdditionally, the Camera class accepts URLs and video files as input. For example:\\n\\n```python\\n\\nopen an mjpeg-over-http stream\\n\\ncamera = Camera(\"http://camera-server/profile1.mjpeg\")\\n\\nopen an rtsp stream\\n\\ncamera = Camera(\"rtsp://camera-server/v0.mp4\")\\n\\nopen a local file using file:// url\\n\\ncamera = Camera(\"file://path/to/my_cool_video.mp4\")\\n\\nopen a camera by device id (when plugin runs on a node)\\n\\ncamera = Camera(\"bottom_camera\") ```\\n\\nCamera buffering and use cases\\n\\n```\\n\\nQ1. How often do you need camera frames? A1. (As many as possible) ---> Refer to use case 1 A2. (Occasionally) ---> Go to Q2\\n\\nQ2. How sensitive is your application to a short delay when capturing an image? A1. (Very sensitive) ---> Refer to use case 1 A2. (A second is ok) ---> Refer to use case 2 ```\\n\\nThe Camera class wrapped in the Python with statement runs a background thread to keep up with the camera stream. This allows users to get the latest frame whenever .stream() or .snapshot() are called. However, this may be uncessary when users want to close the stream after grabbing a frame or the Camera class is used with a file, not a stream.\\n\\nTherefore, it is highly recommended to use the Camera class with the Python with statement when users want to process consequtive frames.\\n\\nUse case 1\\n\\n```python from time import sleep from waggle.data.vision import Camera\\n\\nwith Camera() as camera: former_frame = camera.snapshot() sleep(5) # the current_frame gets the latest frame current_frame = camera.snapshot() calculate_motion(current_frame, former_frame)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nFor simple grab-and-go use cases, users use the Camera class without the with statement to avoid the background process and its resource consumption.\\n\\nUse case 2 ```python from time import sleep from waggle.data.vision import Camera\\n\\nThe Camera class closes the stream after obtaining\\n\\na frame\\n\\ncamera = Camera() former_frame = camera.snapshot() sleep(5)\\n\\nThe Camera class opens the stream and grabs a frame\\n\\ncurrent_frame = camera.snapshot() calculate_motion(current_frame, former_frame) ```\\n\\nRecording video data\\n\\n```python from waggle.data.vision import Camera\\n\\ncamera = Camera()\\n\\nrecord a 30-second video from the camera\\n\\nvideo = camera.record(duration=30) with video: for frame in video: process(frame.data) ```\\n\\nThe Camera class allows users to record a video from camera and store the clip into a file. Because it relies on ffmpeg user code and its container (if in a Docker container) must have ffmpeg installed. You may install it as follow,\\n\\n```bash\\n\\nfor ubuntu\\n\\napt-get update && apt-get install -y ffmpeg ```\\n\\nAlso, the .record() function may NOT be used with Python with statement for USB cameras.\\n\\n```python from waggle.data.vision import Camera\\n\\nthe camera is a USB camera\\n\\ndevice = \"/dev/camera0\" with Camera(device) as camera: # this raises an exception as the camera stream is already open by the with statement video = camera.record(duration=30)\\n\\nUSB cameras can be used as below\\n\\nvideo = Camera(device).record(duration=30) ```\\n\\nRecording audio data\\n\\n```python from waggle.plugin import Plugin from waggle.data.audio import Microphone import time\\n\\nwith Plugin() as plugin, Microphone() as microphone: # record and upload a 10s sample periodically while True: sample = microphone.record(10) sample.save(\"sample.ogg\") plugin.upload_file(\"sample.ogg\") time.sleep(300)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nSimilar to ImageSample, AudioSample provide the following properties:\\n\\nsample.data. captured audio\\'s numpy data array.\\n\\nsample.timestamp. captured audio\\'s nanosecond timestamp.\\n\\nsample.samplerate. captured audio\\'s sample rate.\\n\\nAudioFolder and ImageFolder for testing\\n\\nWe provide a couple simple classes to provide audio and image data from a directory for testing.\\n\\nIn the following example, we assume we have directories:\\n\\ntxt audio_data/ example1.ogg example2.ogg ... image_data/ img1.png cat-7.png example10.jpg ...\\n\\nWe can load up all the audio files in the audio_data folder for testing as follows:\\n\\n```python from waggle.data.audio import AudioFolder\\n\\ndataset = AudioFolder(\"audio_data\")\\n\\nfor sample in dataset: process_data(sample.data) ```\\n\\nSimilarly, we can do something similar for all the image files in the image_data folder.\\n\\n```python from waggle.data.vision import ImageFolder\\n\\ndataset = ImageFolder(\"image_data\")\\n\\nfor sample in dataset: process_image_frame(sample.data) ```\\n\\nAdvanced: Choosing a color format\\n\\nBy default, the waggle.data.vision submodule uses an RGB color format. If you need more control, you can specify one of RGB or BGR to both the Camera and ImageFolder objects as follows:\\n\\n```python from waggle.data.vision import Camera, ImageFolder, RGB, BGR\\n\\nuse BGR data instead of RGB\\n\\ncamera = Camera(format=BGR)\\n\\nuse BGR data instead of RGB\\n\\ncamera = ImageFolder(format=BGR)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nAdvanced: Timing a block\\n\\nThe Plugin class provides a simple utility for timing how long a block of code takes.\\n\\nThe following example shows how we can instrument our code using a typical AI/ML example plugin.\\n\\n```python from waggle.plugin import Plugin\\n\\nwith Plugin() as plugin: # measures duration of input block and publishes to plugin.duration.input with plugin.timeit(\"plugin.duration.input\"): get_inputs(...)\\n\\n# measures duration of inference block and publishes to plugin.duration.inference\\nwith plugin.timeit(\"plugin.duration.inference\"):\\n    do_inference(...)\\n\\npublish_results(...)\\n\\n```\\n\\nIn the example above, the duration of the input and inference steps are measured and then the plugin publishes the duration in nanoseconds to the name provided to plugin.timeit as each block finishes.\\n\\nSeeing the internal details\\n\\nIf we run the basic example, the only thing we\\'ll see is the message \"publishing a value!\" every second. If you need to see more details, pywaggle is designed to easily interface with Python\\'s standard logging module. To enable debug logging, simply make the following additions:\\n\\n```python from waggle.plugin import Plugin from time import sleep\\n\\n1. import standard logging module\\n\\nimport logging\\n\\n2. enable debug logging\\n\\nlogging.basicConfig(level=logging.DEBUG)\\n\\nwith Plugin() as plugin: while True: sleep(1) print(\"publishing a value!\") plugin.publish(\"my.sensor.name\", 123)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nYou should see a lot of information like:\\n\\ntext DEBUG:waggle.plugin:starting plugin worker thread DEBUG:waggle.plugin:connecting to rabbitmq broker at rabbitmq:5672 with username \"plugin\" DEBUG:waggle.plugin:rabbitmq connection error: [Errno 8] nodename nor servname provided, or not known publishing a value! DEBUG:waggle.plugin:adding message to outgoing queue: Message(name=\\'my.sensor.name\\', value=123, timestamp=1619628240863845000, meta={}) DEBUG:waggle.plugin:connecting to rabbitmq broker at rabbitmq:5672 with username \"plugin\" DEBUG:waggle.plugin:rabbitmq connection error: [Errno 8] nodename nor servname provided, or not known publishing a value!\\n\\nThe most important lines are:\\n\\ntext publishing a value! DEBUG:waggle.plugin:adding message to outgoing queue: Message(name=\\'my.sensor.name\\', value=123, timestamp=1619628240863845000, meta={})\\n\\nThese are telling us that our messages are being queued up in an outgoing queue to be shipped.\\n\\nYou\\'ll also see a number of messages related to rabbitmq.\\n\\nThese are simply indicating the our plugin is waiting to connect to the Waggle ecosystem. This is normal when testing a standalone plugin without the rest of the Waggle stack. Plugins will simply queue up measurements in-memory until they exit.'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/README.md'}, page_content='Waggle Module\\n\\nThis page gives an overview of the core functionality provided by this module.\\n\\nPlugin Submodule\\n\\nProvides functionality for publishing sensor data and for processing messages.\\n\\nProtocol Submodule\\n\\nProvides functionality for packing and unpacking sensor and messaging data.')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Split Markdown files\n",
    "md_splitter = MarkdownTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "md_splits = md_splitter.split_documents(md_docs)\n",
    "md_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.87 ms, sys: 726 μs, total: 4.6 ms\n",
      "Wall time: 5.14 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='import unittest\\nimport sage_data_client\\nfrom io import BytesIO\\nfrom datetime import datetime, timedelta\\nimport pandas as pd'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='class TestQuery(unittest.TestCase):\\n    def assertValueResponse(self, df):\\n        self.assertIn(\"name\", df.columns)\\n        df.name.str\\n        self.assertIn(\"timestamp\", df.columns)\\n        df.timestamp.dt\\n        self.assertIn(\"value\", df.columns)\\n\\n    def test_empty_response(self):\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"-2000d\",\\n                filter={\\n                    \"name\": \"should.not.every.exist.XYZ\",\\n                },\\n            )\\n        )\\n\\n    def test_check_one_of_head_or_tail(self):\\n        with self.assertRaises(ValueError):\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00\",\\n                end=\"2021-01-01T10:31:00\",\\n                head=3,\\n                tail=3,\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n\\n    def test_queries(self):\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00\",\\n                end=\"2021-01-01T10:31:00\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00Z\",\\n                end=\"2021-01-01T10:31:00Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00.123Z\",\\n                end=\"2021-01-01T10:31:00.123Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00.123456Z\",\\n                end=\"2021-01-01T10:31:00.123456Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01 10:30:00\",\\n                end=\"2021-01-01 10:31:00\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=datetime(2021, 1, 1, 10, 31, 0),\\n                end=datetime(2021, 1, 1, 10, 32, 0),\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=pd.to_datetime(\"2021-01-01 10:30:00\"),\\n                end=pd.to_datetime(\"2021-01-01 10:31:00\"),\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        for dt in [\"-30s\", \"-3m\", \"-3min\", \"-1d\", \"-1w\"]:\\n            self.assertValueResponse(\\n                sage_data_client.query(\\n                    start=dt,\\n                    tail=1,\\n                    filter={\\n                        \"name\": \"env.temperature\",\\n                    },\\n                )\\n            )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"-4h\",\\n                end=\"-2h\",\\n                tail=1,\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='for dt in [\\n            timedelta(seconds=-30),\\n            timedelta(minutes=-1),\\n            timedelta(hours=-1),\\n            timedelta(days=-1),\\n        ]:\\n            self.assertValueResponse(\\n                sage_data_client.query(\\n                    start=dt,\\n                    tail=1,\\n                    filter={\\n                        \"name\": \"env.temperature\",\\n                    },\\n                )\\n            )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='def test_load(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2021-10-14T21:42:21.149425156Z\",\"name\":\"env.temperature\",\"value\":21.74,\"meta\":{\"host\":\"0000dca632a3069f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31f\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"env.temperature\",\"value\":26.09,\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}\\n{\"timestamp\":\"2021-10-14T21:42:19.087014343Z\",\"name\":\"env.temperature\",\"value\":28.14,\"meta\":{\"host\":\"0000dca632a3074d.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc73\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W024\"}}\\n{\"timestamp\":\"2021-10-14T21:42:23.475857326Z\",\"name\":\"env.temperature\",\"value\":28.16,\"meta\":{\"host\":\"0000dca632a3076b.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc6d\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01B\"}}\\n{\"timestamp\":\"2021-10-14T21:42:34.995766556Z\",\"name\":\"env.temperature\",\"value\":33.27,\"meta\":{\"host\":\"0000dca632a3078f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bdc7\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W020\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.803472584Z\",\"name\":\"env.temperature\",\"value\":9.8,\"meta\":{\"host\":\"0000dca632a30792.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc42\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01D\"}}\\n{\"timestamp\":\"2021-10-14T21:42:30.9261079Z\",\"name\":\"env.temperature\",\"value\":25.63,\"meta\":{\"host\":\"0000dca632a307b6.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc8c\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W039\"}}'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='{\"timestamp\":\"2021-10-14T21:42:24.228048661Z\",\"name\":\"env.temperature\",\"value\":23.96,\"meta\":{\"host\":\"0000dca632a307bf.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc7d\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W028\"}}\\n{\"timestamp\":\"2021-10-14T21:42:13.914329997Z\",\"name\":\"env.temperature\",\"value\":21.84,\"meta\":{\"host\":\"0000dca632a307e6.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d05a1c2\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W02C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:17.300924641Z\",\"name\":\"env.temperature\",\"value\":30.12,\"meta\":{\"host\":\"0000dca632a307fb.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c328\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W016\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertEqual(len(df), 10)\\n        self.assertValueResponse(df)'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='def test_load_small_numbers(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2023-09-25T19:26:26.18944512Z\",\"name\":\"sensor_body_temperature\",\"value\":-655230609742226300000,\"meta\":{\"applicationId\":\"ac81e18b-1925-47f9-839a-27d999a8af55\",\"applicationName\":\"ATMOS test app\",\"devAddr\":\"00f06b4b\",\"devEui\":\"98208e0000032a15\",\"deviceName\":\"MFR Node\",\"deviceProfileId\":\"cf2aec2f-03e1-4a60-a32c-0faeef5730d8\",\"deviceProfileName\":\"MFR node\",\"host\":\"0000e45f014caee8.ws-rpi\",\"node\":\"000048b02d15bc8c\",\"plugin\":\"registry.sagecontinuum.org/flozano/lorawan-listener:0.0.6\",\"task\":\"lorawan-listener\",\"tenantId\":\"52f14cd4-c6f1-4fbd-8f87-4025e1d49242\",\"tenantName\":\"ChirpStack\",\"vsn\":\"W039\",\"zone\":\"shield\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertValueResponse(df)\\n        self.assertEqual(len(df), 1)\\n\\n    def test_load_empty_file(self):\\n        df = sage_data_client.load(\"tests/test-empty.ndjson\")\\n        self.assertEqual(len(df), 0)\\n        self.assertValueResponse(df)\\n\\n        df = sage_data_client.load(\"tests/test-empty.ndjson.gz\")\\n        self.assertEqual(len(df), 0)\\n        self.assertValueResponse(df)\\n\\n    def test_load_compressed_file(self):\\n        df1 = sage_data_client.load(\"tests/test-data.ndjson\")\\n        df2 = sage_data_client.load(\"tests/test-data.ndjson.gz\")\\n        self.assertEqual(len(df1), 1611)\\n        self.assertEqual(len(df2), 1611)\\n        # NOTE In Pandas Nones and NaNs do not equal themselves, so will fill them to make df1 == df2 work.\\n        self.assertTrue((df1.fillna(\"\") == df2.fillna(\"\")).all().all())'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='def test_mixed_types(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2021-10-14T21:42:21.149425156Z\",\"name\":\"test\",\"value\":21.74,\"meta\":{\"host\":\"0000dca632a3069f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31f\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"test\",\"value\":\"26.09\",\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"test\",\"value\":123,\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertEqual(len(df), 3)\\n        self.assertValueResponse(df)\\n        self.assertAlmostEqual(df.iloc[0].value, 21.74)\\n        self.assertEqual(df.iloc[1].value, \"26.09\")\\n        self.assertEqual(df.iloc[2].value, 123)\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/load_data_from_file.py'}, page_content='\"\"\"\\nThis example demonstrates loading a local data file containing 5 minutes of temperature data\\nand printing the mean value grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# load results from local file\\ndf = sage_data_client.load(\"data.json\")\\n\\n# print number of results of each name\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.mean())'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/trigger-stream.py'}, page_content='\"\"\"\\nThis is an example of a simple edge-to-cloud stream trigger which uses sage-data-client\\nto watch the latest internal temperature values and print records which exceed a threshold.\\n\\nAlthough it\\'s simple, this example could easily be extended in multiple ways. For example:\\n* Instead of just printing, an alert could be posted to Slack.\\n* Instead of a fixed threshold, you could learn a moving average per node and flag outliers.\\n\\nNote: In the future, this kind of streaming functionality *might* be provided by sage-data-client,\\nbut for now you can adapt this example to fit you use case.\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\nimport time\\n\\n\\ndef watch(start=None, filter=None):\\n    if start is None:\\n        start = pd.Timestamp.utcnow()\\n\\n    while True:\\n        df = sage_data_client.query(\\n            start=start,\\n            filter=filter,\\n        )\\n\\n        if len(df) > 0:\\n            start = df.timestamp.max()\\n            yield df\\n\\n        time.sleep(3.0)\\n\\n\\ndef main():\\n    filter = {\\n        \"name\": \"env.temperature\",\\n        \"sensor\": \"bme280\",\\n    }\\n\\n    threshold = 50.0\\n\\n    for df in watch(filter=filter):\\n        # print values which exceed threshold\\n        print(df[df.value > threshold].sort_values(\"timestamp\"))\\n\\n\\nif __name__ == \"__main__\":\\n    main()'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/print_rain_event_image_urls.py'}, page_content='\"\"\"\\nThis example demonstrates cross referencing rain gauge data to find rainy images. It outputs a list\\nof urls which can be saved and downloaded as follows:\\n\\npython3 print_rain_event_image_urls.py > urls.txt\\nwget -r -N -i urls.txt\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\n\\nvsn = \"W039\"\\n\\n# query raingauge data for the last week\\ndf = sage_data_client.query(\\n    start=\"2021-12-20\",\\n    end=\"2021-12-27\",\\n    filter={\\n        \"name\": \"env.raingauge.acc\",\\n        \"vsn\": vsn,\\n    }\\n)\\n\\n# compute mean rain in hour window\\nmean_acc = df.resample(\"1h\", on=\"timestamp\").value.mean()\\n\\n# find rain accumulation events\\nrain_events = mean_acc[mean_acc > 0]\\n\\n# collect uploads in each rain event window\\nuploads = pd.concat(sage_data_client.query(\\n        start=ts,\\n        end=ts + pd.to_timedelta(\"1h\"),\\n        filter={\\n            \"name\": \"upload\",\\n            \"vsn\": vsn,\\n            \"task\": \"imagesampler-top\",\\n        }\\n    ) for ts in rain_events.index)\\n\\n# print all urls found\\nfor url in uploads.value.values:\\n    print(url)'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/pressure_event_trigger.py'}, page_content='\"\"\"\\nThis example is a skeleton of how to poll the data system every minute for unusual\\npressure events.\\n\\nIn this case, events are determined windows with a stddev above an example\\nthreshold. For applications, you will need to provide your own criteria for\\nevents.\\n\\nAdditionally, you will need to provide a specific mechanism to carry out the\\nalerts (ex. email, Slack, dedicated alerting / ticketing system, etc).\\n\"\"\"\\nimport sage_data_client\\nimport time\\n\\nwhile True:\\n    # query pressure data in recent 10 minute window\\n    df = sage_data_client.query(\\n        start=\"-10m\",\\n        filter={\\n            \"name\": \"env.pressure\",\\n            \"sensor\": \"bme680\",\\n        }\\n    )\\n\\n    # compute stddev for nodes\\' pressure data in window\\n    std = df.groupby(\"meta.vsn\").value.std()\\n\\n    # find all pressure events exceeding an example threshold\\n    events = std[std > 8.0]\\n\\n    # \"post\" vsn to alert system\\n    for vsn in events.index:\\n        print(f\"post {vsn} to alert system\")\\n\\n    time.sleep(60)'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/temperature_stats.py'}, page_content='\"\"\"\\nThis example demonstrates querying all temperature data and printing basic stats grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# query and load data into pandas data frame\\ndf = sage_data_client.query(\\n    start=\"-1h\",\\n    filter={\\n        \"name\": \"env.temperature\",\\n    }\\n)\\n\\n# print stats of the temperature data grouped by node + sensor.\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"]))'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/join_queries.py'}, page_content='\"\"\"\\nThis example demonstrates one approach for combining multiple queries by resampling\\nresults into 30 minute windows and merging those into a new data frame.\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\n\\n\\ndef join_resampled_queries(start, end, window, filters):\\n    \"\"\"\\n    join_resampled_queries joins resampled data for a set of filters together\\n    into a single data frame\\n    \"\"\"\\n    return pd.DataFrame({\\n        name: sage_data_client.query(\\n            start=start,\\n            end=end,\\n            filter=filter,\\n        ).resample(window, on=\"timestamp\").value.mean()\\n        for name, filter in filters.items()\\n    })\\n\\n\\ndef main():\\n    start = \"2022-01-10T00:00:00Z\"\\n    end = \"2022-01-11T00:00:00Z\"\\n    vsn = \"W023\"\\n\\n    # combine lat, lon, temperature, pressure and humidity into data frame\\n    df = join_resampled_queries(start, end, \"30min\", {\\n        \"lat\": {\\n            \"name\": \"sys.gps.lat\",\\n            \"vsn\": vsn,\\n        },\\n        \"lon\": {\\n            \"name\": \"sys.gps.lon\",\\n            \"vsn\": vsn,\\n        },\\n        \"temperature\": {\\n            \"name\": \"env.temperature\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n        \"pressure\": {\\n            \"name\": \"env.pressure\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n        \"humidity\": {\\n            \"name\": \"env.relative_humidity\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n    })\\n\\n    # print out data for quick inspection\\n    print(df)\\n\\n    # save data to csv\\n    df.to_csv(\"combined.csv\")\\n\\n\\nif __name__ == \"__main__\":\\n    main()'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/raingauge_totals.py'}, page_content='\"\"\"\\nThis example demonstrates querying rain gauge data and printing the total\\nnumber of measurements grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# query and load data into pandas data frame\\ndf = sage_data_client.query(\\n    start=\"-1h\",\\n    filter={\\n        \"name\": \"env.raingauge.*\",\\n    }\\n)\\n\\n# print number of results of each name\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size())'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/trigger-batch.py'}, page_content='\"\"\"\\nThis is an example of a simple edge-to-cloud batch trigger which uses sage-data-client\\nto gather and aggregate internal temperature data every 5 minutes and prints all\\nnodes which exceed a threshold.\\n\\nAlthough it\\'s simple, this example could easily be extended in multiple ways. For example:\\n* Instead of just printing, an alert could be posted to Slack.\\n* Instead of a fixed threshold, the typical value across all nodes could be used to determine outliers.\\n\"\"\"\\nimport sage_data_client\\nimport time\\n\\n\\ndef main():\\n    filter = {\\n        \"name\": \"env.temperature\",\\n        \"sensor\": \"bme280\",\\n    }\\n\\n    threshold = 55.0\\n\\n    while True:\\n        # get the last 5m of temperature data\\n        df = sage_data_client.query(start=\"-5m\", filter=filter)\\n\\n        # get mean temperature by node in batch query\\n        mean_temps = df.groupby(\"meta.vsn\").value.mean()\\n\\n        # print values which exceed threshold\\n        print(mean_temps[mean_temps > threshold])\\n\\n        # wait 5m\\n        time.sleep(300)\\n\\n\\nif __name__ == \"__main__\":\\n    main()'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='from gzip import GzipFile\\nimport json\\nfrom pathlib import Path\\nfrom urllib.request import urlopen, Request\\nimport pandas as pd\\n\\n\\ndef resolve_time(t):\\n    try:\\n        return pd.to_datetime(t)\\n    except (TypeError, ValueError):\\n        pass\\n    return pd.to_datetime(\"now\", utc=True) + pd.to_timedelta(t)\\n\\n\\ndef timestr(t):\\n    return t.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='def query(\\n    start,\\n    end=None,\\n    head: int = None,\\n    tail: int = None,\\n    experimental_func=None,\\n    bucket: str = None,\\n    filter: dict = None,\\n    endpoint: str = \"https://data.sagecontinuum.org/api/v1/query\",\\n) -> pd.DataFrame:\\n    \"\"\"\\n    query makes a query request to the data API and returns the results in a data frame.\\n\\n    Parameters\\n    ----------\\n    start : query start time, required\\n        Timestamps can be a relative like \"-1h\" or absolute like \"2021-05-01T10:30:00Z\".\\n\\n    end : query end time, default: None\\n        Timestamps can be a relative like \"-1h\" or absolute like \"2021-05-01T10:30:00Z\".\\n\\n    head : limit query response to earliest `head` records, default: None (only one of `head` or `tail` can be provided)\\n\\n    tail : limit query response to latest `tail` records, default: None (only one of `head` or `tail` can be provided)\\n\\n    experimental_func : aggregation function to apply to series.\\n\\n    bucket: name of bucket to query\\n\\n    filter : dictionary of query filters, default: None\\n\\n    endpoint : url of query api, default: \"https://data.sagecontinuum.org/api/v1/query\"\\n\\n    Returns\\n    -------\\n    result : pandas.DataFrame\\n        The data frame will contain the query response records.\\n\\n        See the Returns section for the `load` function for more details.\\n\\n    Examples\\n    --------\\n\\n    Querying and perform simple data aggregation\\n\\n    ```python\\n    import sage_data_client\\n\\n    # query and load data into pandas data frame\\n    df = sage_data_client.query(\\n        start=\"-1h\",\\n        filter={\\n            \"name\": \"env.temperature\",\\n        }\\n    )\\n\\n    # print results in data frame\\n    print(df)\\n\\n    # meta columns are expanded into meta.fieldname. for example, here we print the unique nodes\\n    print(df[\"meta.node\"].unique())'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='# print stats of the temperature data grouped by node + sensor.\\n    print(df.groupby([\"meta.node\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"]))\\n    ```\\n    \"\"\"\\n    # build query\\n    q = {\"start\": timestr(resolve_time(start))}\\n    if end is not None:\\n        q[\"end\"] = timestr(resolve_time(end))\\n    if filter is not None:\\n        q[\"filter\"] = filter\\n    if head is not None and tail is not None:\\n        raise ValueError(\"only one of `head` or `tail` can be provided\")\\n    elif head is not None:\\n        q[\"head\"] = head\\n    elif tail is not None:\\n        q[\"tail\"] = tail\\n    if experimental_func is not None:\\n        q[\"experimental_func\"] = experimental_func\\n    if bucket is not None:\\n        q[\"bucket\"] = bucket\\n\\n    data = json.dumps(q).encode()\\n    headers = {\"Accept-Encoding\": \"gzip\"}\\n    req = Request(endpoint, data, headers=headers)\\n\\n    with urlopen(req) as f:\\n        content_encoding = f.headers.get(\"Content-Encoding\", \"\")\\n        if \"gzip\" in content_encoding:\\n            f = GzipFile(fileobj=f, mode=\"rb\")\\n        return load(f)'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='def load(path_or_buf) -> pd.DataFrame:\\n    \"\"\"\\n    load reads a path or file like object containing a response from the data api and returns the results in a data frame.\\n\\n    Parameters\\n    ----------\\n    path_or_buf : path like or file like object\\n\\n    Returns\\n    -------\\n    result : pandas.DataFrame\\n        The data frame will contain the query response records. Standard columns names are:\\n\\n        `name`: measurement name (ex. \"env.temperature\")\\n\\n        `timestamp`: measurement timestamp (nanoseconds since epoch resolution)\\n\\n        `value`: measurement value\\n\\n        Metadata fields like \"node\" and \"vsn\" are stored in columns named \"meta.node\" or \"meta.vsn\".\\n\\n    Examples\\n    --------\\n\\n    Loading saved query results from a file\\n\\n    Suppose we\\'ve saved the results of a query to a file `data.json`. We can load them using the following:\\n\\n    ```python\\n    import sage_data_client\\n\\n    # load results from local file\\n    df = sage_data_client.load(\"data.ndjson\")\\n\\n    # print number of results of each name\\n    print(df.groupby([\"meta.node\", \"name\"]).size())\\n    ```\\n    \"\"\"\\n    if isinstance(path_or_buf, str):\\n        if path_or_buf.endswith(\".gz\"):\\n            with GzipFile(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n        else:\\n            with open(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n\\n    if isinstance(path_or_buf, Path):\\n        if path_or_buf.suffix == \".gz\":\\n            with GzipFile(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n        else:\\n            with open(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n\\n    return _load(path_or_buf)\\n\\n\\ndef _load_row(r):\\n    input = json.loads(r)\\n    output = {}\\n    output[\"timestamp\"] = pd.to_datetime(input[\"timestamp\"], unit=\"ns\", utc=True)\\n    output[\"name\"] = input[\"name\"]\\n    output[\"value\"] = input[\"value\"]\\n    for k, v in input[\"meta\"].items():\\n        output[f\"meta.{k}\"] = v\\n    return output'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='def _load(fileobj) -> pd.DataFrame:\\n    df = pd.DataFrame(map(_load_row, fileobj))\\n\\n    # if dataframe is empty, return empty with known columns\\n    if len(df) == 0:\\n        return pd.DataFrame(\\n            {\\n                \"timestamp\": pd.to_datetime([], utc=True),\\n                \"name\": pd.Series([], dtype=str),\\n                \"value\": [],\\n            }\\n        )\\n\\n    return df'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/__init__.py'}, page_content='\"\"\"\\nsage_data_client - Official Sage Python data API client.\\n========================================================\\n\\nsage_data_client goals are to make writing queries and working with the results easy. It does this by:\\n\\n* Providing a simple query function which talks to the data API.\\n* Providing the results in an easy to use [Pandas](https://pandas.pydata.org) data frame.\\n\"\"\"\\nfrom .query import query, load'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='import unittest\\nfrom pathlib import Path\\nimport json\\nfrom tempfile import TemporaryDirectory\\nimport time\\nfrom datetime import datetime\\nimport os\\nimport pika\\nimport subprocess\\n\\nfrom waggle.plugin import Plugin, PluginConfig, Uploader, get_timestamp\\nimport wagglemsg\\n\\n# TODO(sean) add integration testing against rabbitmq\\n# TODO(sean) clean up the queue interface. it would be better to not know about the plugin.send / plugin.recv queues explicitly.'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='class TestPlugin(unittest.TestCase):\\n    def test_publish(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test.int\", 1)\\n            plugin.publish(\"test.float\", 2.0)\\n            plugin.publish(\"test.str\", \"three\")\\n            plugin.publish(\\n                \"cows.total\",\\n                391,\\n                meta={\\n                    \"camera\": \"bottom_left\",\\n                },\\n            )\\n\\n    def test_publish_check_reserved(self):\\n        with Plugin() as plugin:\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"upload\", \"path/to/data\")\\n\\n    def test_get(self):\\n        with Plugin() as plugin:\\n            plugin.subscribe(\"raw.#\")\\n            with self.assertRaises(TimeoutError):\\n                plugin.get(timeout=0)\\n            with self.assertRaises(TimeoutError):\\n                plugin.get(timeout=0.001)\\n\\n            msg = wagglemsg.Message(\"test\", 1.0, 0, {})\\n            plugin.recv.put(msg)\\n            msg2 = plugin.get(timeout=0)\\n            self.assertEqual(msg, msg2)\\n\\n    def test_get_timestamp(self):\\n        ts = get_timestamp()\\n        self.assertIsInstance(ts, int)\\n\\n    def test_valid_values(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test\", 1)\\n            plugin.publish(\"test\", 1.3)\\n            plugin.publish(\"test\", \"some string\")\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", b\"some bytes\")\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", [1, 2, 3])\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", {1: 1, 2: 2, 3: 3})'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='def test_valid_meta(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test\", 1, meta={\"k\": \"v\"})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": 10})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": 12.3})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": []})\\n\\n    def test_valid_timestamp(self):\\n        with Plugin() as plugin:\\n            # valid int, nanosecond timestamp\\n            plugin.publish(\"test\", 1, timestamp=1649694687904754000)\\n\\n            # must prevent a float type timestamp\\n            ts = datetime(2022, 1, 1, 0, 0, 0).timestamp()\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, timestamp=ts)\\n\\n            # must prevent int timestamp in seconds from being loaded by flagging\\n            # timestamps that are too early.\\n            testcases = [\\n                datetime(2022, 1, 1, 0, 0, 0),\\n                datetime(3000, 1, 1, 0, 0, 0),\\n                datetime(5000, 1, 1, 0, 0, 0),\\n            ]\\n\\n            for dt in testcases:\\n                with self.assertRaises(ValueError):\\n                    plugin.publish(\"test\", 1, timestamp=int(dt.timestamp()))\\n\\n    def test_valid_publish_names(self):\\n        with Plugin() as plugin:\\n            with self.assertRaises(TypeError):\\n                plugin.publish(None, 0)\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"\", 0)\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\".\", 0)\\n\\n            # check for reserved names\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"upload\", 0)\\n\\n            # use _ instead of -\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"my-metric\", 0)\\n            # correct alternative\\n            plugin.publish(\"my_metric\", 0)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='# assert len(name) <= 128\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"x\" * 129, 0)\\n\\n            # no empty parts allowed\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"vision.count..bird\", 0)\\n            # correct alternative\\n            plugin.publish(\"vision.count.bird\", 0)\\n\\n            # no spaces allowed\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"sys.cpu temp\", 0)\\n            # correct alternative\\n            plugin.publish(\"sys.cpu_temp\", 0)\\n\\n    # TODO(sean) refactor messaging part to make testing this cleaner\\n    def test_upload_file(self):\\n        with TemporaryDirectory() as tempdir:\\n            pl = Plugin(\\n                PluginConfig(\\n                    host=\"fake-rabbitmq-host\",\\n                    port=5672,\\n                    username=\"plugin\",\\n                    password=\"plugin\",\\n                    app_id=\"0668b12c-0c15-462c-9e06-7239282411e5\",\\n                ),\\n                uploader=Uploader(Path(tempdir, \"uploads\")),\\n            )\\n\\n            data = b\"here some data in a data\"\\n            upload_path = Path(tempdir, \"myfile.txt\")\\n            upload_path.write_bytes(data)\\n\\n            pl.upload_file(upload_path)\\n            item = pl.send.get_nowait()\\n            msg = wagglemsg.load(item.body)\\n            self.assertEqual(item.scope, \"all\")\\n            self.assertEqual(msg.name, \"upload\")\\n            self.assertIsNotNone(msg.timestamp)\\n            self.assertIsInstance(msg.value, str)\\n            self.assertIsNotNone(msg.meta)\\n            self.assertIn(\"filename\", msg.meta)\\n\\n    def test_timeit(self):\\n        with Plugin() as plugin:\\n            with plugin.timeit(\"dur\"):\\n                time.sleep(0.001)\\n            item = plugin.send.get(0.01)\\n            msg = wagglemsg.load(item.body)\\n            self.assertEqual(msg.name, \"dur\")'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='class TestUploader(unittest.TestCase):\\n    def test_upload_file(self):\\n        with TemporaryDirectory() as tempdir:\\n            uploader = Uploader(Path(tempdir, \"uploads\"))\\n\\n            data = b\"here some data in a data\"\\n\\n            upload_path = Path(tempdir, \"myfile.txt\")\\n            upload_path.write_bytes(data)\\n\\n            path = uploader.upload_file(upload_path)\\n            self.assertFalse(upload_path.exists())\\n\\n            self.assertEqual(data, Path(path, \"data\").read_bytes())\\n            meta = json.loads(Path(path, \"meta\").read_text())\\n            self.assertIn(\"timestamp\", meta)\\n            self.assertIn(\"shasum\", meta)\\n            self.assertEqual(meta[\"labels\"][\"filename\"], upload_path.name)\\n\\n\\ndef rabbitmq_available():\\n    try:\\n        subprocess.check_output([\"docker-compose\", \"exec\", \"rabbitmq\", \"true\"])\\n        return True\\n    except subprocess.CalledProcessError:\\n        return False\\n\\n\\ndef get_admin_connection():\\n    params = pika.ConnectionParameters(\\n        credentials=pika.PlainCredentials(\"admin\", \"admin\")\\n    )\\n    return pika.BlockingConnection(params)\\n\\n\\n@unittest.skipUnless(rabbitmq_available(), \"rabbitmq not available\")'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='class TestPluginWithRabbitMQ(unittest.TestCase):\\n    def setUp(self):\\n        os.environ[\"WAGGLE_PLUGIN_USERNAME\"] = \"plugin\"\\n        os.environ[\"WAGGLE_PLUGIN_PASSWORD\"] = \"plugin\"\\n        os.environ[\"WAGGLE_PLUGIN_HOST\"] = \"127.0.0.1\"\\n        os.environ[\"WAGGLE_PLUGIN_PORT\"] = \"5672\"\\n\\n        with get_admin_connection() as conn, conn.channel() as ch:\\n            ch.queue_purge(\"to-validator\")\\n\\n    def test_publish(self):\\n        now = time.time_ns()\\n\\n        with Plugin() as publisher:\\n            publisher.publish(\"test\", 123, meta={\"sensor\": \"bme680\"}, timestamp=now)\\n\\n        with get_admin_connection() as conn, conn.channel() as ch:\\n            _, _, body = ch.basic_get(\"to-validator\", auto_ack=True)\\n            msg = wagglemsg.load(body)\\n\\n        self.assertEqual(\\n            msg,\\n            wagglemsg.Message(\\n                name=\"test\",\\n                value=123,\\n                meta={\"sensor\": \"bme680\"},\\n                timestamp=now,\\n            ),\\n        )\\n\\n    def test_subscribe(self):\\n        msg = wagglemsg.Message(\\n            name=\"test\",\\n            value=123,\\n            meta={\"sensor\": \"bme680\"},\\n            timestamp=time.time_ns(),\\n        )\\n\\n        with Plugin() as subscriber:\\n            subscriber.subscribe(\"test\")\\n            time.sleep(1)\\n\\n            with get_admin_connection() as conn, conn.channel() as ch:\\n                ch.basic_publish(\"data.topic\", \"test\", wagglemsg.dump(msg))\\n\\n            msg2 = subscriber.get(1)\\n            self.assertEqual(msg, msg2)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='class TestPluginLogDir(unittest.TestCase):\\n    def test_log_dir(self):\\n        import sage_data_client\\n\\n        with TemporaryDirectory() as dir:\\n            dir = Path(dir)\\n\\n            # create dummy upload file\\n            upload_file = Path(dir, \"hello.txt\")\\n            upload_file.write_text(\"hello\")\\n\\n            timestamp = get_timestamp()\\n\\n            # set env var and run plugin\\n            try:\\n                os.environ[\"PYWAGGLE_LOG_DIR\"] = str(dir)\\n                with Plugin() as plugin:\\n                    plugin.publish(\"test\", 123, timestamp=timestamp)\\n                    plugin.publish(\\n                        \"test.with.meta\",\\n                        456,\\n                        meta={\"user\": \"data\"},\\n                        timestamp=timestamp + 10000,\\n                    )\\n                    plugin.upload_file(upload_file, timestamp=timestamp + 20000)\\n            finally:\\n                del os.environ[\"PYWAGGLE_LOG_DIR\"]\\n\\n            df = sage_data_client.load(Path(dir, \"data.ndjson\"))\\n\\n            # ensure records match what was published\\n            self.assertEqual(len(df), 3)\\n\\n            # TODO(sean) test timestamps\\n            self.assertEqual(df.loc[0, \"name\"], \"test\")\\n            self.assertEqual(df.loc[0, \"value\"], 123)\\n\\n            self.assertEqual(df.loc[1, \"name\"], \"test.with.meta\")\\n            self.assertEqual(df.loc[1, \"value\"], 456)\\n            self.assertEqual(df.loc[1, \"meta.user\"], \"data\")\\n\\n            self.assertEqual(df.loc[2, \"name\"], \"upload\")\\n            self.assertEqual(df.loc[2, \"meta.filename\"], \"hello.txt\")\\n\\n            # ensure all uploads exist\\n            for path in df[df.name == \"upload\"].value:\\n                self.assertTrue(Path(path).exists())\\n\\n\\ndef assertDictContainsSubset(t, a, b):\\n    t.assertLessEqual(a.items(), b.items())\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='import unittest\\nfrom waggle.data.audio import AudioFolder, AudioSample\\nfrom waggle.data.vision import RGB, BGR, ImageFolder, ImageSample, resolve_device\\nfrom waggle.data.timestamp import get_timestamp\\nimport numpy as np\\nfrom tempfile import TemporaryDirectory\\nfrom pathlib import Path\\nimport os.path\\nfrom itertools import product\\n\\n\\ndef generate_audio_data(samplerate, channels, dtype):\\n    if dtype == np.float32:\\n        return np.random.uniform(-1, 1, (samplerate, channels)).astype(dtype)\\n    if dtype == np.float64:\\n        return np.random.uniform(-1, 1, (samplerate, channels)).astype(dtype)\\n    if dtype == np.int16:\\n        return np.random.randint(\\n            -(2**15), 2**15, (samplerate, channels), dtype=dtype\\n        )\\n    if dtype == np.int32:\\n        return np.random.randint(\\n            -(2**31), 2**31, (samplerate, channels), dtype=dtype\\n        )\\n    raise ValueError(\"unsupported audio settings\")\\n\\n\\ndef generate_audio_sample(samplerate, channels, dtype):\\n    return AudioSample(generate_audio_data(samplerate, channels, dtype), samplerate, 0)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='class TestData(unittest.TestCase):\\n    def test_colors(self):\\n        for fmt in [RGB, BGR]:\\n            data = np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8)\\n            data2 = fmt.format_to_cv2(fmt.cv2_to_format(data))\\n            self.assertTrue(\\n                np.all(np.isclose(data, data2, 1.0)), f\"checking format {fmt}\"\\n            )\\n\\n    def test_resolve_device(self):\\n        self.assertEqual(\\n            resolve_device(Path(\"test.jpg\")), str(Path(\"test.jpg\").absolute())\\n        )\\n        self.assertEqual(\\n            resolve_device(\"file://path/to/test.jpg\"),\\n            str(Path(\"path/to/test.jpg\").absolute()),\\n        )\\n        self.assertEqual(\\n            resolve_device(\"http://camera-ip.org/image.jpg\"),\\n            \"http://camera-ip.org/image.jpg\",\\n        )\\n        self.assertEqual(\\n            resolve_device(\"rtsp://camera-ip.org/image.jpg\"),\\n            \"rtsp://camera-ip.org/image.jpg\",\\n        )\\n        self.assertEqual(resolve_device(0), 0)\\n\\n    def test_image_save(self):\\n        with TemporaryDirectory() as dir:\\n            sample = ImageSample(\\n                np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8), 0, RGB\\n            )\\n            for fmt in [\"jpg\", \"png\"]:\\n                name = f\"sample.{fmt}\"\\n                sample.save(os.path.join(dir, name))\\n                sample.save(Path(dir, name))\\n\\n    def test_image_save_load(self):\\n        with TemporaryDirectory() as dir:\\n            sample = ImageSample(\\n                np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8), 0, RGB\\n            )\\n            sample.save(Path(dir, \"sample.png\"))\\n            samples = ImageFolder(dir, RGB)\\n            self.assertTrue(np.allclose(sample.data, samples[0].data))\\n\\n    def test_audio_save(self):\\n        test_formats = [\"wav\", \"flac\"]\\n        test_samplerates = [22050, 44100, 48000]\\n        test_channels = [1, 2]\\n        test_dtypes = [np.float32, np.float64, np.int16, np.int32]'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='for format, samplerate, channels, dtype in product(\\n            test_formats, test_samplerates, test_channels, test_dtypes\\n        ):\\n            name = f\"sample.{format}\"\\n            with TemporaryDirectory() as dir:\\n                sample = generate_audio_sample(\\n                    samplerate, channels=channels, dtype=dtype\\n                )\\n                # test saving as any PathLike\\n                sample.save(os.path.join(dir, name))\\n                sample.save(Path(dir, name))\\n\\n    def test_audio_save_load(self):\\n        test_formats = [\"wav\", \"flac\"]\\n        test_samplerates = [22050, 44100, 48000]\\n        test_channels = [1, 2]\\n        test_dtypes = [np.float32, np.float64]\\n\\n        for format, samplerate, channels, dtype in product(\\n            test_formats, test_samplerates, test_channels, test_dtypes\\n        ):\\n            name = f\"sample.{format}\"\\n            with TemporaryDirectory() as dir:\\n                sample = generate_audio_sample(\\n                    samplerate, channels=channels, dtype=dtype\\n                )\\n                sample.save(Path(dir, name))\\n                samples = AudioFolder(dir)\\n                self.assertTrue(\\n                    np.allclose(sample.data, samples[0].data, atol=1e-4),\\n                    msg=f\"failed: format={format} samplerate={samplerate} channels={channels} dtypes={dtype}\",\\n                )\\n\\n    def test_get_timestamp(self):\\n        ts = get_timestamp()\\n        self.assertIsInstance(ts, int)\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/time.py'}, page_content=\"import time\\n\\n# BUG This *must* be addressed with the behavior written up in the plugin spec.\\n# We don't want any surprises in terms of accuraccy\\ntry:\\n    from time import time_ns as get_timestamp\\nexcept ImportError:\\n\\n    def get_timestamp():\\n        return int(time.time() * 1e9)\\n\\n\\n# NOTE to preserve the best accuracy, we implement the backwards compatible perf\\n# counter by only abstracting how to measure the duration between two times in\\n# nanoseconds\\ntry:\\n    from time import perf_counter_ns as timeit_perf_counter\\n\\n    def timeit_perf_counter_duration(start, finish):\\n        return finish - start\\n\\nexcept ImportError:\\n    from time import perf_counter as timeit_perf_counter\\n\\n    def timeit_perf_counter_duration(start, finish):\\n        return int((finish - start) * 1e9)\"),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/config.py'}, page_content='from typing import NamedTuple\\n\\n\\nclass PluginConfig(NamedTuple):\\n    \"\"\"\\n    PluginConfig represents the config required to setup and run a Plugin.\\n    \"\"\"\\n\\n    # TODO generalize to support different backends\\n    username: str\\n    password: str\\n    host: str\\n    port: int\\n    app_id: str'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/uploader.py'}, page_content='import hashlib\\nimport json\\nfrom pathlib import Path\\nfrom shutil import copyfile\\nfrom .time import get_timestamp\\n\\n\\nclass Uploader:\\n    def __init__(self, root):\\n        self.root = Path(root)\\n\\n    # NOTE uploads are stored in the following directory structure:\\n    # root/\\n    #   timestamp-sha1sum/\\n    #     data\\n    #     meta\\n    def upload_file(self, path, meta={}, timestamp=None, keep=False):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n\\n        path = Path(path)\\n        checksum = sha1sum_for_file(path)\\n\\n        # create upload dir\\n        upload_dir = Path(self.root, f\"{timestamp}-{checksum}\")\\n        upload_dir.mkdir(parents=True, exist_ok=True)\\n\\n        # stage data file\\n        # NOTE we do a copy instead of move, as the upload dir may\\n        # be mounted from another disk.\\n        copyfile(path, Path(upload_dir, \"data\"))\\n        if not keep:\\n            path.unlink()\\n\\n        # stage meta file\\n        metafile = {\\n            \"timestamp\": timestamp,\\n            \"shasum\": checksum,\\n            \"labels\": {k: v for k, v in meta.items()},\\n        }\\n        metafile[\"labels\"][\"filename\"] = path.name\\n        write_json_file(Path(upload_dir, \"meta\"), metafile)\\n\\n        return upload_dir\\n\\n\\ndef sha1sum_for_file(path):\\n    h = hashlib.sha1()\\n    with open(path, \"rb\") as f:\\n        while True:\\n            chunk = f.read(32768)\\n            if chunk == b\"\":\\n                break\\n            h.update(chunk)\\n    return h.hexdigest()\\n\\n\\ndef write_json_file(path, obj):\\n    with open(path, \"w\") as f:\\n        json.dump(obj, f, separators=(\",\", \":\"), sort_keys=True)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/__init__.py'}, page_content='from .config import PluginConfig\\nfrom .plugin import Plugin\\nfrom .uploader import Uploader\\nfrom .time import get_timestamp'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='import logging\\nfrom threading import Thread, Event\\nfrom queue import Queue, Empty\\nimport time\\nimport pika\\nimport pika.exceptions\\nimport wagglemsg\\nfrom .config import PluginConfig\\n\\n\\nlogger = logging.getLogger(__name__)\\n# pika is very verbose at DEBUG level. we turn it down here.\\nlogging.getLogger(\"pika\").setLevel(logging.CRITICAL)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='class RabbitMQPublisher:\\n    \"\"\"\\n    RabbitMQPublisher manages a connection to RabbitMQ and publishes messages from the provided queue.\\n\\n    This is done in a background thread which must be stopped by setting the provided stop Event.\\n    \"\"\"\\n\\n    def __init__(self, config: PluginConfig, messages: Queue, stop: Event):\\n        self.config = config\\n        self.params = get_connection_parameters_for_config(config)\\n        self.messages = messages\\n        self.stop = stop\\n        self.done = Event()\\n        Thread(target=self.__main).start()\\n\\n    def __main(self):\\n        logger.debug(\"publisher started.\")\\n        try:\\n            while not self.stop.is_set():\\n                try:\\n                    self.__connect_and_flush_messages()\\n                except Exception:\\n                    if logger.isEnabledFor(logging.DEBUG):\\n                        logger.exception(\"__connect_and_flush_messages exception\")\\n                    time.sleep(1)\\n        finally:\\n            self.done.set()\\n            logger.debug(\"publisher stopped.\")\\n\\n    def __connect_and_flush_messages(self):\\n        logger.debug(\"publisher connecting to rabbitmq...\")\\n        with pika.BlockingConnection(self.params) as conn, conn.channel() as ch:\\n            while not self.stop.is_set():\\n                self.__flush_messages(ch)\\n            logger.debug(\"publisher stopping...\")\\n            # attempt to flush any remaining messages\\n            self.__flush_messages(ch)\\n\\n    def __flush_messages(self, ch):\\n        while True:\\n            try:\\n                logger.debug(\"publisher checking for message...\")\\n                item = self.messages.get(timeout=1)\\n            except Empty:\\n                return\\n\\n            properties = pika.BasicProperties(\\n                delivery_mode=2, user_id=self.params.credentials.username\\n            )'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='# NOTE app_id is used by data service to validate and tag additional metadata provided by k3s scheduler.\\n            if self.config.app_id != \"\":\\n                properties.app_id = self.config.app_id\\n\\n            if logger.isEnabledFor(logging.DEBUG):\\n                logger.debug(\\n                    \"publishing message to rabbitmq: %s\", wagglemsg.load(item.body)\\n                )\\n\\n            try:\\n                ch.basic_publish(\\n                    exchange=\"to-validator\",\\n                    routing_key=item.scope,\\n                    properties=properties,\\n                    body=item.body,\\n                )\\n            except Exception:\\n                if logger.isEnabledFor(logging.DEBUG):\\n                    logger.exception(\\n                        \"basic_publish to rabbitmq failed. will requeue message...\"\\n                    )\\n                # requeue message so we can again later\\n                # NOTE(sean) this will reorder messages. if we realized we *must* preserve message\\n                # order, we must to change this to avoid subtle bugs!\\n                self.messages.put(item)\\n                # propagate error up to trigger reconnect\\n                raise'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='class RabbitMQConsumer:\\n    \"\"\"\\n    RabbitMQConsumer manages a connection to RabbitMQ and puts received messages into the provided queue.\\n\\n    This is done in a background thread which must be stopped by setting the provided stop Event.\\n    \"\"\"\\n\\n    def __init__(self, topics, config: PluginConfig, messages: Queue, stop: Event):\\n        self.topics = topics\\n        self.config = config\\n        self.params = get_connection_parameters_for_config(config)\\n        self.messages = messages\\n        self.stop = stop\\n        self.done = Event()\\n        Thread(target=self.__main).start()\\n\\n    def __main(self):\\n        logger.debug(\"consumer started.\")\\n        try:\\n            while not self.stop.is_set():\\n                try:\\n                    self.__connect_and_consume_messages()\\n                except Exception:\\n                    if logger.isEnabledFor(logging.DEBUG):\\n                        logger.exception(\"__connect_and_consume_messages exception\")\\n                    time.sleep(1)\\n        finally:\\n            self.done.set()\\n            logger.debug(\"consumer stopped.\")\\n\\n    def __connect_and_consume_messages(self):\\n        logger.debug(\"consumer connecting to rabbitmq...\")\\n        with pika.BlockingConnection(self.params) as conn, conn.channel() as ch:\\n            # setup subscriber queue and bind to topics\\n            queue = ch.queue_declare(\"\", exclusive=True).method.queue\\n            ch.basic_consume(queue, self.__process_message, auto_ack=True)\\n\\n            for topic in self.topics:\\n                ch.queue_bind(queue, \"data.topic\", topic)\\n                logger.debug(\"consumer binding queue %s to topic %s\", queue, topic)\\n\\n            def check_stop():\\n                if self.stop.is_set():\\n                    logger.debug(\"consumer stopping...\")\\n                    ch.stop_consuming()\\n                else:\\n                    conn.call_later(1, check_stop)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='conn.call_later(1, check_stop)\\n            logger.debug(\"consumer start processing messages...\")\\n            ch.start_consuming()\\n\\n    def __process_message(self, ch, method, properties, body):\\n        try:\\n            logger.debug(\"consumer processing message %s...\", body)\\n            msg = wagglemsg.load(body)\\n        except TypeError:\\n            logger.debug(\"unsupported message type: %s %s\", properties, body)\\n            return\\n        logger.debug(\"consumer putting message in waiting queue\")\\n        self.messages.put(msg)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='def get_connection_parameters_for_config(\\n    config: PluginConfig,\\n) -> pika.ConnectionParameters:\\n    return pika.ConnectionParameters(\\n        host=config.host,\\n        port=config.port,\\n        credentials=pika.PlainCredentials(\\n            username=config.username,\\n            password=config.password,\\n        ),\\n        connection_attempts=1,\\n        socket_timeout=1.0,\\n    )'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='import logging\\nimport re\\nimport wagglemsg\\n\\nfrom contextlib import contextmanager\\nfrom datetime import datetime\\nfrom os import getenv\\nfrom pathlib import Path\\nfrom queue import Queue, Empty\\nfrom threading import Event\\nfrom typing import NamedTuple\\n\\nfrom .config import PluginConfig\\nfrom .rabbitmq import RabbitMQPublisher, RabbitMQConsumer\\nfrom .time import get_timestamp, timeit_perf_counter, timeit_perf_counter_duration\\nfrom .uploader import Uploader\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass PublishData(NamedTuple):\\n    scope: str\\n    body: bytes\\n\\n\\n# Nanoseconds since epoch for 2000-01-01T00:00:00Z\\nMIN_TIMESTAMP_NS = 946706400000000000'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='class FilesystemPublisher:\\n    def __init__(self, root):\\n        self.root = Path(root)\\n        self.root.mkdir(parents=True, exist_ok=True)\\n        self.datafile = Path(root, \"data.ndjson\").open(\"a\")\\n        self.uploads_dir = Path(root, \"uploads\")\\n        self.uploads_dir.mkdir(parents=True, exist_ok=True)\\n\\n    def close(self):\\n        self.datafile.close()\\n\\n    def publish(self, msg: wagglemsg.Message):\\n        import json\\n\\n        out = {\\n            \"name\": msg.name,\\n            \"value\": msg.value,\\n            \"meta\": msg.meta,\\n            # python doesn\\'t have builtin support for nanosecond\\n            \"timestamp\": isoformat_time_ns(msg.timestamp),\\n        }\\n        print(\\n            json.dumps(out, sort_keys=True, separators=(\",\", \":\")),\\n            file=self.datafile,\\n            flush=True,\\n        )\\n\\n    def upload_file(self, path, timestamp, meta):\\n        from shutil import copyfile\\n\\n        src = Path(path)\\n        dst = Path(self.uploads_dir, f\"{timestamp}-{src.name}\")\\n        copyfile(src, dst)\\n        meta = meta.copy()\\n        meta[\"filename\"] = Path(src).name\\n        self.publish(\\n            wagglemsg.Message(\\n                name=\"upload\",\\n                value=str(dst.absolute()),\\n                meta=meta,\\n                timestamp=timestamp,\\n            )\\n        )\\n\\n\\ndef isoformat_time_ns(ns: int) -> str:\\n    # python doesn\\'t have builtin support for nanosecond timestamps and formatting, so we provide\\n    # a backfill for it. this is only intended to be used in the run log for testing.\\n    nanostr = f\"{ns%1000:03d}\"\\n    return datetime.fromtimestamp(ns / 1e9).isoformat() + nanostr'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='class Plugin:\\n    \"\"\"\\n    Plugin provides methods to publish and consume messages inside the Waggle ecosystem.\\n\\n    Examples\\n    --------\\n\\n    The simplest example is creating a Plugin and publishing a message. This can be done using:\\n\\n    ```python\\n    from waggle.plugin import Plugin\\n\\n    with Plugin() as plugin:\\n        plugin.publish(\"test_value\", 99)\\n    ```\\n    \"\"\"\\n\\n    def __init__(\\n        self, config=None, uploader=None, file_publisher: FilesystemPublisher = None\\n    ):\\n        self.config = config or get_default_plugin_config()\\n        self.uploader = uploader or get_default_plugin_uploader()\\n        self.send = Queue()\\n        self.recv = Queue()\\n        self.stop = Event()\\n        self.tasks = []\\n\\n        # TODO(sean) can we use ExitStack to clean up???\\n\\n        self.file_publisher = file_publisher\\n\\n        if self.file_publisher is None and getenv(\"PYWAGGLE_LOG_DIR\") is not None:\\n            self.file_publisher = FilesystemPublisher(getenv(\"PYWAGGLE_LOG_DIR\"))\\n\\n    def __enter__(self):\\n        self.tasks.append(RabbitMQPublisher(self.config, self.send, self.stop))\\n        return self\\n\\n    def __exit__(self, exc_type, exc_value, exc_traceback):\\n        self.stop.set()\\n\\n        if self.file_publisher is not None:\\n            self.file_publisher.close()\\n\\n        for task in self.tasks:\\n            task.done.wait()\\n\\n    def subscribe(self, *topics):\\n        self.tasks.append(RabbitMQConsumer(topics, self.config, self.recv, self.stop))\\n        # TODO(sean) add mock or integration testing against rabbitmq to actually test this\\n\\n    def get(self, timeout=None):\\n        try:\\n            return self.recv.get(timeout=timeout)\\n        except Empty:\\n            pass\\n        raise TimeoutError(\"plugin get timed out\")'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='def publish(self, name, value, meta={}, timestamp=None, scope=\"all\", timeout=None):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n        raise_for_invalid_publish_name(name)\\n        self.__publish(name, value, meta, timestamp, scope, timeout)\\n\\n    # NOTE __publish is used internally by publish and upload_file to do an unchecked\\n    # message publish. the main reason this exists is to guard against reserved names\\n    # like \"upload\" in publish but still allow upload_file to use it.\\n    def __publish(self, name, value, meta, timestamp, scope=\"all\", timeout=None):\\n        if not isinstance(value, (int, float, str)):\\n            raise TypeError(\"Value must be an int, float or str.\")\\n        if not isinstance(timestamp, int):\\n            raise TypeError(\\n                \"Timestamp must be an int and have units of nanoseconds since epoch. Please see the documentation for more information on setting timestamps.\"\\n            )\\n        if timestamp < MIN_TIMESTAMP_NS:\\n            raise ValueError(\\n                \"Timestamp probably has wrong units and is being processed as before 2000-01-01T00:00:00Z. Timestamp must have units of nanoseconds since epoch. Please see the documentation for more information on setting timestamps.\"\\n            )\\n        if not valid_meta(meta):\\n            raise TypeError(\"Meta must be a dictionary of strings to strings.\")\\n        msg = wagglemsg.Message(name=name, value=value, timestamp=timestamp, meta=meta)\\n\\n        # hack to use file publisher for everything except uploads\\n        if self.file_publisher is not None and name != \"upload\":\\n            self.file_publisher.publish(msg)\\n\\n        logger.debug(\"adding message to outgoing queue: %s\", msg)\\n        self.send.put(PublishData(scope, wagglemsg.dump(msg)), timeout=timeout)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='def upload_file(self, path, meta={}, timestamp=None, keep=False):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n\\n        if self.file_publisher is not None:\\n            self.file_publisher.upload_file(path, meta=meta, timestamp=timestamp)\\n\\n        if self.uploader is not None:\\n            meta = meta.copy()\\n            meta[\"filename\"] = Path(path).name\\n            upload_path = self.uploader.upload_file(\\n                path=path, meta=meta, timestamp=timestamp, keep=keep\\n            )\\n            self.__publish(\"upload\", upload_path.name, meta, timestamp)\\n\\n    @contextmanager\\n    def timeit(self, name):\\n        logger.debug(\"starting timeit block %s\", name)\\n        start = timeit_perf_counter()\\n        yield\\n        finish = timeit_perf_counter()\\n        duration = timeit_perf_counter_duration(start, finish)\\n        self.publish(name, duration)\\n        logger.debug(\"finished timeit block %s\", name)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='def get_default_plugin_config() -> PluginConfig:\\n    return PluginConfig(\\n        username=getenv(\"WAGGLE_PLUGIN_USERNAME\", \"plugin\"),\\n        password=getenv(\"WAGGLE_PLUGIN_PASSWORD\", \"plugin\"),\\n        host=getenv(\"WAGGLE_PLUGIN_HOST\", \"rabbitmq\"),\\n        port=int(getenv(\"WAGGLE_PLUGIN_PORT\", 5672)),\\n        app_id=getenv(\"WAGGLE_APP_ID\", \"\"),\\n    )\\n\\n\\ndef valid_meta(meta):\\n    return isinstance(meta, dict) and all(isinstance(v, str) for v in meta.values())\\n\\n\\ndef get_default_plugin_uploader():\\n    if (\\n        getenv(\"WAGGLE_PLUGIN_UPLOAD_PATH\") is None\\n        and getenv(\"PYWAGGLE_LOG_DIR\") is not None\\n    ):\\n        return None\\n    return Uploader(Path(getenv(\"WAGGLE_PLUGIN_UPLOAD_PATH\", \"/run/waggle/uploads\")))\\n\\n\\npublish_name_part_pattern = re.compile(\"^[a-z0-9_]+$\")\\n\\n\\ndef raise_for_invalid_publish_name(s: str):\\n    if not isinstance(s, str):\\n        raise TypeError(f\"publish name must be a string: {s!r}\")\\n    if len(s) > 128:\\n        raise ValueError(f\"publish must be at most 128 characters: {s!r}\")\\n    if s == \"upload\":\\n        raise ValueError(f\"name {s!r} is reserved for system use only\")\\n    parts = s.split(\".\")\\n    for p in parts:\\n        if not publish_name_part_pattern.match(p):\\n            raise ValueError(\\n                f\"publish name invalid: {s!r} part: {p!r} (names must consist of [a-z0-9_] and may be joined by .)\"\\n            )'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='import cv2\\nfrom pathlib import Path\\nimport numpy\\nfrom typing import Union\\nimport os\\nfrom os import PathLike\\nimport random\\nimport json\\nimport re\\nimport threading\\nimport time\\nfrom base64 import b64encode\\nfrom .timestamp import get_timestamp\\nfrom shutil import which\\nimport ffmpeg\\nimport logging\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass BGR:\\n    @classmethod\\n    def cv2_to_format(cls, data):\\n        return data\\n\\n    @classmethod\\n    def format_to_cv2(cls, data):\\n        return data\\n\\n\\nclass RGB:\\n    @classmethod\\n    def cv2_to_format(cls, data):\\n        return cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\\n\\n    @classmethod\\n    def format_to_cv2(cls, data):\\n        return cv2.cvtColor(data, cv2.COLOR_RGB2BGR)\\n\\n\\nWAGGLE_DATA_CONFIG_PATH = Path(\\n    os.environ.get(\"WAGGLE_DATA_CONFIG_PATH\", \"/run/waggle/data-config.json\")\\n)\\n\\n\\ndef read_device_config(path):\\n    config = json.loads(Path(path).read_text())\\n    return {\\n        section[\"match\"][\"id\"]: section\\n        for section in config\\n        if \"id\" in section[\"match\"]\\n    }\\n\\n\\n# TODO use format spec like rgb vs bgr in config file\\nclass ImageSample:\\n    data: numpy.ndarray\\n    timestamp: int\\n    format: Union[BGR, RGB]\\n\\n    def __init__(self, data, timestamp, format):\\n        self.format = format\\n        self.data = self.format.cv2_to_format(data)\\n        self.timestamp = timestamp\\n\\n    def save(self, path: PathLike):\\n        path = Path(path)\\n        data = self.format.format_to_cv2(self.data)\\n        cv2.imwrite(str(path), data)\\n\\n    def _repr_html_(self):\\n        data = self.format.format_to_cv2(self.data)\\n        ok, buf = cv2.imencode(\".png\", data)\\n        if not ok:\\n            raise RuntimeError(\"could not encode image\")\\n        b64data = b64encode(buf.ravel()).decode()\\n        return f\\'<img src=\"data:image/png;base64,{b64data}\" />\\''),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='class VideoSample:\\n    path: str\\n    timestamp: int\\n\\n    def __init__(self, path, timestamp, format=RGB):\\n        self.format = format\\n        self.path = path\\n        self.timestamp = timestamp\\n        self.capture = None\\n\\n    def __enter__(self):\\n        self.capture = cv2.VideoCapture(self.path)\\n        if not self.capture.isOpened():\\n            raise RuntimeError(\\n                f\"unable to open video capture for file {self.path!r}\"\\n            )\\n        self.fps = self.capture.get(cv2.CAP_PROP_FPS)\\n        if self.fps > 100.:\\n            self.fps = 0.\\n            logger.debug(f\\'pywaggle cannot calculate timestamp because the fps ({self.fps}) is too high.\\')\\n            self.timestamp_delta = 0\\n        else:\\n            self.timestamp_delta = 1 / self.fps\\n        self._frame_count = 0\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        if self.capture.isOpened():\\n            self.capture.release()\\n\\n    def __iter__(self):\\n        self._frame_count = 0\\n        return self\\n\\n    def __next__(self):\\n        if self.capture == None or not self.capture.isOpened():\\n            raise RuntimeError(\"video is not opened. use the Python WITH statement to open the video\")\\n        ok, data = self.capture.read()\\n        if not ok or data is None:\\n            raise StopIteration\\n        # timestamp must be an integer in nanoseconds\\n        approx_timestamp = self.timestamp + int(self.timestamp_delta * self._frame_count * 1e9)\\n        self._frame_count += 1\\n        return ImageSample(data=data, timestamp=approx_timestamp, format=self.format)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def resolve_device(device):\\n    if isinstance(device, Path):\\n        return resolve_device_from_path(device)\\n    # objects that are not paths or strings are considered already resolved\\n    if not isinstance(device, str):\\n        return device\\n    match = re.match(r\"([A-Za-z0-9]+)://(.*)$\", device)\\n    # non-url like paths refer to data shim devices\\n    if match is None:\\n        return resolve_device_from_data_config(device)\\n    # return file:// urls as path\\n    if match.group(1) == \"file\":\\n        return resolve_device_from_path(Path(match.group(2)))\\n    # return other urls as-is\\n    return device\\n\\n\\ndef resolve_device_from_path(path):\\n    return str(path.absolute())\\n\\n\\ndef resolve_device_from_data_config(device):\\n    config = read_device_config(WAGGLE_DATA_CONFIG_PATH)\\n    section = config.get(device)\\n    if section is None:\\n        raise KeyError(f\"no device found {device!r}\")\\n    try:\\n        return section[\"handler\"][\"args\"][\"url\"]\\n    except KeyError:\\n        raise KeyError(f\"missing .handler.args.url field for device {device!r}.\")'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='class Camera:\\n    INPUT_TYPE_FILE = \"file\"\\n    INPUT_TYPE_OTHER = \"other\"\\n\\n    def __init__(self, device=0, format=RGB):\\n        self.capture = _Capture(resolve_device(device), format)\\n        match = re.match(r\"([A-Za-z0-9]+)://(.*)$\", device)\\n        if match is not None and match.group(1) == \"file\":\\n            self.input_type = self.INPUT_TYPE_FILE\\n        else:\\n            self.input_type = self.INPUT_TYPE_OTHER\\n\\n    def __enter__(self):\\n        if self.input_type == self.INPUT_TYPE_FILE:\\n            logger.info(f\\'input is a file. the background thread disabled for grabbing frames\\')\\n            self.capture.enable_daemon = False\\n        else:\\n            self.capture.enable_daemon = True\\n        self.capture.__enter__()\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.capture.__exit__(exc_type, exc_val, exc_tb)\\n\\n    def snapshot(self):\\n        with self.capture:\\n            return self.capture.snapshot()\\n\\n    def stream(self):\\n        with self.capture:\\n            yield from self.capture.stream()\\n\\n    def record(self, duration, file_path=\"./sample.mp4\", skip_second=1):\\n        return self.capture.record(duration, file_path, skip_second)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='class _Capture:\\n    def __init__(self, device, format):\\n        self.device = device\\n        self.format = format\\n        self.context_depth = 0\\n        self.enable_daemon = False\\n        self.daemon_need_to_stop = threading.Event()\\n        self._ready_for_next_frame = threading.Event()\\n        self.daemon = threading.Thread(target=self._run, daemon=True)\\n        self.lock = threading.Lock()\\n\\n    def __enter__(self):\\n        if self.context_depth == 0:\\n            self.capture = cv2.VideoCapture(self.device)\\n            if not self.capture.isOpened():\\n                raise RuntimeError(\\n                    f\"unable to open video capture for device {self.device!r}\"\\n                )\\n            # spin up a thread to keep up with the camera frame rate\\n            if self.enable_daemon:\\n                self.daemon_need_to_stop.clear()\\n                self.daemon.start()\\n        self.context_depth += 1\\n        return self'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.context_depth -= 1\\n        if self.context_depth == 0:\\n            if self.enable_daemon:\\n                self.daemon_need_to_stop.set()\\n            self.capture.release()\\n    \\n    def _run(self):\\n        # we sleep slighly shorter than FPS to drain the buffer efficiently\\n        # NOTE: OpenCV\\'s FPS get function is inaccurate as a USB webcam gives 1 FPS while\\n        #       a RTSP stream returns 180000. none of them are correct. therefore, we cannot\\n        #       decide the sleep time based on obtained FPS\\n        # fps = self.capture.get(cv2.CAP_PROP_FPS)\\n        sleep = 0.01\\n        # if fps > 0 and fps < 100:\\n        #    sleep = 1 / (fps + 1)\\n        # logging.debug(f\\'camera FPS is {fps}. the background thread sleeps {sleep} seconds in between grab()\\')\\n        while not self.daemon_need_to_stop.is_set():\\n            try:\\n                self.lock.acquire()\\n                ok = self.capture.grab()\\n                if not ok:\\n                    raise RuntimeError(\"failed to grab a frame\")\\n                self.timestamp = get_timestamp()\\n            finally:\\n                self.lock.release()\\n            self._ready_for_next_frame.set()\\n            time.sleep(sleep)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def grab_frame(self):\\n        if self.daemon.is_alive():\\n            if not self._ready_for_next_frame.wait(timeout=10.):\\n                raise RuntimeError(\"failed to grab a frame from the background thread: timed out\")\\n            self._ready_for_next_frame.clear()\\n            try:\\n                self.lock.acquire(timeout=1)\\n                timestamp = self.timestamp\\n                ok, data = self.capture.retrieve()\\n                if not ok:\\n                    raise RuntimeError(\"failed to retrieve the taken snapshot\")\\n            finally:\\n                self.lock.release()\\n            return ImageSample(data=data, timestamp=timestamp, format=self.format)\\n        else:\\n            ok = self.capture.grab()\\n            if not ok:\\n                raise RuntimeError(\"failed to take a snapshot\")\\n            timestamp = get_timestamp()\\n            ok, data = self.capture.retrieve()\\n            if not ok:\\n                raise RuntimeError(\"failed to retrieve the taken snapshot\")\\n            return ImageSample(data=data, timestamp=timestamp, format=self.format)\\n\\n    def snapshot(self):\\n        return self.grab_frame()\\n\\n    def stream(self):\\n        try:\\n            while True:\\n                yield self.grab_frame()\\n        except:\\n            pass'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def record(self, duration, file_path=\"./sample.mp4\", skip_second=1):\\n        if which(\"ffmpeg\") == None:\\n            raise RuntimeError(\"ffmpeg does not exist to record video. please install ffmpeg\")\\n        if self.context_depth > 0:\\n            raise RuntimeError(f\\'the stream {self.device} is already open. please close first or use without the Python\\\\\\'s WITH statement\\')\\n        if isinstance(self.device, str) and self.device.startswith(\"rtsp\"):\\n            c = ffmpeg.input(self.device, rtsp_transport=\"tcp\", ss=skip_second)\\n        else:\\n            c = ffmpeg.input(self.device, ss=skip_second)\\n        c = ffmpeg.output(c, file_path, codec=\"copy\", f=\\'mp4\\', t=duration).overwrite_output()\\n        timestamp = get_timestamp()\\n        _, stderr = ffmpeg.run(c, quiet=True)\\n        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\\n            return VideoSample(path=file_path, timestamp=timestamp)\\n        else:\\n            raise RuntimeError(f\\'error while recording: {stderr}\\')'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='class ImageFolder:\\n    available_formats = {\".jpg\", \".jpeg\", \".png\"}\\n\\n    def __init__(self, root, format=RGB, shuffle=False):\\n        self.files = sorted(\\n            p.absolute()\\n            for p in Path(root).glob(\"*\")\\n            if p.suffix in self.available_formats\\n        )\\n        self.format = format\\n        if shuffle:\\n            random.shuffle(self.files)\\n\\n    def __len__(self):\\n        return len(self.files)\\n\\n    def __getitem__(self, i):\\n        data = cv2.imread(str(self.files[i]))\\n        timestamp = Path(self.files[i]).stat().st_mtime_ns\\n        return ImageSample(data=data, timestamp=timestamp, format=self.format)\\n\\n    def __repr__(self):\\n        return f\"ImageFolder{self.files!r}\"'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/__init__.py'}, page_content='# Maintaining backwards compatibility for now.\\nfrom .data_shim import open_data_source'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/audio.py'}, page_content='from os import PathLike\\nfrom pathlib import Path\\nimport numpy\\nimport soundfile\\nfrom typing import NamedTuple\\nimport random\\nfrom base64 import b64encode\\nfrom io import BytesIO\\nfrom .timestamp import get_timestamp\\n\\n\\nclass AudioSample(NamedTuple):\\n    data: numpy.ndarray\\n    samplerate: int\\n    timestamp: int\\n\\n    def save(self, path: PathLike):\\n        path = Path(path)\\n        soundfile.write(str(path), self.data, self.samplerate)\\n\\n    def _repr_html_(self):\\n        with BytesIO() as buf:\\n            soundfile.write(\\n                buf, self.data, self.samplerate, format=\"flac\", closefd=False\\n            )\\n            b64data = b64encode(buf.getvalue()).decode()\\n        return f\"\"\"\\n<audio controls=\"controls\" autobuffer=\"autobuffer\">\\n<source src=\"data:audio/wav;base64,{b64data}\" />\\n</audio>\\n\"\"\"\\n\\n\\nclass Microphone:\\n    def __init__(self, samplerate=48000, channels=1, name=None):\\n        import soundcard\\n\\n        self.microphone = soundcard.default_microphone()\\n        self.samplerate = samplerate\\n        self.channels = channels\\n        self.name = name\\n\\n    def record(self, duration):\\n        timestamp = get_timestamp()\\n        data = self.microphone.record(\\n            samplerate=self.samplerate,\\n            numframes=int(duration * self.samplerate),\\n            channels=self.channels,\\n        )\\n        return AudioSample(data, self.samplerate, timestamp=timestamp)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/audio.py'}, page_content='class AudioFolder:\\n    available_formats = {\".\" + s.lower() for s in soundfile.available_formats().keys()}\\n\\n    def __init__(self, root, shuffle=False):\\n        self.files = sorted(\\n            p.absolute()\\n            for p in Path(root).glob(\"*\")\\n            if p.suffix in self.available_formats\\n        )\\n        if shuffle:\\n            random.shuffle(self.files)\\n\\n    def __len__(self):\\n        return len(self.files)\\n\\n    def __getitem__(self, i):\\n        data, samplerate = soundfile.read(str(self.files[i]), always_2d=True)\\n        timestamp = Path(self.files[i]).stat().st_mtime_ns\\n        return AudioSample(data, samplerate, timestamp=timestamp)\\n\\n    def __repr__(self):\\n        return f\"AudioFolder{self.files!r}\"'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/timestamp.py'}, page_content='try:\\n    from time import time_ns as get_timestamp\\nexcept ImportError:\\n    from time import time\\n\\n    def get_timestamp():\\n        return int(time() * 1e9)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='import logging\\nimport numpy as np\\nfrom urllib.request import urlopen\\nfrom threading import Thread, Event\\nfrom queue import Queue, Empty, Full\\nimport time\\nimport os\\nimport socket\\nfrom pathlib import Path\\nimport json\\nimport random\\nimport re\\n\\nlogger = logging.getLogger(__name__)\\n\\ntry:\\n    import cv2\\nexcept ImportError:\\n    logger.warning(\\n        \"cv2 module not found. pywaggle requires this to capture image and video data.\"\\n    )\\n\\n\\n# BUG This *must* be addressed with the behavior written up in the plugin spec.\\n# We don\\'t want any surprises in terms of accuraccy\\ntry:\\n    from time import time_ns\\nexcept ImportError:\\n    logger.warning(\"using backwards compatible implementation of time_ns\")\\n\\n    def time_ns():\\n        return int(time.time() * 1e9)\\n\\n\\ndef cvtColor(bgr_img, pixel_format=\"rgb\"):\\n    if pixel_format == \"rgb\":\\n        return cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\\n    return bgr_img'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='class ImageHandler:\\n    def __init__(self, query, url, pixel_format=\"rgb\"):\\n        self.url = url\\n        self.pixel_format = pixel_format\\n\\n    def get(self, timeout=None):\\n        try:\\n            with urlopen(self.url, timeout=timeout) as f:\\n                data = f.read()\\n                ts = time_ns()\\n                arr = np.frombuffer(data, np.uint8)\\n                bgr_img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\\n                return ts, cvtColor(bgr_img, self.pixel_format)\\n        except socket.timeout:\\n            raise TimeoutError(\"get timed out\")\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, *exc):\\n        pass\\n\\n\\ndef video_worker(handler):\\n    try:\\n        while not handler.quit.is_set():\\n            ok, bgr_img = handler.cap.read()\\n            if not ok:\\n                break\\n            img = cvtColor(bgr_img, handler.pixel_format)\\n            item = (time_ns(), img)\\n\\n            # attempt to add an item to the queue\\n            try:\\n                handler.queue.put_nowait(item)\\n                continue\\n            except Full:\\n                logger.debug(\"video frame queue full. evicting oldest frame...\")\\n            # evict an item from queue\\n            try:\\n                handler.queue.get_nowait()\\n            except Empty:\\n                pass\\n            # queue should have space to add now. (assuming this\\n            # is the only producer adding to this queue)\\n            handler.queue.put_nowait(item)\\n    finally:\\n        handler.cap.release()\\n        handler.released.set()\\n\\n\\n# TODO We need to use a flexible model where the data returned is\\n# extensible. For example, serial data won\\'t really have a good\\n# notion of \"timestamp\". Maybe it\\'s better to not include that.'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='class VideoHandler:\\n    def __init__(self, query, url, pixel_format=\"rgb\"):\\n        self.pixel_format = pixel_format\\n        self.cap = cv2.VideoCapture(url)\\n        if not self.cap.isOpened():\\n            raise RuntimeError(f\\'could not open camera at \"{url}\".\\')\\n        self.queue = Queue(8)\\n        self.quit = Event()\\n        self.released = Event()\\n        # NOTE(sean) no further mutation can be done on VideoHandler state. all\\n        # interaction with cap *must* be done in the worker thread or via queue\\n        # and quit primitives\\n        worker = Thread(target=video_worker, args=(self,), daemon=True)\\n        worker.start()\\n\\n    def get(self, timeout=None):\\n        try:\\n            return self.queue.get(timeout=timeout)\\n        except Empty:\\n            raise TimeoutError(\"get timed out\")\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, *exc):\\n        self.quit.set()\\n        self.released.wait()  # <- wait for cleanup in worker thread\\n\\n\\nWAGGLE_DATA_CONFIG_PATH = Path(\\n    os.environ.get(\"WAGGLE_DATA_CONFIG_PATH\", \"/run/waggle/data-config.json\")\\n)\\n\\ntry:\\n    config = json.loads(WAGGLE_DATA_CONFIG_PATH.read_text())\\nexcept FileNotFoundError:\\n    logger.debug(\\n        \"could not find data config file %s. using empty resource list.\",\\n        WAGGLE_DATA_CONFIG_PATH,\\n    )\\n    config = []\\n\\n\\ndef dict_is_subset(a, b):\\n    return all(k in b and re.match(b[k], a[k]) for k in a.keys())\\n\\n\\ndef find_all_matches(query):\\n    return [c for c in config if dict_is_subset(query, c[\"match\"])]\\n\\n\\ndef find_match(query):\\n    matches = find_all_matches(query)\\n    if len(matches) == 0:\\n        raise RuntimeError(\"no matches found\")\\n    if len(matches) > 1:\\n        raise RuntimeError(\"multiple devices found\")\\n    return matches[0]\\n\\n\\nhandlers = {\\n    \"image\": ImageHandler,\\n    \"video\": VideoHandler,\\n}\\n\\n\\n# optimizations *could* happen here, on demand...'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='def open_data_source(**query):\\n    match = find_match(query)\\n    handler = handlers[match[\"handler\"][\"type\"]]\\n    args = match[\"handler\"][\"args\"]\\n    return handler(query, **args)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/measurements.py'}, page_content='from datetime import datetime\\nimport json\\nimport time\\n\\n\\nclass MeasurementsFile:\\n    def __init__(self, filename):\\n        self.records = []\\n\\n        with open(filename, \"r\") as f:\\n            for r in map(json.loads, f):\\n                # 2021-06-25T18:52:15.404690128Z\\n                r[\"timestamp\"] = datetime.strptime(\\n                    r[\"timestamp\"][:26], \"%Y-%m-%dT%H:%M:%S.%f\"\\n                )\\n                self.records.append(r)\\n        self.records.sort(key=lambda r: r[\"timestamp\"])\\n\\n    def play(self, nodelay=False):\\n        if len(self.records) == 0:\\n            return\\n        last_record = self.records[0]\\n        for r in self.records:\\n            delta = r[\"timestamp\"] - last_record[\"timestamp\"]\\n            if not nodelay:\\n                time.sleep(delta.total_seconds())\\n            yield r\\n            last_record = r\\n\\n\\n# MessagePlayer can take a SDR format file and replay the contents\\n# this will help support use cases where someone wants to inject known\\n# data into their plugin from a file.\\n# (think about name?)\\n# other features:\\n# should sort by timestamp / or leave no sort as flag?\\n# should be able to decide starting time')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Split Python files\n",
    "py_splitter = PythonCodeTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "py_splits = py_splitter.split_documents(py_docs)\n",
    "py_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 448 μs, sys: 16 μs, total: 464 μs\n",
      "Wall time: 527 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sage-data-client/examples/plotting_example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Basic Plotting Example\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import sage_data_client\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"First, we\\'ll query the last 7 days of temperature data from W022\\'s BME680 sensor.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df = sage_data_client.query(\\\\n\\', \\'    start=\"-7d\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\",\\\\n\\', \\'        \"vsn\": \"W022\",\\\\n\\', \\'        \"sensor\": \"bme680\",\\\\n\\', \\'    }\\\\n\\', \\')\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"Next, we\\'ll plot a simple line chart.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df.set_index(\"timestamp\").value.plot()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"Finally, we\\'ll plot the temperature distribution.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.hist(bins=100)\\']\\'\\n\\n\\'code\\' cell: \\'[]\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Geospatial Mapping Example\\\\n\\', \\'Within this example, we walk through how to query for SAGE data, filter our values, and plot maps of the data using Cartopy, Matplotlib, and hvPlot!\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'## Imports\\\\n\\', \\'We import our sage_data_client, along with the plotting libraries matplotlib, Cartopy, hvPlot and holoviews.\\\\n\\', \\'\\\\n\\', \\'If you have not installed these packages already, make sure to run this line!\\']\\'\\n\\n\\'code\\' cell: \\'[\\'!pip3 install matplotlib bokeh holoviews hvplot cartopy pandas metpy\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import sage_data_client\\\\n\\', \\'from bokeh.models.formatters import DatetimeTickFormatter\\\\n\\', \\'import hvplot.pandas\\\\n\\', \\'import holoviews as hv\\\\n\\', \\'import cartopy.crs as ccrs\\\\n\\', \\'import cartopy.feature as cfeature\\\\n\\', \\'import matplotlib.pyplot as plt\\\\n\\', \\'from metpy.plots import USCOUNTIES\\\\n\\', \\'import pandas as pd\\\\n\\', \\'import warnings\\\\n\\', \\'\\\\n\\', \\'warnings.filterwarnings(\"ignore\")\\\\n\\', \\'hv.extension(\"bokeh\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Query and load data into pandas data frame\\\\n\\', \\'We have two queries we are interested in:\\\\n\\', \\'- Temperature\\\\n\\', \\'- Location data (latitude and longitude of the sensor)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'temperature_df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\",\\\\n\\', \\'    }\\\\n\\', \\')\\\\n\\', \\'\\\\n\\', \\'location_df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"sys.gps.*\",\\\\n\\', \\'    }\\\\n\\', \\')\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Investigate the Temperature Dataframe\\\\n\\', \\'Notice how the dataframe containing temperature data stores the temperature value as `value`, along with several `meta.` fields.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'temperature_df\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'### Investigate the Location Dataframe\\\\n\\', \\'This dataframe does not have as many metadata fields... but we do have enough to join our two dataframes. Another issue with this dataframe is that the location values are stored as individual rows, when we would ideally like these to be their own columns (ex. latitude and longitude for a given location)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'location_df\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Clean Up the Data\\\\n\\', \\'\\\\n\\', \"Let\\'s start the data cleaning process!\\\\n\", \\'\\\\n\\', \\'### Join Latitude and Longitude into the Same Rows\\\\n\\', \\'Our first step is to join our latitude and longitude values into the same row. We start by\\\\n\\', \\'- Subsetting for latitude and longitude in the dataframe\\\\n\\', \\'- Rename the fields accordingly\\\\n\\', \\'- Join the columns based on the timestamp, host, and node\\\\n\\', \\'- Drop any extra columns\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Subset the latitude values, and rename to latitude\\\\n\\', \"lat = location_df.loc[location_df.name == \\'sys.gps.lat\\']\\\\n\", \"lat_df = lat.rename(columns={\\'value\\':\\'latitude\\'})\\\\n\", \\'\\\\n\\', \\'# Subset the longitude values, and rename to latitude\\\\n\\', \"lon = location_df.loc[location_df.name == \\'sys.gps.lon\\']\\\\n\", \"lon_df = lon.rename(columns={\\'value\\':\\'longitude\\'})\\\\n\", \\'\\\\n\\', \\'# Join the latitude and longitude dataframes, returning a dataframe with shared latitude and longitude information\\\\n\\', \"joined_lats_lons = pd.merge(lat_df, lon_df, on=[\\'timestamp\\', \\'meta.host\\', \\'meta.node\\'],  how=\\'outer\\', )\\\\n\", \\'\\\\n\\', \\'# Filter out unwanted columns\\\\n\\', \"joined_lats_lons = joined_lats_lons[[x for x in joined_lats_lons.columns if ((\\'x\\' not in x) and (\\'y\\' not in x) and (\\'timestamp\\' not in x))]]\\\\n\", \\'\\\\n\\', \\'# Return our dataframe\\\\n\\', \\'joined_lats_lons\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Join our Latitude and Longitude Information with the Temperature Dataframe\\\\n\\', \\'Now that we have our location dataframe cleaned up, we can join this with the temperature dataframe, so we know where our temperature values are collected!\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'code\\' cell: \\'[\\'# Merge the dataframes using the host and node as the shared fields to join on\\\\n\\', \"df = pd.merge(temperature_df, joined_lats_lons,  on=[\\'meta.host\\', \\'meta.node\\'], how=\\'right\\')\\\\n\", \\'\\\\n\\', \\'# Drop any duplicates, based on the timestamp, host, and node\\\\n\\', \"df_filtered = df.drop_duplicates([\\'timestamp\\', \\'meta.host\\', \\'meta.node\\'])\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Run Statistics on our Dataframe\\\\n\\', \"Let\\'s say we are interested in hourly mean temperature... we can calculate that!\"]\\'\\n\\n\\'code\\' cell: \\'[\\'hours = df_filtered.timestamp.dt.hour.unique()\\']\\'\\n\\n\\'code\\' cell: \\'[\\'hourly_mean = df_filtered.groupby([df_filtered.timestamp.dt.hour,\\\\n\\', \"                                   \\'meta.node\\']).mean()\"]\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \\'    fig = plt.figure(figsize=(14,8))\\\\n\\', \\'    ax = plt.subplot(projection=ccrs.PlateCarree())\\\\n\\', \"    s = hourly_mean.loc[hour].plot.scatter(ax=ax, x=\\'longitude\\', y=\\'latitude\\', c=\\'value\\', cmap=\\'Spectral_r\\')\\\\n\", \\'    ax.add_feature(cfeature.LAND)\\\\n\\', \\'    ax.add_feature(cfeature.STATES)\\\\n\\', \\'    ax.add_feature(cfeature.LAKES)\\\\n\\', \\'    ax.set_extent([-125, -66.5, 20, 50])\\\\n\\', \\'    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\\\\n\\', \"                  linewidth=2, color=\\'gray\\', alpha=0.5, linestyle=\\'--\\')\\\\n\", \\'    gl.top_labels = False\\\\n\\', \\'    gl.right_labels = False\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    plt.title(f\\'Average Temperature (degF) at \\\\\\\\n {time_label}\\', fontsize=16);\\\\n\", \\'    plt.show()\\\\n\\', \\'    plt.close()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Zoom in on the Chicago Area\\\\n\\', \"It\\'s nice having a national map, but let\\'s zoom into Chicago for a higher resolution view of the sensors around the city and surrounding suburbs.\"]\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \\'    fig = plt.figure(figsize=(14,8))\\\\n\\', \\'    ax = plt.subplot(projection=ccrs.PlateCarree())\\\\n\\', \"    s = hourly_mean.loc[hour].plot.scatter(ax=ax, x=\\'longitude\\', y=\\'latitude\\', c=\\'value\\', cmap=\\'Spectral_r\\', vmin=28, vmax=38)\\\\n\", \\'    ax.add_feature(cfeature.LAND)\\\\n\\', \\'    ax.add_feature(cfeature.STATES)\\\\n\\', \\'    ax.add_feature(cfeature.LAKES)\\\\n\\', \\'    ax.add_feature(USCOUNTIES)\\\\n\\', \\'    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\\\\n\\', \"                  linewidth=2, color=\\'gray\\', alpha=0.5, linestyle=\\':\\')\\\\n\", \\'    gl.top_labels = False\\\\n\\', \\'    gl.right_labels = False\\\\n\\', \\'    ax.set_extent([-89, -87, 41, 43])\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    plt.title(f\\'Average Temperature (degF) at \\\\\\\\n {time_label}\\', fontsize=16);\\\\n\", \\'    plt.show()\\\\n\\', \\'    plt.close()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Plot Interactive National Maps\\\\n\\', \\'We can use hvPlot here to plot interactive national maps\\']\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    display(hourly_mean.loc[hour].hvplot.points(x=\\'longitude\\',\\\\n\", \"                                                y=\\'latitude\\',\\\\n\", \"                                                color=\\'value\\',\\\\n\", \"                                                cmap=\\'Spectral_r\\',\\\\n\", \\'                                                geo=True,\\\\n\\', \"                                                tiles=\\'CartoLight\\',\\\\n\", \"                                                title=f\\'Average Temperature (degF) at {time_label}\\',\\\\n\", \"                                                clabel=\\'Temperature (degF)\\',\\\\n\", \\'                                                crs=ccrs.PlateCarree()))\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'### Plot an Interactive Regional Map of Chicago\\\\n\\', \\'And the same for the region around Chicago\\\\n\\', \\'\\\\n\\', \\'We start first by subsetting points out of our dataframe around the Northern Illinois area.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'illinois_points = hourly_mean.loc[(hourly_mean.latitude > 41.) & (hourly_mean.longitude > -89)]\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'Now that we have this, we loop through and plot our data!\\']\\'\\n\\n\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    display(illinois_points.loc[hour].hvplot.points(x=\\'longitude\\',\\\\n\", \"                                                    y=\\'latitude\\',\\\\n\", \"                                                    color=\\'value\\',\\\\n\", \"                                                    cmap=\\'Spectral_r\\',\\\\n\", \"                                                    tiles=\\'CartoLight\\',\\\\n\", \\'                                                    geo=True,\\\\n\\', \"                                                    title=f\\'Average Temperature (degF) at {time_label}\\',\\\\n\", \"                                                    clabel=\\'Temperature (degF)\\'))\"]\\'\\n\\n\\'code\\' cell: \\'[]\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/sage-interactive-plot.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Interactive Plotting Example\\\\n\\', \\'Within this example, we walk through how to query for SAGE data, filter our values, and plot using the hvPlot interactive plotting library!\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Imports and Query\\\\n\\', \\'We import our `sage_data_client`, along with the plotting libraries `hvPlot` and `holoviews`\\\\n\\', \\'\\\\n\\', \\'**If you have not installed these packages already, make sure to run this line!**\\']\\'\\n\\n\\'code\\' cell: \\'[\\'!pip3 install matplotlib bokeh holoviews hvplot\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import sage_data_client\\\\n\\', \\'from bokeh.models.formatters import DatetimeTickFormatter\\\\n\\', \\'import hvplot.pandas\\\\n\\', \\'import holoviews as hv\\\\n\\', \\'import warnings\\\\n\\', \\'\\\\n\\', \\'# query and load data into pandas data frame\\\\n\\', \\'df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\"\\\\n\\', \\'    }\\\\n\\', \\')\\\\n\\', \\'\\\\n\\', \\'warnings.filterwarnings(\"ignore\")\\\\n\\', \\'hv.extension(\"bokeh\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Clean our Data\\\\n\\', \\'When we first visualize our dataset (temperature), notice how we have some **very** low values (< -100 degrees Fahrenheit).\\']\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.plot();\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'We can flag these values as bad data using the following:\\']\\'\\n\\n\\'code\\' cell: \\'[\\'df = df[df.value > -100]\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'Now, if we visualize our data again, notice how we do not have these abnormally low values.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.plot();\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Create an Interactive Plot\\\\n\\', \\'We can use hvPlot to plot our data! Instead of using `.plot()` like we did before, which creates a matplotlib static plot, we can use `.hvplot()` which creates an interactive plot. \\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/sage-interactive-plot.ipynb'}, page_content='\\'code\\' cell: \\'[\\'# Set\\\\n\\', \\'formatter = DatetimeTickFormatter(days=\"%d %b %Y \\\\\\\\n %H:%M UTC\",\\\\n\\', \\'                                  hours=\"%d %b %Y \\\\\\\\n %H:%M UTC\",\\\\n\\', \\'                                  minutes=\"%d %b %Y \\\\\\\\n %H:%M UTC\",)\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \"df.hvplot.line(x=\\'timestamp\\',\\\\n\", \"               y=\\'value\\',\\\\n\", \"               ylabel=\\'Temperature (degF)\\',\\\\n\", \"               xlabel=\\'Time\\',\\\\n\", \"               by=\\'meta.vsn\\',\\\\n\", \\'               groupby=[\"meta.sensor\"],\\\\n\\', \\'               xformatter=formatter, \\\\n\\', \"               color=hv.Palette(\\'Category20\\'),\\\\n\", \\'               height=400,\\\\n\\', \\'               width=600)\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Split Notebook files\n",
    "ipynb_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "ipynb_splits = ipynb_splitter.split_documents(ipynb_docs)\n",
    "ipynb_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'sage-website/docs/contact-us.md'}, page_content='sidebar_label: Contact us\\n\\nContact us\\n\\nEmail\\n\\nFor support, general questions, or comments, you can always reach us at:\\n\\nsupport@waggle-edge.ai\\n\\nMessage Board\\n\\nWe also encourage developers and users to start a new topic or issue on the Waggle sensor message board:\\n\\nGitHub Discussions'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/sesctl.md'}, page_content=\"sidebar_label: sesctl sidebar_position: 2\\n\\nsesctl: a tool to schedule jobs in Waggle edge computing\\n\\nThe tool sesctl is a command-line tool that communicates with an Edge scheduler in the cloud to manage user jobs. Users can create, edit, submit, suspend, and remove jobs via the tool.\\n\\nInstallation\\n\\nThe tool can be downloaded from the edge scheduler repository and be run on person's desktop or laptop.\\n\\n:::note Please make sure to download the correct version of the tool based on the system architecture. For example, if you run it on a Mac download sesctl-darwin-amd64. :::\\n\\nbash chmod +x sesctl-<system>-<arch> ln sesctl-<system>-<arch> sesctl sesctl\\n\\nSubmit a job\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/sesctl.md'}, page_content='You can follow the tutorial to submit an example job to understand how to design your own job.\\n\\nFor more tutorials\\n\\nThe in-depth tutorials on the functionalities that sesctl offers can be found in the README.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/pluginctl.md'}, page_content='sidebar_label: pluginctl sidebar_position: 1\\n\\npluginctl: a tool to develop and test plugins on a node\\n\\nWe developed the tool pluginctl to help end users develop and test their edge application (i.e., plugin) on a node before registering the plugin in Edge code repository. The tool helps on simplifying the process of testing the edge code and making changes as needed for development, by buildig the code into a container, running the container inside the node, and checking the result from the container.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/pluginctl.md'}, page_content='All of Waggle nodes have the tool already installed. For plugin developers who have access to nodes, they can simply type the following to start with once they are logged into a node, bash sudo pluginctl\\n\\nThe in-depth tutorials on the functionalities that pluginctl offers can be found in the README.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/triggers.md'}, page_content='sidebar_label: Trigger examples sidebar_position: 3\\n\\nTrigger Examples\\n\\nThis page provides a few examples of triggers within Sage. Triggers are programs which generally use data and events from the edge or cloud to automatically drive or notify other behavior in the system.\\n\\nCloud-to-Edge Examples\\n\\nCloud-to-edge triggers are programs running in the cloud which monitor events or external data sources and then, in response, change some behavior on the nodes.\\n\\nSevere Weather Trigger\\n\\nThis example starts and stops jobs in response to severe weather events scraped from the National Weather Service API.\\n\\nWildfire Trigger'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/triggers.md'}, page_content='This example looks at results from the smoke detector job and modify its own scheduling interval in response. The concept is that as smoke is detected, we want to run more frequent detections.\\n\\nEdge-to-Cloud Examples\\n\\nEdge-to-cloud triggers are programs which monitor data published from the nodes and use it, potentially along with additional data sources, to perform some computation or actions.\\n\\nSage Data Client Batch Trigger\\n\\nThis is a simple batch trigger example of using Sage Data Client to print nodes where the internal mean temperature exceeds a threshold every 5 minutes.\\n\\nSage Data Client Stream Trigger'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/triggers.md'}, page_content='This is an example of using Sage Data Client to watch the data stream and print nodes where the internal temperature exceeds a threshold.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Developer quick reference\\n\\nDisclaimer\\n\\n:warning: This is a quick-reference guide, not a complete guide to making a plugin. Use this to copy-paste commands while working on plugins and to troubleshoot them in the testing and scheduling stages. Please consult the official :green_book:Plugin Tutorials for detailed guidance.\\n\\nTips\\n\\n:information_source: Plugin=App\\n\\n:green_book: = recommended code docs and tutorials from Sage.\\n\\n:point_right: First make a minimalistic app with a core functionality to test on the node. Later you may add all the options you want.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=':point_up: Avoid making a plugin from scratch. Use another plugin or this template for your first plugin or use :new: Cookiecutter Template.\\n\\n:warning: Repository names should be all in small alphanumeric letters and - (Do not use _)\\n\\nRequirements : Install Docker, git, and Python\\n\\nComponents of a plugin\\n\\nTypical components of a Sage plugin are described below:\\n\\n1. An application\\n\\nThis is just your usual Python program, either a single .py script or a set of directories with many components (e.g. ML models, unit tests, test data, etc).\\n\\n:point_right: First do this step on your machine and perfect it until you are happy with the core functionality.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=\"app/app.py* : the main Python file (sometimes also named main.py) contains the code that defines the functionality of the plugin or calls other scripts to do tasks. It usually has from waggle.plugin import Plugin call to get the data from in-built sensors and publishes the output.\\n\\nNote: Variable names in plugin.publish should be descriptive and specific.\\n\\nInstall pywaggle pip3 install -U 'pywaggle[all]'\\n\\napp/test.py : optional but recommended file, contains the unit tests for the plugin.\\n\\n2. Dockerizing the app\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=':point_right: Put everything in a Docker container using a waggle base image and make it work. This may require some work if libraries are not compatible. Always use the latest base images from Dockerhub\\n\\nDockerfile* : contains instructions for building a Docker image for the plugin. It specifies the waggle base image from dockerhub, sets up the environment, installs dependencies, and sets the entrypoint for the container.\\n\\n:warning: Keep it simple ENTRYPOINT [\"python3\", \"/app/app.py\"]\\n\\nrequirements.txt* : lists the Python dependencies for the plugin. It is used by the Dockerfile to install the required packages using pip.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='build.sh : is an optional shell script to automate building the complicated Docker image with tags etc.\\n\\nMakefile : optional but the recommended file includes commands for building the Docker image, running tests, and deploying the plugin.\\n\\n3. ECR configs and docs\\n\\nYou can do this step (except sage.yaml) after testing on the node but before the ERC submission. :smile:\\n\\nsage.yaml* : is the configuration file useful for ECR and job submission? Most importantly it specifies the version and input arguments.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='README.md and ecr-meta/ecr-science-description.md* : a Markdown file describing the scientific rationale of the plugin as an extended abstract. This includes a description of the plugin, installation instructions, usage examples, data downloading code snippets, and other relevant information.\\n\\n:bulb: Keep the same text in both files and follow the template of ecr-science-description.md.\\n\\necr-meta/ecr-icon.jpg : is an icon (512px x 512px or smaller) for the plugin in the Sage portal.\\n\\necr-meta/ecr-science-image.jpg : is a key image or figure plot that best represents the scientific output of the plugin.\\n\\n:::info :green_book: Check Sage Tuorial Part1 and Part2 :::\\n\\nGetting access to the node'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Follow this page: https://portal.sagecontinuum.org/account/access to access the nodes.\\n\\nTo test your connection the first time, execute ssh waggle-dev-sshd and enter your ssh key passphrase. You should get the following output,\\n\\nEnter passphrase for key /Users/bhupendra/.ssh/id_rsa: no command provided Connection to 192.5.86.5 closed.\\n\\nEnter the passphrase to continue.\\n\\nTo connect to the node, execute ssh waggle-dev-node-V032 and enter your passphrase (required twice).\\n\\nYou should see the following message,\\n\\nWe are connecting you to node V032\\n\\n:::info :green_book: See Sage Tuorial: Part 3 for details on this topic. :::\\n\\nTesting plugins on the nodes'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=\":::danger :warning: Do not run any app or install packages directly on the node. Use Docker container or pluginctl commands. :::\\n\\n1. Download and run it\\n\\nDownload\\n\\nIf you have not already done it, you need your plugin in a public GitHub repository at this stage.\\n\\nTo test the app on a node, go to nodes W0xx (e.g. W023) and clone your repo there using the command git clone.\\n\\nAt this stage, you can play with your plugin in the docker container until you are happy. Then if there are changes made to the plugin, I reccomend replicating the same in your local repository and pushing it to the github and node.\\n\\nor do git commit -am 'changes from node' and git push -u origin main.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='However, before commiting from node, you must run following commands at least once in your git repository on the node. git config [--locale] user.name \"Full Name\" git config [--locale] user.email \"email@address.com\"\\n\\n:::danger :warning: Make sure your Dockerfile has a proper entrypoint or the pluginctl run will fail. :::\\n\\nTesting with Pluginctl\\n\\n:::info :green_book: For more details on this topic check pluginctl docs. :::\\n\\nThen to test execute the command sudo pluginctl build .. This will output the plugin-image registry address at the end of the build. Example: 10.31.81.1:5000/local/my-plugin-name'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='To run the plugin without input argument, use sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name>\\n\\nExecute the command with input arguments. sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name> -- -i top_camera.\\n\\nIf you need GPU, use the selector sudo pluginctl deploy -n <some-unique-name> <10.31.81.1:5000/local/my-plugin-name> -- -i top_camera.\\n\\n:exclamation: -- is a separator. After the -- all arguments are for your entrypoint i.e. app.py.\\n\\nTo check running plugins, execute sudo pluginctl ps.\\n\\nTo stop the plugin, execute sudo pluginctl rm cloud-motion.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='To check the log pluginctl logs cloud-motion :warning:Do not forget to stop the plugins after testing or it will run forever.\\n\\nTesting USBSerial devices\\n\\n:point_right:The USBserial device template is in Cookiecutter Template. Also check wxt536 plugin.\\n\\nSteps for working with a USB serial device\\n\\nFirst, you need to confirm which computing unit the USB device is connected to, RPi or nxcore.\\n\\nThen, you add the --selector and --privileged options to the pluginctl command during testing and specifying the same in the job.yaml for scheduling.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='To test the plugin on nxcore, which has the USB device, use the command sudo pluginctl run -n testname --selector zone=core --privileged 10.31.81.1:5000/local/plugin-name.\\n\\nThe --selector and --privileged attributes should be added to the pluginSpec in the job submission script as shown in the example YAML code.\\n\\nYou can check which computing unit is being used by the edge scheduler by running the kubectl describe pod command and checking the output.\\n\\n:warning: Re/Check that you are using the correct USB port for the device if getting empty output or folder not found error.\\n\\n2. Check if it worked?'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Login to the Sage portal and follow the instructions from the section See Your Data on Sage Portal\\n\\n3. Check why it failed?\\n\\nWhen you encounter a failing/long pending job with an error, you can use the following steps to help you diagnose the issue:\\n\\nFirst check the Dockerfile entrypoint.\\n\\nUse the command sudo kubectl get pod to get the name of the pod associated with the failing job.\\n\\nUse the command sudo kubectl logs <<POD_NAME>> to display the logs for the pod associated with the failing job. These logs will provide you with information on why the job failed.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Use the command sudo kubectl describe pod POD_NAME to display detailed information about the pod associated with the failing job.\\n\\nThis information can help you identify any issues with the pod itself, such as issues with its configuration or resources.\\n\\nBy following these steps, you can better understand why the job is failing and take steps to resolve the issue.\\n\\n4. Troubleshooting inside the container using pluginctl\\n\\nFollow this tutorial to get in an already running container to troubleshoot the issue. If the plugin fails instantly and your are not able to get inside the container use following commands to override the entrypoint'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=\"First Deploy with Custom Entrypoint --entrypoint /bin/bash: sudo pluginctl deploy -n testnc --entrypoint /bin/bash 10.31.81.1:5000/local/plugin-mobotix-scan -- -c 'while true; do date; sleep 1; done' Note the -c 'while true; do date; sleep 1; done' instead of your usual plugin arguments. Now if you do sudo pluginctl logs testnc you will see the logs i.e. date.\\n\\nAccess the Plugin Container: sudo pluginctl exec -ti testnc -- /bin/bash\\n\\nEdge Code Repository\\n\\nHow to get your plugin on ECR\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='To publish your Plugin on ECR, follow these steps: 1. Go to https://portal.sagecontinuum.org/apps/. 2. Click on \"Explore the Apps Portal\". 3. Click on \"My Apps\". You must be logged in to continue. 4. Click \"Create App\" and enter your Github Repo URL. 5. \\'Click \"Register and Build App\". 6. On Your app page click on the \"Tags\" tab to get the registry link when you need to run the job on the node either using pluginctl or job script. This will look like:docker pull registry.sagecontinuum.org/bhupendraraut/mobotix-move:1.23.3.2 7. Repeat the above process for updating the plugin.\\n\\n:::warning After the build process is complete, you need to make the plugin public to schedule it. :::'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=\":point_right: If you have skipped step 3. ECR Configs and Docs, do it before submitting it to the ECR. Ensure that your ecr-meta/ecr-science-description.md and sage.yaml files are properly configured for this process.\\n\\nVersioning your code\\n\\n:::danger You can not resubmit the plugin to ECR with the same version number again. ::: - So think about how you change it every time you resubmit to ERC and make your style of versioning. :thinking_face: - I use 'vx.y.m.d' e.g. 'v0.23.3.4' but then I can only have 1 version a day, so now I am thinking of adding an incremental integer to it.\\n\\nAfter ECR registry test (generally not required)\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Generally successfully tested plugins just work. However, in case they are failing in the scheduled jobs after running for a while or after successfully running in the above tests, do the following.\\n\\nTo test a plugin on a node after it has been built on the ECR, follow these steps: sudo pluginctl run --name test-run registry.sagecontinuum.org/bhupendraraut/cloud-motion:1.23.01.24 -- -input top\\n\\nThis command will execute the plugin with the specified ECR image (version 1.23.01.24), passing the \"-input top\" argument to the plugin (Note -- after the image telling pluginctl that these arguments are for the plugin).'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=':point_right: Note the use of sudo in all pluginctl and docker commands on the node.\\n\\nAssuming that the plugin has been installed correctly and the ECR image is available, running this command should test the \"test-motion\" plugin on the node.\\n\\nYou may also have to call the kubectl <POD> commands as in the testing section if this fails.\\n\\nScheduling the job\\n\\n:::warning :exclamation: If you get an error like registry does not exist in ECR, then check that your plugin is made public. :::\\n\\nFollow this link to get an understanding of how to submit a job\\n\\nHere are the parameters we set for the Mobotix sampler plugin,'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='less= -name thermalimaging registry.sagecontinuum.org/bhupendraraut/mobotix-sampler:1.22.4.13 \\\\ --ip 10.31.81.14 \\\\ -u userid \\\\ -p password \\\\ --frames 1 \\\\ --timeout 5 \\\\ --loopsleep 60 - Your science rule can be a cronjob (More information can be found here - This runs every 15 minutes \"thermalimaging\": cronjob(\"thermalimaging\", \"*/15 * * * *\"). - Use Crontab Guru. - You can also make it triggered by a value. Please read this for supported functions.\\n\\nScheduling scripts\\n\\n:sparkles: Check user friendly job submission UI.\\n\\n:green_book: Check sesctl docs for command line tool.\\n\\n:point_up: Do not use _, upper case letters or . in the job name. Use only lowercase letters, numbers and -.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=':point_up: Ensure that the plugin is set to \\'public\\' in the Sage app portal.\\n\\njob.yaml example for USB device\\n\\nyaml= name: atmoswxt plugins: - name: waggle-wxt536 pluginSpec: image: registry.sagecontinuum.org/jrobrien/waggle-wxt536:0.23.4.13 privileged: true selector: zone: core nodeTags: [] nodes: W057: true W039: true scienceRules: - \\'schedule(\"waggle-wxt536\"): cronjob(\"waggle-wxt536\", \"1/10 * * * *\")\\' successCriteria: - WallClock(\\'1day\\')\\n\\nMultiple jobs example\\n\\nIf you want to run your plugins not all at the same time. Use this example.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='```yaml= name: w030-k3s-upgrade-test plugins: - name: object-counter-bottom pluginSpec: image: registry.sagecontinuum.org/yonghokim/object-counter:0.5.1 args: - -stream - bottom_camera - -all-objects selector: resource.gpu: \"true\" - name: cloud-cover-bottom pluginSpec: image: registry.sagecontinuum.org/seonghapark/cloud-cover:0.1.3 args: - -stream - bottom_camera selector: resource.gpu: \"true\" - name: surfacewater-classifier pluginSpec: image: registry.sagecontinuum.org/seonghapark/surface_water_classifier:0.0.1 args: - -stream - bottom_camera - -model - /app/model.pth - name: avian-diversity-monitoring pluginSpec: image:'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='registry.sagecontinuum.org/dariodematties/avian-diversity-monitoring:0.2.5 args: - --num_rec - \"1\" - --silence_int - \"1\" - --sound_int - \"20\" - name: cloud-motion-v1 pluginSpec: image: registry.sagecontinuum.org/bhupendraraut/cloud-motion:1.23.02.20 args: - --input - bottom_camera - name: imagesampler-bottom pluginSpec: image: registry.sagecontinuum.org/theone/imagesampler:0.3.1 args: - -stream - bottom_camera - name: audio-sampler pluginSpec: image: registry.sagecontinuum.org/seanshahkarami/audio-sampler:0.4.1 nodeTags: [] nodes: W030: true scienceRules: - \\'schedule(object-counter-bottom): cronjob(\"object-counter-bottom\", \"/5 * * * \")\\' - \\'schedule(cloud-cover-bottom):'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='cronjob(\"cloud-cover-bottom\", \"01-59/5 * * * \")\\' - \\'schedule(surfacewater-classifier): cronjob(\"surfacewater-classifier\", \"02-59/5 * * * \")\\' - \\'schedule(\"avian-diversity-monitoring\"): cronjob(\"avian-diversity-monitoring\", \" * * * \")\\' - \\'schedule(\"cloud-motion-v1\"): cronjob(\"cloud-motion-v1\", \"03-59/5 * * * \")\\' - \\'schedule(imagesampler-bottom): cronjob(\"imagesampler-bottom\", \"04-59/5 * * * \")\\' - \\'schedule(audio-sampler): cronjob(\"audio-sampler\", \"/5 * * * \")\\' successCriteria: - Walltime(1day)'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='```\\n\\nhere objecct-counter runs at 0, 5, 10, etc cloud-cover: 1, 6, 11, etc. surface water: 2, 7, 12, etc. cloud-motion: 3, 8, 13, etc. image-sampl: 4, 9, 14, etc.\\n\\nDebugging failed jobs\\n\\nDo you know how to identify why a job is failing\\n\\n:sparkles: When the job failures are seen as red markers on your job page, you can click them to see the error.\\n\\nOr detail errors can be found using using sage_data_client\\n\\nRequirements: sage_data_client and utils.py\\n\\nBy specifying the plugin name and node, the following code will print out the reasons for job failure within the last 60 minutes.\\n\\n```python= from utils import *\\n\\nmynode = \"w030\"'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='myplugin = \"water\" df = fill_completion_failure(parse_events(get_data(mynode, start=\"-60m\"))) for _, p in df[df[\"plugin_name\"].str.contains(myplugin)].iterrows(): print(p[\"error_log\"])'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='```\\n\\nDownloading the data\\n\\nSage docs for accessing-data\\n\\nSee Your Data on Sage Portal\\n\\nTo check your data on Sage Portal, follow these steps: 1. Click on the Data tab at the top of the portal page. 2. Select Data Query Browser from the dropdown menu. 3. Then, select your app in the filter. This will show all the data that is uploaded by your app using the plugin.publish() and plugin.upload() methods.\\n\\nIn addition, you can data visualize as a time series and select multiple variables to visualize together in a chart, which can be useful for identifying trends or patterns.\\n\\nDownload all images with wget\\n\\nVisit https://training-data.sagecontinuum.org/\\n\\nselect the node and period for data.'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=\"Select the required data and download the text file urls-xxxxxxx.txt with urls\\n\\nTo select only the top camera images, use the vim command: g/^\\\\(.*top\\\\)\\\\@!.*$/d. This will delete URLs that do not contain the word 'top'\\n\\nCopy the following command from the website and run it in your terminal. wget -r -N -i urls-xxxxxxx.txt\\n\\nSage data client for text data\\n\\nSage data client python Notebook Example\\n\\npypi link pip install sage-data-client\\n\\n:::info :green_book: Documentation for accessing the data. :::\\n\\nQuerying data example\\n\\nThe sage_data_client provides query() function which takes the parameters:\\n\\n```python import sage_data_client import pandas as pd\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='df = sage_data_client.query( start=\"2023-01-08T00:00:09Z\", # Start time in \"YYYY-MM-DDTHH:MM:SSZ\" or \"YYYYMMDD-HH:MM:SS\" format end=\"2024-01-08T23:23:24Z\", # End time in the same format as start time filter={ \"plugin\": \".mobotix-scan.\", # Regex for filtering by plugin name \"vsn\": \"W056\", # Specific node identifier \"name\": \"upload\", # Specific data field \"filename\": \".*_position1.nc\" # Regex for filtering filenames } )\\n\\ndf.sort_values(\\'timestamp\\') df'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=\"```\\n\\nFilter Criteria\\n\\nstart and end: Time should be specified in UTC, using the format YYYY-MM-DDTHH:MM:SSZ or YYYYMMDD-HH:MM:SS.\\n\\nfilter: A dictionary for additional filtering criteria. Each key is a column name in the df.\\n\\nUse regular expressions (denoted as .*pattern.*) for flexible matching within text fields like plugin or filename.\\n\\nDownloading Files\\n\\nUse additional pandas operations on df to to include only the records of interest and download the files using a function like the one provided below, which gets the URLs in the value column, using authentication.\\n\\n```python import requests import os from requests.auth import HTTPBasicAuth\\n\\nuname = 'username' upass = 'token_as_password'\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=\"def download_files(df, download_path, uname, upass): # check download directory if not os.path.exists(download_path): os.makedirs(download_path)\\n\\nfor index, row in df.iterrows(): # 'value' column has url url = row['value']\\n\\n  filename = url.split('/')[-1]\"),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='# Download using credentials\\n  response = requests.get(url, auth=HTTPBasicAuth(uname, upass))\\n  if response.status_code == 200:\\n     # make the downloads path\\n     file_path = os.path.join(download_path, filename)\\n     # Write a new file\\n     with open(file_path, \\'wb\\') as file:\\n     file.write(response.content)\\n     print(f\"Downloaded {filename} to {file_path}\")\\n  else:\\n     print(f\"Failed to download {url}, status code: {response.status_code}\")\\n\\nusage\\n\\ndownload_files(df, \\'/Users/bhupendra/projects/epcape_pier/data/downloaded/nc_pos1\\', uname, upass)'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='```\\n\\nMore data analysis resources\\n\\nSAGE Examples\\n\\nCROCUS Cookbooks\\n\\nMiscellaneous\\n\\nFind PT Mobotix thermal camera ip on the node\\n\\nLogin to the node where the PTmobotix camera is connected. 1. run nmap -sP 10.31.81.1/24'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content='Nmap scan report for ws-nxcore-000048B02D3AF49F (10.31.81.1) Host is up (0.0012s latency). Nmap scan report for switch (10.31.81.2) Host is up (0.0058s latency). Nmap scan report for ws-rpi (10.31.81.4) Host is up (0.00081s latency). Nmap scan report for 10.31.81.10 Host is up (0.0010s latency). Nmap scan report for 10.31.81.15 Host is up (0.00092s latency). Nmap scan report for 10.31.81.17 Host is up (0.0014s latency). Nmap done: 256 IP addresses (6 hosts up) scanned in 2.42 seconds\\n\\nFrom the output run any command for each ip e.g. curl -u admin:meinsm -X POST http://10.31.81.15/control/rcontrol?action=putrs232&rs232outtext=%FF%01%00%0F%00%00%10'),\n",
       " Document(metadata={'source': 'sage-website/docs/reference-guides/dev-quick-reference.md'}, page_content=\"The ip for which output is OK is the Mobotix.\\n\\nSSH 'Broken Pipe' Issue and Solution\\n\\nA 'Broken pipe' occurs when the SSH session to waggle-dev-node is inactive for longer than 10/15 minutes, resulting in a closed connection.\\n\\nclient_loop: send disconnect: Broken pipe Connection to waggle-dev-node-w021 closed by remote host. Connection to waggle-dev-node-w021 closed.\\n\\nSolution\\n\\nTo prevent the SSH session from timing out and to maintain the connection, the following configuration options can be added to the SSH config file: ```ssh\\n\\nKeep the SSH connection alive by sending a message to the server every 60 seconds\\n\\nHost * TCPKeepAlive yes ServerAliveInterval 60 ServerAliveCountMax 999 ```\"),\n",
       " Document(metadata={'source': 'sage-website/docs/installation-manuals/wsn-manual.md'}, page_content='Wild Sage Node manual\\n\\nThe Wild Sage Node \"Getting Started\" manual is a complete overview of getting started with your new WSN.\\n\\nDownload WSN manual'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='sidebar_label: Architecture sidebar_position: 2\\n\\nArchitecture\\n\\nThe cyberinfrastructure consists of coordinating hardware and software services enabling AI at the edge. Below is a quick summary of the different infrastructure pieces, starting at the highest-level and zooming into each component to understand the relationships and role each plays.\\n\\nHigh-Level Infrastructure\\n\\nThere are 2 main components of the cyberinfrastructure: - Nodes that exist at the edge - The cloud that hosts services and storage systems to facilitate running “science goals” @ the edge\\n\\nEvery edge node maintains connections to 2 core cloud components: one to a Beehive and one to a Beekeeper\\n\\nBeekeeper'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The Beekeeper is an administrative server that allows system administrators to perform actions on the nodes such as gather health metrics and perform software updates. All nodes \"phone home\" to their Beekeeper and maintain this \"life-line\" connection.\\n\\nDetails & source code: https://github.com/waggle-sensor/beekeeper\\n\\nBeehive\\n\\nThe Node-to-Beehive connection is the pipeline for the science. It is over this connection that instructions for the node will be sent, in addition to how data is published into the Beehive storage systems from applications (plugins) running on the nodes.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The overall infrastructure supports multiple Beehives, where each node is associated with a single Beehive. The set of nodes associated with a Beehive creates a \"project\" where each \"project\" is separate, having its own data store, web services, etc.\\n\\nIn the example above, there are 2 nodes associated with Beehive 1, while a single node is associated with Beehive 2. With all nodes, in this example, being administered by a single Beekeeper.\\n\\nNote: the example above shows a single Beekeeper, but a second Beekeeper could have been used for administrative isolation.\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-beehive-v2\\n\\nBeehive Infrastructure'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='Looking deeper into the Beehive infrastructure, it contains 2 main components: - software services such as the Edge Scheduler (ES), Lambda Triggers (LT), data APIs, and websites/portals - data storage systems such as the Data Repository (DR) and the Edge Code Repository (ECR)\\n\\nThe Beehive is the “command center” for interacting with the Waggle nodes at the edge. Hosting websites and interfaces allowing scientists to create science goals to run plugins at the edge & browse the data produced by those plugins.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The software services and data storage systems are deployed within a kubernetes environment to allow for easy administration and to support running in a multiple server architecture, supporting redundancy and service replication.\\n\\nWhile the services running within Beehive are many (both graphical and REST style API interfaces), the following is an outline of the most vital.\\n\\nData Repository (DR)'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The Data Repository is the data store for housing all the edge produced plugin data. It consists of different storage technologies (i.e. influxdb) and techniques to store simple textual data (i.e. key-value pairs) in addition to large blobular data (i.e. audio, images, video). The Data Repository additionally has an API interface for easy access to this data.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The data store is a time-series database of key-value pairs with each entry containing metadata about how and when the data originated @ the edge. Included in this metadata is the data collection timestamp, plugin version used to collect the data, the node the plugin was run on, and the specific compute unit within the node that the plugin was running on.\\n\\njson { \"timestamp\":\"2022-06-10T22:37:47.369013647Z\", \"name\":\"iio.in_temp_input\", \"value\":25050, \"meta\":{ \"host\":\"0000dca632ed6d06.ws-rpi\", \"job\":\"sage\", \"node\":\"000048b02d35a97c\", \"plugin\":\"plugin-iio:0.6.0\", \"sensor\":\"bme680\", \"task\":\"iio-rpi\", \"vsn\":\"W08C\" } }'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='In the above example, the value of 25050 was collected @ 2022-06-10T22:37:47.369013647Z from the bme680 sensor on node 000048b02d35a97c via the plugin-iio:0.6.0 plugin.\\n\\nNote: see the Access and use data site for more details and data access examples.\\n\\nDetails & source code: https://github.com/waggle-sensor/data-repository\\n\\nEdge Scheduler (ES)'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content=\"The Edge Scheduler is defined as the suite of services running in Beehive that facilitate running plugins @ the edge. Included here are user interfaces and APIs for scientists to create and manage their science goals. The Edge Scheduler continuously analyzes node workloads against all the science goals to determine how the science goals are deployed to the Beehive nodes. When it is determined that a node's science goals are to be updated, the Edge Scheduler interfaces with WES running on those nodes to update the node's local copy of the science goals. Essentially, the Edge Scheduler is the overseer of all the Beehive's nodes, deploying science goals to them to meet the scientists plugin\"),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='execution objectives.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='Details & source code: https://github.com/waggle-sensor/edge-scheduler\\n\\nEdge Code Repository (ECR)\\n\\nThe Edge Code Repository is the \"app store\" that hosts all the tested and benchmarked edge plugins that can be deployed to the nodes. This is the interface allowing users to discover existing plugins (for potential inclusion in their science goals) in addition to submitting their own. At it\\'s core, the ECR provides a verified and versioned repository of plugin Docker images that are pulled by the nodes when a plugin is to be downloaded as run-time component of a science goal.\\n\\nDetails & source code: https://github.com/waggle-sensor/edge-code-repository\\n\\nLambda Triggers (LT)'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content=\"The Lambda Triggers service provides a framework for running reactive code within the Beehive. There are two kinds of reaction triggers considered here: From-Edge and To-Edge.\\n\\nFrom-Edge triggers, or messages that originate from an edge node, can be used to trigger lambda functions -- for example, if high wind velocity is detected, a function could be triggered to determine how to reconfigure sensors or launch a computation or send an alert.\\n\\nTo-Edge triggers are messages that are to change a node's behavior. For example an HPC calculation or cloud-based data analysis could trigger an Edge Scheduler API call to request a science goal to be run on a particular set of edge nodes.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content=\"Details & source code: https://github.com/waggle-sensor/lambda-triggers\\n\\nNodes\\n\\nNodes are the edge computing component of the cyberinfrastructure. All nodes consist of 3 items: 1. Persisent storage for housing downloaded plugins and caching published data before it is transferred to the node's Beehive 2. CPU and GPU compute modules where plugins are executed and perform the accelerated inferences 3. Sensors such as environment sensors, cameras and LiDAR systems\"),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='Edge nodes enable fast computation @ the edge, leveraging the large non-volatile storage to handle caching of high frequency data (including images, audio and video) in the event the node is \"offline\" from its Beehive. Through expansion ports the nodes support the adding and removing of sensors to fully customize the node deployments for the particular deployment environment.\\n\\nOverall, even though the nodes may use different CPU architectures and different sensor configurations, they all leverage the same Waggle Edge Stack (WES) to run plugins.\\n\\nWild Sage Node (Wild Waggle Node)'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The Wild Sage Node (or Wild Waggle Node) is a custom built weather-proof enclosure intended for remote outdoor installation. The node features software and hardware resilience via a custom operating system and custom circuit board. Internal to the node is a power supply and PoE network switch supporting the addition of sensors through standard Ethernet (PoE), USB and other embedded protocols via the node expansion ports.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The technical capabilities of these nodes consists of: - NVidia Xavier NX ARM64 Node Controller w/ 8GB of shared CPU/GPU RAM - 1 TB of NVMe storage - 4x PoE expansion ports - 1x USB2 expansion port - optional Stevenson Shield housing a RPi 4 w/ environmental sensors & microphone - optional 2nd NVidia Xavier NX ARM64 Edge Processor\\n\\nNode installation manual: https://sagecontinuum.org/docs/installation-manuals/wsn-manual\\n\\nDetails & source code: https://github.com/waggle-sensor/wild-waggle-node\\n\\nBlade Nodes'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='A Blade Node is a standard commercially available server intended for use in a climate controlled machine room, or extended temperature range telecom-grade blades for harsher environments. The AMD64 based operating system supports these types of nodes, enabling the services needed to support WES.\\n\\nThe above diagram shows the basic technical configuration of a Blade Node: - Multi-core ARM64 - 32GB of RAM - Dedicated NVida T4 GPU - 1 TB of SSD storage\\n\\nNote: it is possible to add the same optional Stevenson Shield housing that is available to the Wild Sage Nodes\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-blade\\n\\nRunning plugins @ the Edge'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content=\"Included in the Waggle operating systems are the core components necessary to enable running plugins @ the edge. At the heart of this is k3s, which creates a protected & isolated run-time environment. This environment combined with the tools and services provided by WES enable plugin access to the node's CPU, GPU, sensors and cameras.\\n\\nWaggle Edge Stack (WES)\"),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content=\"The Waggle Edge Stack is the set of core services running within the edge node's k3s run-time environment that supports all the features that plugins need to run on the Waggle nodes. The WES services coordinate with the core Beehive services to download & run scheduled plugins (including load balancing) and facilitate uploading plugin published data to the Beehive data repository. Through abstraction technologies and WES provided tools, plugins have access to sensor and camera data.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The above diagram demonstrates 2 plugins running on a Waggle node. Plugin 1 (\"neon-kafka\") is an example plugin that is running alongside Plugin 2 (\"data-smooth\"). In this example, \"neon-kafka\" (via the WES tools) is reading metrics from the node\\'s sensors and then publishing that data within the WES run-time environment (internal to the node). At the same time, the \"data-smooth\" plugin is subscribing to this data stream, performing some sort of inference and then publishing the inference results (via WES tools) to Beehive.\\n\\nNote: see the Edge apps guide on how to create a Waggle plugin.\\n\\nDetails & source code: https://github.com/waggle-sensor/waggle-edge-stack\\n\\nWhat is a plugin?'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='Plugins are the user-developed modules that the cyberinfrastructure is designed around. At it\\'s simplest definition a \"plugin\" is code that runs @ the edge to perform some task. That task may be simply collecting sample camera images or a complex inference combining sensor data and results published from other plugins. A plugin\\'s code will interface with the edge node\\'s sensor(s) and then publish resulting data via the tools provided by WES. All developed plugins are hosted by the Beehive Edge Code Repository.\\n\\nSee how to create plugins for details.\\n\\nScience Goals'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='A \"science goal\" is a rule-set for how and when plugins are run on edge nodes. These science goals are created by scientist to accomplish a science objective through the execution of plugins in a specific manner. Goals are created, in a human language, and managed within the Beehive Edge Scheduler. It is then the cyberinfrastucture responsibility to deploy the science goals to the edge nodes and execute the goal\\'s plugins. The tutorial walks through running a science goal.\\n\\nLoRaWAN'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The Waggle Edge Stack includes the ChirpStack software stack and other services to facilitate communication between Nodes and LoRaWAN devices. This empowers Nodes to effortlessly establish connections with wireless sensors, enabling your plugins to seamlessly access and harness valuable data from these sensors.\\n\\nTo get started using LoRaWAN, head over to the Contact Us page. A tutorial will be available soon showing you how to get started with LoRaWAN.\\n\\nThe above diagram demonstrates the hardware in Nodes and services in WES that enable Nodes to use LoRaWAN and publish the measurements to a Beehive. The following sections will explain each componenent and service.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='source code: - wes-chirpstack - wes-chirpstack-server - wes-rabbitmq - Tracker - Lorawan Listener Plugin\\n\\nWhat is LoRaWAN?\\n\\nLoRaWAN, short for \"Long Range Wide Area Network,\" is a wireless communication protocol designed for low-power, long-range communication between IoT (Internet of Things) devices. It employs a low-power wide-area network (LPWAN) technology, making it ideal for connecting remote sensors and devices. For more information view the documentation here.\\n\\nChirpstack'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='ChirpStack is a robust and open-source LoRaWAN Network Server that enables efficient management of LoRaWAN devices, gateways, and data. Its architecture consists of several crucial components, each serving a distinct role in LoRaWAN network operations. Below, we provide a brief overview of these components along with links to ChirpStack documentation for further insights.\\n\\nChirpstack documentation\\n\\nUDP Packet Forwarder'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The UDP Packet Forwarder is an essential component that acts as a bridge between LoRa gateways and the ChirpStack Network Server. It receives incoming packets from LoRa gateways and forwards them to the ChirpStack Gateway Bridge for further processing. To learn more about the UDP Packet Forwarder, refer to the documentation here.\\n\\nChirpStack Gateway Bridge'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The ChirpStack Gateway Bridge is responsible for translating gateway-specific protocols into a standard format for the ChirpStack Network Server. It connects to a UDP Packet Forwader, ensuring that data is properly formatted and can be seamlessly processed by the network server. For in-depth information on the ChirpStack Gateway Bridge, explore the documentation here.\\n\\nMQTT Broker'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='WES includes a MQTT (Message Queuing Telemetry Transport) broker to handle communication between various services. MQTT provides a lightweight and efficient messaging system. This service ensures that data flows smoothly between the network server, gateways, and applications. You can find detailed information about the MQTT broker integration in the ChirpStack documentation here.\\n\\nChirpStack Server'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='The ChirpStack Server serves as the core component, managing device sessions, data, and application integrations. It utilizes Redis for device sessions, metrics, and caching, ensuring efficient data handling and retrieval. For persistent data storage, ChirpStack uses PostgreSQL, accommodating records for tenants, applications, devices, and more. For a comprehensive understanding of the ChirpStack Server and its associated database technologies, consult the ChirpStack documentation here.\\n\\nNOTE: Chirpstack v4 combined the application and network server into one component.\\n\\nTracker'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content=\"The Tracker is a service designed to record the connectivity of LoRaWAN devices to the Nodes. This service uses the information received from the MQTT broker to call ChirpStack's gRPC API. The information received from the API is then used to keep the Node's manifest up-to-date. Subsequently, it forwards this updated manifest to the Beehive. For more information, view the documentation here.\\n\\nLorawan Listener Plugin\\n\\nThe LoRaWAN Listener is a plugin designed to publish measurements collected from LoRaWAN devices. It simplifies the process of extracting and publishing valuable data from these devices. For more information about the plugin view the plugin page here.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content=\"Lorawan Device Compatibility\\n\\nThe Wild Sage Node is designed to support a wide range of Lorawan devices, ensuring flexibility and adaptability for various applications. If you are wondering which Lorawan devices can be connected to a Wild Sage Node, the device must have the following tech specs:\\n\\ndesigned for US915 (902–928 MHz) frequency region.\\n\\ncompatible with Lorawan Mac versions 1.0.0 - 1.1.0\\n\\ncompatible with Chirpstack's Lorawan Network Server\\n\\nThe device supports Over-The-Air Activation (OTAA) or Activation By Personalization (ABP)\\n\\nThe device has a Lorawan device class of A, B, or C\"),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='It is important to note that all channels within the US915 frequency band are enabled in a Wild Sage Node. If you wish to learn more about our Lorawan Gateway, please visit our portal. For inquiries about supporting Lorawan regions other than US915, please Contact Us.\\n\\nDevice Examples\\n\\nWhether you are designing your own Lorawan sensor, looking for a Lorawan data logger, or seeking an off-the-shelf Lorawan device the Wild Sage Node will support it, we have examples for you:\\n\\nDesigning your own Lorawan sensor?\\n\\nArduino MKR WAN 1310\\n\\nLooking for a Lorawan data logger?\\n\\nICT International MFR Node\\n\\nLooking for an off-the-shelf Lorawan device?\\n\\nICT International SFM1X Sap Flow Meter'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/architecture.md'}, page_content='Seeking Lorawan device manufacturers?\\n\\nICT International\\n\\nRAKwireless\\n\\nThe Things Network Device Marketplace\\n\\nDecentLab'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='sidebar_label: Overview sidebar_position: 1\\n\\nSage: A distributed software-defined sensor network.\\n\\nWhat is Sage?'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='Geographically distributed sensor systems that include cameras, microphones, and weather and air quality stations can generate such large volumes of data that fast and efficient analysis is best performed by an embedded computer connected directly to the sensor. Sage is exploring new techniques for applying machine learning algorithms to data from such intelligent sensors and then building reusable software that can run programs within the embedded computer and transmit the results over the network to central computer servers. Distributed, intelligent sensor networks that can collect and analyze data are an essential tool for scientists seeking to understand the impacts of global'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='urbanization, natural disasters such as flooding and wildfires, and climate change on natural ecosystems and city infrastructure.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='Sage is deploying sensor nodes that support machine learning frameworks in environmental testbeds in California, Colorado, and Kansas and in urban environments in Illinois and Texas. The reusable cyberinfrastructure running on these testbeds will give climate, traffic, and ecosystem scientists new data for building models to study these coupled systems. The software components developed are open source and provide an open architecture to enable scientists from a wide range of fields to build their own intelligent sensor networks.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='Partners are deploying testbeds in Australia, Japan, UK, and Taiwan, providing scientists with even more data for analysis. The toolkit is also extending the current educational curriculum used in Chicago to inspire young people – with an emphasis on women and minorities, to pursue science, technology, and mathematics careers – by providing a platform for students to explore measurement-based science questions related to the natural and built environments.\\n\\nThe data from sensors and applications is hosted in the cloud to facilitate easy data analysis.\\n\\nWho are the users?\\n\\nThe most common users have included:\\n\\nDomain scientists interested in developing edge AI applications.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='Users interested in sensor and application-produced datasets.\\n\\nCyberinfrastructure researchers interested in platform research.\\n\\nDomain scientists interested in adding new sensors and deploying nodes to answer specific science questions.\\n\\nHow do I use the platform?\\n\\nThis depends on your desired interaction interest. The platform consists of edge compute applications which process data (ex. sensor readings, camera images, audio recordings, etc). These edge applications then produce their own data (ex. inferences) and upload the results to a cloud database. This cloud database can be accessed directly and/or additional compute can be performed on the cloud data.'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='The entry-point into learning about your interaction with the system might be best directed by getting answers (by following the links) to the question(s) you are most interested in.\\n\\nHow do I access sensors? - Want to learn about existing, supported sensors? - Do you have a new sensor that you want to write an edge application for?\\n\\nHow do I run edge apps? - Want to know how to create an edge app? - Want to know how your edge app can get access to edge sensor data? - Want to share your edge app data with other edge applications? - Want to know how to upload data to the cloud?'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='How do I access and use data? - Want to learn about how data is stored/organized? - Do you have data that is up in the cloud and want to know how to access it?\\n\\nHow do I compute in the cloud? - Want to know how to autonomously react to edge produced data? - Want to know how to trigger an HPC event? - Want to get a text message when your edge application does something cool?\\n\\nHow do I build my own device? - Want to set up your own device for local edge app development? - Want to teach AI to a classroom of students?\\n\\nHow is the cyberinfrastructure architected?'),\n",
       " Document(metadata={'source': 'sage-website/docs/about/overview.md'}, page_content='If you are interested in learning more about how the cyberinfrastructure works you can head on over to the Architecture Overview page.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='sidebar_position: 3\\n\\nSubmit your job\\n\\nAre you ready to deploy your plugins to measure the world? We will use edge scheduler to submit a job and demonstrate how you can deploy plugins to field-deployed Waggle nodes.\\n\\n:::caution If you have not created your account, please go to https://portal.sagecontinuum.org and sign in to create a new account with your email. Once signed in, you will be able to create and edit your jobs, but will need a permission to submit jobs to the scheduler. Please contact-us to request for the job submission permission. :::'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='Jobs are an instance of a science goal. They detail what needs to be accomplished on Waggle nodes. A science goal may have multiple jobs to fill the missing data to answer scientific questions of the goal. A job describes, - plugins that are registered and built in edge code repository with specification including any plugin arguments, - a list of Waggle nodes on which the plugins will be scheduled and run, - science rules describing a condition-action set that includes when the plugins should be scheduled, - conditions to determine when the job is considered as completed\\n\\nCreating and submitting jobs are an important step for successful science mission using Waggle nodes.\\n\\nCreate a job'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='We create a job file in YAML format (JSON format is also supported. Please check out details of job attributes.)\\n\\n```bash cat << EOF > myjob.yaml\\n\\nname: myjob plugins: - name: image-sampler pluginSpec: image: registry.sagecontinuum.org/theone/imagesampler:0.3.0 args: - -stream - bottom_camera nodes: W023: scienceRules: - \"schedule(image-sampler): cronjob(\\'image-sampler\\', \\' * * * \\')\" successcriteria: - WallClock(1d) EOF'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='```\\n\\nIn this example, we want to schedule a plugin named image-sampler to collect an image from the camera named bottom_camera on W023 node. As a result of the job execution, we will get images from the node\\'s camera. The job also specifies that the plugin needs to be scheduled every minute (i.e., * * * * * in crontab expression). The job completes 24 hours after the job started to run on the node.\\n\\n:::info We support human-friendly names for the sensors we host. The \"bottom_camear\" is named based on the orientation the camera is attached to the node. The full list of sensors including cameras for the W023 node can be found here :::'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content=\":::note We currently do not check job's success criteria. This means that once a job is submitted it is served forever. We will update our system to support different conditions for the success criteria attribute. :::\\n\\nUpload your job to the scheduler\\n\\nsesctl is a command-line tool to manage jobs in the scheduler. You can download the latest version from our Github repository. Please make sure you download the tool supported for your machine. For example, on Linux desktop or laptop you would download linux-amd64 version of the tool. Please see the sesctl document for more details.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content=':::note Once you have contacted us for access permissions, you will need a token provided from the access page. Replace the <<user token>> below with the access token provided on this page. :::\\n\\nYou can set the SES host and user token as an environmental variable to your terminal. Please follow your shell\\'s guidance to set them properly. In Bash shell, bash export SES_HOST=https://es.sagecontinuum.org export SES_USER_TOKEN=<<user token>>\\n\\nLet\\'s ping the scheduler in the cloud, bash sesctl ping\\n\\nYou will get a response \"pong\" from the scheduler, { \"id\": \"Cloud Scheduler (cloudscheduler-sage)\", \"version\": \"0.18.0\" }'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='To create a job using the job file, bash sesctl create --file-path myjob.yaml\\n\\nThe scheduler will return a job id and the state for the job creation, bash { \"job_id\": \"56\", \"job_name\": \"myjob\", \"state\": \"Created\" }\\n\\nTo verify that we have uploaded the job, bash sesctl stat\\n\\nYou will see the job entry from the response of the command, bash JOB_ID NAME USER STATUS AGE ==================================================================== ... 56 myjob theone Created - ...\\n\\nSubmit the job\\n\\nTo submit the job,\\n\\nbash sesctl submit --job-id 56\\n\\nThe response should indicate that the job state is changed to \"Submitted\", bash { \"job_id\": \"56\", \"state\": \"Submitted\" }'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content=':::note You may receive a list of errors from the scheduler if the job fails to be validated. For instance, your account may not have scheduling permission on the node W023. Please consult with us for any error, especially errors related to scheduling permission on nodes in the job. :::\\n\\nCheck status of jobs\\n\\nWe check status of the job we submitted, bash sesctl stat --job-id 56\\n\\nThe tool will print details of the job, ```bash ===== JOB STATUS ===== Job ID: 56 Job Name: myjob Job Owner: Job Status: Submitted Job Starttime: 2022-10-10 02:21:37.373437 +0000 UTC\\n\\n===== SCHEDULING DETAILS ===== Science Goal ID: 45afe963-5b8b-4e15-654c-54e2946f2ddb Total number of nodes 1'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='```\\n\\nThe job status can be also shown in job status page.\\n\\nAccess to data\\n\\nA few minutes later, the W023 Waggle node would start collecting images by scheduling the plugin on the node. Collected images are transferred to Beehive for users to download.\\n\\nconsole curl -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-5m\", \"filter\": { \"task\": \"image-sampler\", \"vsn\": \"W023\", \"name\": \"upload\" } } \\'\\n\\nClean it up\\n\\nAs we approach to the end of this tutorial, we need to clean up the job because otherwise it will be served forever. To remove the job from the scheduler, ```bash\\n\\nsince the job is running, we remove the job forcefully\\n\\nsesctl rm --force 56'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='```\\n\\nYou should see output that looks like, bash { \"job_id\": \"56\", \"state\": \"Removed\" }\\n\\nMore tutorials using sesctl\\n\\nMore tutorials can be found in our Github repository.\\n\\nCreating job description with advanced science rules for supporting realistic science mission'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/schedule-jobs.md'}, page_content='The science rule used in the tutorial asked the scheduler to schedule the image sampler plugin every minute. For collecting training images from a set of Waggle nodes this makes total sense with the science rule. However, users in Waggle should want more complex behaviors at the node to not only schedule plugins, but enable cloud computation triggered by sending local events to the cloud. The events and triggers can be captured by creating science rules that monitor local sensor measurement on nodes. Please visit the science rules to know more complex science rules that user can create.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-waggle.md'}, page_content='sidebar_position: 7\\n\\nBuilding your own Waggle device\\n\\nAre you a professor that wants to use affordable Waggle devices to teach students interested in AI? Are you someone interested in developing a new edge app using a local development platform? Are you a Waggle user interested in using a new sensor (i.e. a new camera, a bat signal detector, a custom sensor they built)? If you would like to build, design and deploy software that could answer your questions above, then Waggle is the right choice for you.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-waggle.md'}, page_content='This tutorial will guide you in preparing your own Waggle device and (optionally) registering it to upload data to a shared development Beehive. This Waggle device is a fully unlocked development platform running the same WES infrastructure that runs in production Waggle edge devices (ex. the Wild Waggle Node). This is an ideal platform for users interested in developing a new edge app and/or experimenting with a new sensor.\\n\\nGetting Started\\n\\nTo get started in boot-strapping your Waggle Edge Computing kit you can follow the instructions for the various supported platforms on the node-platforms GitHub page.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-waggle.md'}, page_content='We currently support a limited set of hardware platform because making edge devices into Waggle requires some hardware specific instructions. Check out the platforms we support as of now. More platforms will be added in the future. However, if you would like to add support for other platforms go ahead and submit a pull request to node-platforms.\\n\\nRegistering your Waggle device'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-waggle.md'}, page_content='During the bootstrapping process you will have the option to register your device within the web portal here. It is highly recommended to register your device, as this enables all the core WES tools to be automatically downloaded, enabling the edge app development and run-time environment. Additionally, this enables your edge apps to publish data to the development Beehive, accessible to cloud-based analysis tools and workflow frameworks.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-waggle.md'}, page_content='To register your device, use the dev devices form. Enter your device ID (which you will obtain through the hardware boot-strapping process) then click \"Get Keys\" button. A \"registration zip\" file will be generated and available for download. Then follow the instructions for your device to load the registration keys.\\n\\nYou may register as many times as you want. But note that each registration key has a short expiration time and should be used shortly after generation.\\n\\nNow you are ready to develop your edge apps and/or introduce new sensors to the Waggle platform. Head over to the overview to find the instructions you need for development.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='sidebar_position: 4\\n\\nAccess and use data\\n\\nRaw sensor data is collected by edge code. This edge code can either talk to sensor hardware directly or may obtain data from an abstraction layer (not show in image above). Edge code may forward unprocessed sensor data, do light processing to convert raw sensor values into final data products, or may use CPU/GPU-intensive workloads (e.g. AI application) to extract information from data-intensive sensors such as cameras, microphone or LIDAR.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='Sensor data from nodes that comes in numerical or textual form (e.g. temperature) is stored natively in our time series database. Sensor data in form of large files (images, audio, movies..) is stored in the Waggle object store, but is referenced in the time series data (thus the dashed arrow in the figure above). Thus, the primary way to find all data (sensor and large files) is via the Waggle sensor query API described below.\\n\\nCurrently the Waggle sensor database contains data such as:\\n\\nRelative humidity, barometric pressure, ambient temperature and gas (VOC) BME680.\\n\\nRainfall measurements (Hydreon RG-15).\\n\\nAI-based cloud coverage estimation from camera images.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='AI-based object counts from camera images.\\n\\nSystem data such as uptime, cpu and memory.\\n\\nData can be accessed in realtime via our data API or in bulk via data bundles.\\n\\nData API\\n\\nWaggle provides a data API for immediate and flexible access to sensor data via search over time and metadata tags. It is primarily intended to support exploratory and near real time use cases.\\n\\nDue to the wide variety of possible queries, we do not attempt to provide DOIs for results from the data API. Instead, we leave it up to users to organize and curate datasets for their own applications. Long term, curated data is instead provided via data bundles.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='There are two recommended approaches to working with the Data API:\\n\\nUsing the Python Sage Data Client.\\n\\nUsing the HTTP API.\\n\\nEach is appropriate for different use cases and integrations, but generally the following rule applies:\\n\\nIf you just want to get data into a Pandas dataframe for analysis and plotting, use the sage-data-client, otherwise use the HTTP API.\\n\\nUsing Sage data client\\n\\nThe Sage data client is a Python library which streamlines querying the data API and getting the results into a Pandas dataframe. For details on installation and usage, please see the Python package.\\n\\nUsing HTTP API'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='This example shows how to retrieve data the latest data from a specific sensor (you can adjust the start field if you do not get any recent data):\\n\\n```console curl -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-10s\", \"filter\": { \"sensor\": \"bme680\" } } \\''),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='Example results:json {\"timestamp\":\"2021-08-09T19:26:03.880781217Z\",\"name\":\"iio.in_humidityrelative_input\",\"value\":70.905,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.878659392Z\",\"name\":\"iio.in_pressure_input\",\"value\":975.78,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}} {\"timestamp\":\"2021-08-09T19:26:03.872652127Z\",\"name\":\"iio.in_resistance_input\",\"value\":93952,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}}'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='{\"timestamp\":\"2021-08-09T19:26:03.874998057Z\",\"name\":\"iio.in_temp_input\",\"value\":27330,\"meta\":{\"node\":\"000048b02d15bdcd\",\"plugin\":\"plugin-metsense:0.1.1\",\"sensor\":\"bme680\"}}'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='```\\n\\n:::tip More details of using the data API and the data model can be found here and here. :::\\n\\nData bundles\\n\\nData bundles provide sensor data and associated metadata in a single, large, downloadable file. Soon, each Data Bundle available for download will have a DOI that can be used for publication citations.\\n\\nData Bundles are compiled nightly and may be downloaded in this archive.\\n\\nAccessing file uploads\\n\\nUser applications can upload files for AI training purposes. These files stored in an S3 bucket hosted by the Open Storage Network.\\n\\nTo find these files use the filter \"name\":\"upload\" and specify additional filters to limit search results, for example:'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='console curl -s -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\'{ \"start\": \"2021-09-10T12:51:36.246454082Z\", \"end\":\"2021-09-10T13:51:36.246454082Z\", \"filter\": { \"name\":\"upload\", \"plugin\":\"imagesampler-left:0.2.3\" } }\\''),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='Output: json {\"timestamp\":\"2021-09-10T13:19:27.237651354Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d05a0a4/1631279967237651354-2021-09-10T13:19:26+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d05a0a4\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T13:50:32.29028603Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bc3d/1631281832290286030-2021-09-10T13:50:32+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bc3d\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}}'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='{\"timestamp\":\"2021-09-10T12:52:59.782262376Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdc2/1631278379782262376-2021-09-10T12:52:59+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}} {\"timestamp\":\"2021-09-10T13:49:49.084350086Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdd2/1631281789084350086-2021-09-10T13:49:48+0000.jpg\",\"meta\":{\"job\":\"sage\",\"node\":\"000048b02d15bdd2\",\"plugin\":\"imagesampler-left:0.2.3\",\"task\":\"imagesampler-left:0.2.3\"}}'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='For a quick way to only extract the urls from the json objects above, a tool like jq can be used:\\n\\nconsole curl -s -H \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\'{ \"start\": \"2021-09-10T12:51:36.246454082Z\", \"end\":\"2021-09-10T13:51:36.246454082Z\", \"filter\": { \"name\":\"upload\", \"plugin\":\"imagesampler-left:0.2.3\" } }\\' | jq -r .value > urls.txt'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='The resulting file urls.txt will look like this: text https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d05a0a4/1631279967237651354-2021-09-10T13:19:26+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bc3d/1631281832290286030-2021-09-10T13:50:32+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdc2/1631278379782262376-2021-09-10T12:52:59+0000.jpg https://storage.sagecontinuum.org/api/v1/data/sage/sage-imagesampler-left-0.2.3/000048b02d15bdd2/1631281789084350086-2021-09-10T13:49:48+0000.jpg\\n\\nTo download the files: console wget -i urls.txt'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content='If many files are downloaded, it is better to preserve the directory tree structure to prevent filename collision: console wget -r -i urls.txt\\n\\nProtected data\\n\\nWhile most Waggle data is open and public - some types of data, such as raw images and audio from sensitive locations, may require additional steps:\\n\\nYou will need a Sage account.\\n\\nYou will need to sign our Data Use Agreement for access.\\n\\nYou will need to provide authentication to tools you are using to download files. (ex. wget, curl)\\n\\nAttempting to download protected files without meeting these criteria will yield a 401 Unauthorized response.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/accessing-data.md'}, page_content=\"If you've identified protected data you are interested in, please contact us so we can help get you access.\\n\\nIn the case of protected files, you'll need to provide authentication to your tool of choice. These will be your portal username and access token which can be found in the Access Credentials section of the site.\\n\\nThese can be provided to tools like wget and curl as follows:\\n\\n```console\\n\\nexample using wget\\n\\nwget --user=\\n\\nexample using curl\\n\\ncurl -u\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-an-account.md'}, page_content='sidebar_label: Create an account sidebar_position: 1\\n\\nOverview\\n\\nWhile some Sage features are open for public use, you\\'ll need an approved account to perform tasks such as:\\n\\nGet access to protected data.\\n\\nPublish apps to the ECR.\\n\\nSchedule app on nodes.\\n\\nIn this document, we\\'ll walk though creating an account.\\n\\nCreating an account\\n\\nClick on the Portal button in the upper right corner.\\n\\nClick on the Sign In button in the upper right corner.\\n\\nThis will take you to the Globus login page where you\\'ll need to provide your organization credentials. If you do not see your organization, please see the \"Didn\\'t find your organization?\" note at the bottom of the Globus login page.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-an-account.md'}, page_content=\"Finally, if this is your first time signing in, you'll need to choose a username which will complete your account creation.\\n\\nAt this point, our team will need to review and approve your account before you'll have permission to perform certain tasks. If you your account is not approved within 72 hours or you have special requirements, please Contact us so that we can help perform any account configuration.\\n\\nNext steps\\n\\nOnce your account is approved, you will have scheduling access and protected data browsing in the portal for nodes we've assigned to your account.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/create-an-account.md'}, page_content='For CLI tools and SSH access to nodes, please go to Portal → Your Account → Access Creds and follow the Update SSH Public Keys and Finish Setup for Node Access instructions.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content='sidebar_position: 5\\n\\nAccess Waggle sensors\\n\\nA Waggle sensor is an entity that produces measurements of a phenomenon and that helps users analyze what is happening in the environment. There are sensors already hosted by Waggle and also sensors that are being integrated into Waggle as a user-hosted sensor. A sensor does not necessarily mean a physical device, but can be a program producing measurements from data -- we call it software-defined sensor. Once those sensors become available in Waggle nodes edge applications running inside the nodes can pull measurements from the sensors to process them.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content='In general, Waggle sensors are desinged to be accessible from any edge applications running on the Waggle node that hosts the sensors, but can be limited their access to groups and personnel. For example, a pan-tilt-zoom featured camera may be only accessed from authorized applications in order to prevent other applications from operating the camera. Ideally, Waggle sensors can form and support the Waggle ecosystem where sensor measurements are integrated and used by edge applications for higher level computation and complex decision making.\\n\\nWaggle physical sensors'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content='The Waggle node is designed to accommodate sensors commonly used to support environmental science, but not limited to host other sensors. The currently supported sensors are,\\n\\nNOTE: not all Waggle nodes have the same set of sensors, and the sensor configuration depends on what to capture from the environment where the node is deployed'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content='BME680 temperature, humidity, pressure, and gas preview RG-15 rainfall preview ETS ML1-WS 20-16 kHz microphone recording sound XNV-8080R 5 MP camera with 92.1 degree horizontal and 67.2 degree vertical angle view XNV-8082R 6 MP camera with 114 degree horizontal and 62 degree vertical angle view XNF-8010RV 6 MP fisheye camera with 192 degree horizontal and vertical angle view XNV-8081Z 5 MP digital pan-tilt-rotate-zoom camera\\n\\nAny collaborators and user communities can bring up their sensors to Waggle node. The node can easily host sensor devices that support serial interface as well as network interface (e.g., http, rtsp, etc). Other currently supported user sensors include:'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content=\"Software-defined Radio: detecting raindrops and snow flakes\\n\\nRadiation detector: radiation detector\\n\\nLIDAR: distance of nearby objects\\n\\nMobotix: infrared camera\\n\\n[view more...]\\n\\nWaggle software-defined sensors\\n\\nSoftware-defined sensors are limitless as edge applications define them. You can start building your edge application that publishes outputs using PyWaggle's basic example that can become a software-defined sensor. Later, such outputs can be consumed by other edge applications to produce higher level information about the measurements. A few example of Waggle software-defined sensors are,\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content=\"Object Counter: env.count.OBJECT counts objects from an image, where OBJECT is the object name that is recognized\\n\\nCloud Coverage Estimator: env.coverage.cloud provides a percentage of cloud covered in an image\\n\\nAccess to Waggle sensors\\n\\nWaggle sensors are integrated into Waggle using the PyWaggle library. PyWaggle utilizes AMQP, the message publishing and subscribing mechanism, to support exchanging sensor measurements between device plugins and edge applications. An edge application can subscribe and process those measurements using PyWaggle's subscriber. The application then produces its output and publishes it as a measurement back to the system using PyWaggle publisher.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content=\"PyWaggle often provides edge applications direct access to physical sensors. For sensors that support realtime protocols like RTSP and RTP and others, PyWaggle exposes those protocols to edge applications, and it is up to the applications to process data using given protocol. For example, RTSP protocol can be handled by OpenCV's VideoCapture class inside an application. If any physical sensor device that requires a special interfacing to the device, an edge application that supports the interfacing need to run in order to publish sensor measurements to the system, and later those measurements are used by other edge applications.\\n\\nExample: sampling images from camera\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content='It is often important to sample images from cameras in the field to create initial dataset for a machine learning algorithm. The example describes how to access to a video stream from a camera sensor using PyWaggle.\\n\\nBring your own sensor to Waggle'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/access-waggle-sensors.md'}, page_content='Users may need to develop their own device plugin to expose the sensor to the system, or to publish measurement data from the sensor to the cloud. Unlike an edge application or software-defined sensors, device plugins communicating with a physical sensor may need special access, e.g. serial port, in order to talk to the sensor attached to Waggle node. Such device plugin may need to be verified by the Waggle team. Visit the Building your own Waggle device page for the guide to set up your Waggle device.\\n\\nTo integrate your sensor device into Waggle, head over to the Contact Us page'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/cloud-compute.md'}, page_content='sidebar_position: 6\\n\\nCloud compute & HPC on edge data\\n\\nWaggle provides a number of interfaces which other computing and HPC systems can build on top of. In this section, we explore some of the most common applications of Waggle.\\n\\nTriggering on data from the edge\\n\\nA common application is monitoring data from the edge and triggering actions when values exceed a threshold or an unusual event is detected.\\n\\nAs a getting started example, we demonstrate an outline of how this can be done in Waggle using the Sage data client.\\n\\n```python import sage_data_client import time'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/cloud-compute.md'}, page_content='while True: # query pressure data in recent 10 minute window df = sage_data_client.query( start=\"-10m\", filter={ \"name\": \"env.pressure\", \"sensor\": \"bme680\", } )\\n\\n# compute stddev for nodes\\' pressure data in window\\nstd = df.groupby(\"meta.vsn\").value.std()\\n\\n# find all pressure events exceeding an example threshold\\nevents = std[std > 8.0]\\n\\n# \"post\" vsn to alert system\\nfor vsn in events.index:\\n    print(f\"post {vsn} to alert system\")\\n\\ntime.sleep(60)'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/cloud-compute.md'}, page_content='```\\n\\nThe above code queries a 10 minute window of atmospheric pressure data every minute and \"posts\" alerts for nodes exceeding a predefined standard deviation threshold.\\n\\nThis example and more can be found in the Sage data client examples directory.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/4-publishing-to-ecr.md'}, page_content='sidebar_position: 4\\n\\nPart 4: Publishing to ECR\\n\\nNow that we\\'ve finished preparing our code and testing it, we\\'re almost ready to publish it to the Edge Code Repository!\\n\\nPreparing our app\\n\\nBefore publishing an app to the Edge Code Repository, we need to add a few packaging items to it.\\n\\nFirst, update the homepage in your sage.yaml to point to your app-tutorial Github repo and verify that it matches the following:\\n\\nyaml name: \"app-tutorial\" version: \"0.1.0\" description: \"My really amazing app!\" keywords: \"\" authors: \"Your name\" collaborators: \"\" funding: \"\" license: \"\" homepage: \"https://github.com/username/app-tutorial\" source: architectures: - \"linux/amd64\" - \"linux/arm64\"'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/4-publishing-to-ecr.md'}, page_content='Next, create an ecr-meta directory in your repo and populate it with the following text and media:\\n\\necr-science-description.md - Markdown with in depth description of the science being done here (1 page of text).\\n\\necr-icon.jpg - An icon for the project/work 512x512px.\\n\\necr-science-image.jpg - A science image for the project with a minimum size of 1920x1080px.\\n\\nOnce we\\'ve commited and pushed those files to your repo, we\\'re ready to publish our app!\\n\\nPublishing our app\\n\\nPlease visit the Edge Code Repository and complete the following steps:\\n\\nGo to \"Sign In\" and follow the instructions.\\n\\nGo to \"My Apps\".\\n\\nGo to \"Create app\" and follow the instructions.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/4-publishing-to-ecr.md'}, page_content='If everything is successful, your plugin will appeared and be marked as \"Built\".\\n\\nConclusion\\n\\nCongratulation! You\\'ve successfully written, tested and published an app to ECR!\\n\\nWe encourage you to check out other apps in the ECR and explore additional functionality provided by pywaggle.'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/1-intro-to-edge-apps.md'}, page_content='sidebar_position: 1\\n\\nPart 1: Intro to edge apps\\n\\nWhat are edge apps?\\n\\nEdge apps are programs which read data (ex. sensors, audio, video), process it and then publish information derived from that data.\\n\\nA basic example of an app is one which reads and publishes a value from a sensor every minute. A more complex example could publish the number of birds in a scene using a deep learning model.\\n\\nEdge apps are composed of code, dependencies and models which are packaged so they can be scheduled on Waggle nodes. At a high level, the typical app lifecycle is:\\n\\nExploring existing edge apps'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/1-intro-to-edge-apps.md'}, page_content=\"One of the major goals of Waggle is to provide the science community with a diverse catalog of edge apps to enable the sharing of new research. This catalog is maintained as part of the Edge Code Repository where you can find more background information and links to their source repos.\\n\\nWe encourage users to explore the ECR to get familiar with existing apps as well a references if you develop your own edge app.\\n\\nNext steps\\n\\nIf this sounds exciting and you'd like to write you own edge app, please continue to part 2!\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content=\"sidebar_position: 2\\n\\nPart 2: Creating an edge app\\n\\nIn part 1, we showed an overview of what edge apps are and how they fit into the Waggle ecosystem. Now, we'll dive right in and start writing our very own edge app!\\n\\nPrerequisites\\n\\nFor this part of the tutorial, we'll assume you are developing directly on a laptop or machine with a camera or webcam available. You should have some basic development experience in Python and with git for version control.\\n\\nDevelopment workflow\\n\\nIn the next few parts of this tutorial, we'll deep dive into the following app development workflow:\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content=\"First, data and model selection is where you scope the problem and identify a new or existing model for your application. This typically happens outside of our ecosystem.\\n\\nSecond, develop and test is where you begin to integrate your initial code with our ecosystem, test and finally build your application in ECR.\\n\\nFinally, deploy and iterate is where you schedule your application for deployment and look at the results.\\n\\nA driving example\\n\\nIn order to illustrate progress through each of these stages, we'll start with a concrete code example and iterate on it over the next few sections.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='In practice, lots of work goes into the data and model selection step. For now, we\\'ll assume that groundwork has already been done and we\\'ve settled on the following code snippit to start with.\\n\\n```python import numpy as np import cv2\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): # read example image from file image = cv2.imread(\"example.jpg\")\\n\\n# compute mean color\\nmean_color = compute_mean_color(image)\\n\\n# print mean color\\nprint(mean_color)\\n\\nif name == \"main\": main()'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content=\"```\\n\\nBootstrapping our app from a template\\n\\nWe'll start our by using a cookiecutter template to bootstrap our app.\\n\\nFirst, ensure the latest cookiecutter is installed:\\n\\nsh pip3 install --upgrade cookiecutter\\n\\nNow, run the following command:\\n\\nsh cookiecutter gh:waggle-sensor/cookiecutter-sage-app\\n\\nYou should be prompted to fill in the following fields:\\n\\ntxt [1/5] name (my-amazing-app-name): my-amazing-app-name [2/5] description (My really amazing app!): [3/5] author (My name): [4/5] version (0.1.0): [5/5] Select kind 1 - vision 2 - usbserial_sensor 3 - minimal 4 - tutorial <<< use 4 for tutorial Choose from [1/2/3/4] (1): 4\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content=\"If this succeeds, a new app-tutorial directory will be created with the following files:\\n\\nName Description main.py Main code requirements.txt Code dependencies Dockerfile App build instructions sage.yaml App metadata\\n\\nInstalling the dependencies\\n\\nThe first step in preparing our example for the edge is to install pywaggle in our local development environment.\\n\\npywaggle is our Python SDK which provides edge apps access to devices (ex. cameras and microphones) and messaging within a node.\\n\\nFor this tutorial, we'll install the latest version of the requirements included in the template:\\n\\nsh pip3 install --upgrade --requirement requirements.txt\\n\\nAccessing a camera\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='Now that we have pywaggle, the first change we\\'ll make is to use a camera as input rather than a static image file. We\\'ll use the following shapshot() function to take an RGB snapshot from the camera.\\n\\n```python import numpy as np\\n\\nfrom waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\\n\\ndef main(): # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n# compute mean color\\nmean_color = compute_mean_color(snapshot.data)\\n\\n# print mean color\\nprint(mean_color)\\n\\nif name == \"main\": main()'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content=\"```\\n\\nNow, we can try this out by running:\\n\\nsh python3 main.py\\n\\nYou should see output like:\\n\\ntxt [51.43575738 51.83611871 54.64226671]\\n\\nYou're exact numbers may differ as this is computed using your default camera.\\n\\nPublishing results\\n\\nThe next change we'll make is to publish our data to the Beehive Data Repository instead of just print it. This will allow it to be sent to a Beehive once it's scheduled on a node.\\n\\n```python import numpy as np\\n\\nfrom waggle.plugin import Plugin from waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='def main(): with Plugin() as plugin: # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n    # compute mean color\\n    mean_color = compute_mean_color(snapshot.data)\\n\\n    # publish mean color\\n    plugin.publish(\"color.mean.r\", mean_color[0], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.g\", mean_color[1], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.b\", mean_color[2], timestamp=snapshot.timestamp)\\n\\nif name == \"main\": main()'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content=\"```\\n\\nNow, we'll run this using:\\n\\nsh python3 main.py\\n\\nYou may notice something... there's no output! Usually, published data is sent to a beehive where it can be viewed later. However, because we're developing locally and have not configured a beehive, the data isn't going anywhere. In the next section, we'll see how we can tap into our published data.\\n\\nViewing run logs\\n\\nIn order to make developing and debugging apps easier, pywaggle can write out a log directory as follows:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nThis will create a new directory named test-run and will contain a file named data.ndjson which contains something like:\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='json {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":32.67932074652778} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":19.087491319444446} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":10.337491319444444}\\n\\nIf we run python3 main.py again, then we\\'ll see new data appended to that file:'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='json {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":32.67932074652778} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":19.087491319444446} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:04.619466000\",\"value\":10.337491319444444} {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":30.90709743923611} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":16.61302517361111} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:38:19.719910000\",\"value\":8.565154079861111}'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content=\"This provides a convenient way to understand the behavior of an app, particularly one with a more complicated flow.\\n\\nUploading a snapshot\\n\\nFinally, the last change we'll make is to upload our snapshots after publishing the mean color.\\n\\nWe'll upload every snapshot for demonstration purposes, but you wouldn't want to do this in a real app. Instead, you'd typically upload in response to detecting an event such as an anomalous object or loud noise.\\n\\n```python import numpy as np\\n\\nfrom waggle.plugin import Plugin from waggle.data.vision import Camera\\n\\ndef compute_mean_color(image): return np.mean(image, (0, 1)).astype(float)\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='def main(): with Plugin() as plugin: # open camera and take snapshot with Camera() as camera: snapshot = camera.snapshot()\\n\\n    # compute mean color\\n    mean_color = compute_mean_color(snapshot.data)\\n\\n    # publish mean color\\n    plugin.publish(\"color.mean.r\", mean_color[0], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.g\", mean_color[1], timestamp=snapshot.timestamp)\\n    plugin.publish(\"color.mean.b\", mean_color[2], timestamp=snapshot.timestamp)\\n\\n    # save and upload image\\n    snapshot.save(\"snapshot.jpg\")\\n    plugin.upload_file(\"snapshot.jpg\", timestamp=snapshot.timestamp)\\n\\nif name == \"main\": main()'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content=\"```\\n\\nLet's run our app again using:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nIf you take a look in the test-run/uploads directory, you should now see an image.\\n\\nUploads are added to the run log directory using the format nstimestamp-filename.\\n\\nYou should also see a corresponding item in the data.ndjson file.\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='json {\"meta\":{},\"name\":\"color.mean.r\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":29.601871744791666} {\"meta\":{},\"name\":\"color.mean.g\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":16.004838324652777} {\"meta\":{},\"name\":\"color.mean.b\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":8.217218967013888} {\"meta\":{\"filename\":\"snapshot.jpg\"},\"name\":\"upload\",\"timestamp\":\"2022-08-23T13:39:34.985679000\",\"value\":\"/Users/sean/dev/pw-example/test-run/uploads/1661279974985679000-snapshot.jpg\"}\\n\\nTools for analyzing run logs (Optional)'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/2-creating-an-edge-app.md'}, page_content='If you find yourself working with run logs frequently, we recommend the Sage data client which provides convenient functionality for loading and doing analysis on the data.ndjson file. See the \"Load results from file\" example for more info.\\n\\nNext steps\\n\\nCongratulations! You\\'ve finished preparing our example code for the edge!\\n\\nIn the part 3, we\\'ll look at how we can build and test our app on a real node!'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content=\"sidebar_position: 3\\n\\nPart 3: Testing an edge app\\n\\nIn the previous part, we took a code snippit and iterated on it until it was ready for the edge. By the end, we had basic camera access and publishing working!\\n\\nNow, we're ready to start testing it on a development node and describing our build steps.\\n\\nAccessing development nodes\\n\\nThe first thing we need to do is get access to a development node. Unfortunately, we are still developing the infrastructure to open this up to general users.\\n\\nFor now, please contact us to request access to a development node and we'll work with you to setup access.\\n\\nCreating a repo for our app\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content=\"Before connecting to our node, let's take a moment to organize our code into a repo we will later use on the node.\\n\\nGo ahead and create a new Github repo named app-tutorial and commit the files from previous part.\\n\\nBuilding our app\\n\\nNow that we've setup node access, ssh to the node then clone and cd into your app-tutorial repo:\\n\\nsh git clone https://github.com/username/app-tutorial cd app-tutorial\\n\\nThe first thing we'll do is build our app on the node:\\n\\nsh sudo pluginctl build .\\n\\nThis may take some time, but once it completes you should see something like:\"),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='```txt Sending build context to Docker daemon 59.39kB Step 1/6 : FROM waggle/plugin-base:1.1.1-base ... Step 2/6 : WORKDIR /app ... Step 3/6 : COPY requirements.txt . ... Step 4/6 : RUN pip3 install --no-cache-dir -r requirements.txt ... Step 5/6 : COPY . . ... Step 6/6 : ENTRYPOINT [\"python3\", \"main.py\"] ... b38bc0a208d0: Pushed 1101ffccd70a: Pushed latest: digest: sha256:7bee2a62fbcc9913f1c53bbdab79e973e70947618ffe4db90cae6a8f0ff6c8d7 size: 2407 Successfully built plugin\\n\\n10.31.81.1:5000/local/app-tutorial'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='```\\n\\nOnce we see Successfully built plugin, we can continue to running our app.\\n\\nRunning our app\\n\\nWhen we successfully built our app, the last line of output was 10.31.81.1:5000/local/app-tutorial. We will now use this reference to run our app.\\n\\nsh sudo pluginctl run --name app-tutorial 10.31.81.1:5000/local/app-tutorial\\n\\nWhen you run this, you\\'ll see that there\\'s a bug in the code:\\n\\n```sh Launched the plugin app-tutorial-1659971085 successfully INFO: 2022/08/08 15:04:45 run.go:63: Plugin is in \"Pending\" state. Waiting...'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='[ WARN:0@0.032] global /io/opencv/modules/videoio/src/cap_v4l.cpp (902) open VIDEOIO(V4L2:/dev/video0): can\\'t open camera by index Traceback (most recent call last): File \"main.py\", line 32, in\\n\\nThis was caused by the fact that most nodes have multiple cameras, so we need to be more specific about which camera to use.\\n\\nTo address this, we\\'ll change the following line in main.py from:\\n\\npython with Camera() as camera:\\n\\nto:\\n\\npython with Camera(\"left\") as camera:\\n\\nThe specific camera name will depend on your specific node. If you are having problems accessing a camera, please contact us for more details.\\n\\nAfter rebuilding and running this again, the plugin should run and exit cleanly:'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='```txt Launched the plugin app-tutorial-1659971085 successfully INFO: 2022/08/08 15:04:45 run.go:63: Plugin is in \"Pending\" state. Waiting...\\n\\nshould exit cleanly with no output'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='```\\n\\nNow that we know this works, please commit and push the change to the repo from your machine.\\n\\nFinally, if you are rebuilding and running code frequently, you can combine the build and run into a single step as follows:\\n\\nsh sudo pluginctl run --name app-tutorial $(sudo pluginctl build .)\\n\\nViewing our output\\n\\nWe\\'ll close this part, by looking at the data we just published. To do this, we\\'ll query the Beehive Data Repository:\\n\\nsh curl -s \\'Content-Type: application/json\\' https://data.sagecontinuum.org/api/v1/query -d \\' { \"start\": \"-5m\", \"filter\": { \"task\": \"app-tutorial\" } }\\'\\n\\nYou should see some results like:'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='json {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.b\",\"value\":133.61671793619792,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.g\",\"value\":136.46639404296874,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}}'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content='{\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"color.mean.r\",\"value\":134.48696818033855,\"meta\":{\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}} {\"timestamp\":\"2022-08-08T15:04:48.820981933Z\",\"name\":\"upload\",\"value\":\"https://storage.sagecontinuum.org/api/v1/data/Pluginctl/sage-app-tutorial-app-tutorial/000048b02d15bdc2/1659971088820981933-snapshot.jpg\",\"meta\":{\"filename\":\"snapshot.jpg\",\"host\":\"000048b02d15bdc2.ws-nxcore\",\"job\":\"Pluginctl\",\"node\":\"000048b02d15bdc2\",\"plugin\":\"app-tutorial\",\"task\":\"app-tutorial\",\"vsn\":\"W02F\"}}'),\n",
       " Document(metadata={'source': 'sage-website/docs/tutorials/edge-apps/3-testing-an-edge-app.md'}, page_content=\"These are exactly the mean color values we computed and published!\\n\\nThis is intended to be a quick preview of how to access data to help get you started. If you are interested, we cover this topic in much depth here.\\n\\nNext steps\\n\\nNow we've been able to build, run and even fix a bug in our code! In part 4, we'll see how to publish a first release of our code to the Edge Code Repository!\"),\n",
       " Document(metadata={'source': 'sage-website/docs/events/past/hackathon-2023.md'}, page_content='sidebar_label: August 2023 Hackathon\\n\\nAugust 2023 Sage Hackathon\\n\\nWe are hosting a Hackathon for Sage in late August ~~with preliminary dates August 30-31~~!\\n\\nUpdate: Based on user feedback, we will run afternoon sessions on both August 30 and 31 starting at 1pm CST. We will email out an agenda and invite to Slack to participants who have signed up a few days before the event.\\n\\nThe goal of the Hackathon is to set aside a few hours dedicated to working through user applications, code and science examples in depth. If this interests you, please fill out the signup form as soon as possible!'),\n",
       " Document(metadata={'source': 'sage-website/docs/events/past/hackathon-2023.md'}, page_content='Prior to the Hackathon, we request that you do a few things: 1. Read the Sage Overview in the Sage docs to understand what Sage is and how it might connect to your work. 2. Start assembling the people on your team likely to participate. 3. Ensure they have accounts in the Sage Portal. 4. Start working through the Edge apps tutorial in the Sage docs.\\n\\nDuring the Hackathon, we will review how to use the portal and parts of the Edge app tutorial. However, already having done some preliminary work will allow more time for our team to provide support for your unique application.'),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content='Sage Data Client\\n\\nThis is the official Sage Python data API client. Its main goal is to make writing queries and working with the results easy. It does this by:\\n\\nProviding a simple query function which talks to the data API.\\n\\nProviding the results in an easy to use Pandas data frame.\\n\\nInstallation\\n\\nSage Data Client can be installed with pip using:\\n\\nsh pip3 install sage-data-client\\n\\nIf you prefer to install this package into a Python virtual environment or are unable to install it system wide, you can use the venv module as follows:\\n\\n```sh\\n\\n1. Create a new virtual environment called my-venv.\\n\\npython3 -m venv my-venv\\n\\n2. Activate the virtual environment\\n\\nsource my-venv/bin/activate'),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content='3. Install sage data client in the virtual environment\\n\\npip3 install sage-data-client'),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content='```\\n\\nNote: If you are using Linux, you may need to install the python3-venv package which is outside of the scope of this document.\\n\\nNote: You will need to activate this virtual environment when opening a new terminal before running any Python scripts using Sage Data Client.\\n\\nUsage Examples\\n\\nQuery API\\n\\n```python import sage_data_client\\n\\nquery and load data into pandas data frame\\n\\ndf = sage_data_client.query( start=\"-1h\", filter={ \"name\": \"env.temperature\", } )\\n\\nprint results in data frame\\n\\nprint(df)\\n\\nmeta columns are expanded into meta.fieldname. for example, here we print the unique nodes\\n\\nprint(df[\"meta.vsn\"].unique())\\n\\nprint stats of the temperature data grouped by node + sensor.'),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content='print(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"]))'),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content='```\\n\\n```python import sage_data_client\\n\\nquery and load data into pandas data frame\\n\\ndf = sage_data_client.query( start=\"-1h\", filter={ \"name\": \"env.raingauge.*\", } )\\n\\nprint number of results of each name\\n\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size()) ```\\n\\nLoad results from file\\n\\nIf we have saved the results of a query to a file data.json, we can also load using the load function as follows:\\n\\n```python import sage_data_client\\n\\nload results from local file\\n\\ndf = sage_data_client.load(\"data.json\")\\n\\nprint number of results of each name\\n\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size())'),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content=\"```\\n\\nIntegration with Notebooks\\n\\nSince we leverage the fantastic work provided by the Pandas library, performing things like looking at dataframes or creating plots is easy.\\n\\nA basic example of querying and plotting data can be found here.\\n\\nAdditional Examples\\n\\nAdditional code examples can be found in the examples directory.\\n\\nIf you're interested in contributing your own examples, feel free to add them to examples/contrib and open a PR!\\n\\nReference\\n\\nThe query function accepts the following arguments:\\n\\nstart. Absolute or relative start timestamp. (required)\\n\\nend. Absolute or relative end timestamp.\"),\n",
       " Document(metadata={'source': 'sage-data-client/README.md'}, page_content='head. Limit results to head earliest values per series. (Only one of head or tail can be provided.)\\n\\ntail. Limit results to tail latest values per series. (Only one of head or tail can be provided.)\\n\\nfilter. Key-value patterns to filter data on.'),\n",
       " Document(metadata={'source': 'pywaggle/README.md'}, page_content='Waggle Python Module\\n\\npywaggle is a Python module for implementing Waggle plugins and system services.\\n\\nInstallation Guides\\n\\nMost users getting started with pywaggle will want to install latest version with all optional dependencies using:\\n\\nsh pip install -U pywaggle[all]\\n\\nAdvanced users can install specific subsets of functionality using the following extras flags:\\n\\naudio - Audio and microphone support for plugins.\\n\\nvision - Image, video and camera support for plugins.\\n\\n```sh\\n\\ninstall only core plugin features\\n\\npip install pywaggle\\n\\ninstall only audio features\\n\\npip install pywaggle[audio]\\n\\ninstall only vision features\\n\\npip install pywaggle[vision]\\n\\ninstall both audio and vision features'),\n",
       " Document(metadata={'source': 'pywaggle/README.md'}, page_content='pip install pywaggle[audio,vision]'),\n",
       " Document(metadata={'source': 'pywaggle/README.md'}, page_content='```\\n\\nUsage Guides\\n\\nWriting a plugin'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content=\"Writing a plugin\\n\\nAlthough this doc is still useful as a reference for more advanced pywaggle features, we have migrated the getting started portions to their own tutorial on our central docs site. The pywaggle docs will become more of a reference guide in the near future.\\n\\nIn this guide, we'll walk through writing a basic plugin and exploring some of the functionality provided by pywaggle.\\n\\nThat being said, we do want to emphasize that pywaggle is designed to make it easy to interface existing Python code with the Waggle stack. To a first approximation, pywaggle aims to augment print statements with publish statements.\"),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content=\"If you'd like to jump ahead to real code, please see the following examples:\\n\\nMinimal Numpy Example\\n\\nHello World ML Example\\n\\nThese repos can be used as starter templates for your own plugin development.\\n\\nWhat is a plugin?\\n\\nA plugin is a self-contained program which typically reads sensors, audio or video data, does some processing and finally publishes results derived from that data.\\n\\nThe most basic example of a plugin is one which simply reads and publishes a value from a sensor. A more complex plugin could publish the number of cars seen in a video stream using a deep learning model.\"),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='Plugins fit into the wider Waggle infrastructure by being tracked in the Edge Code Repository, deployed to nodes and publishing data to our data repository.\\n\\nWriting \"Hello World\" plugin code\\n\\nNote: In this guide, we currently only cover writing the plugin __code__. We still are updating the docs on building and running a plugin inside Virtual Waggle and natively. As such, this guide will help you structure and run your code locally but not against the rest of platform.\\n\\nWe\\'ll walk through writing a \"hello world\" plugin which simply publishes a increasing counter as measurement hello.world.counter every second.\\n\\n1. Install pywaggle\\n\\nFirst, we\\'ll install the latest version of pywaggle.'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content=\"sh pip install -U pywaggle[all]\\n\\nThis will install the core pywaggle modules along with the extra developer modules.\\n\\n2. Create empty plugin directory\\n\\nCreate a new empty directory which we'll write our plugin in.\\n\\nsh mkdir plugin-hello-world cd plugin-hello-world\\n\\n3. Create a requirements.txt dependency file\\n\\nCreate a new requirements.txt file and add this following:\\n\\nsh pywaggle[all]\\n\\nThis will be used when building our plugin to ensure all dependencies are available. Right now, it only contains pywaggle but you can add your own custom dependencies here.\\n\\n4. Create main.py file\\n\\nCreate a new file called main.py with the following code:\"),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```python from waggle.plugin import Plugin import time\\n\\nwith Plugin() as plugin: for i in range(10): print(\"publishing value\", i) plugin.publish(\"hello.world.value\", i) time.sleep(1)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content=\"```\\n\\n5. Run plugin\\n\\nThe plugin can now be run using:\\n\\nsh python3 main.py\\n\\nYou should see the output:\\n\\ntxt publishing value 0 publishing value 1 publishing value 2 ...\\n\\n6. Access run logs (Optional)\\n\\nAs you're developing and debugging a plugin, it can be very helpful to see the run log of published messages and uploads.\\n\\nYou can enable this by defining the PYWAGGLE_LOG_DIR=path/to/run/logs environment variable as follows:\\n\\nsh export PYWAGGLE_LOG_DIR=test-run python3 main.py\\n\\nNow, you should see a new directory named test-run with the following contents:\\n\\ntxt test-run/ data.ndjson <- measurements published uploads/ <- timestamped files uploaded nstimestamp1-filename1 nstimestamp2-filename2 ...\"),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content=\"The data.ndjson file is a newline delimited JSON file containing raw measurement messages.\\n\\nHere's an example from a more complete plugin:\"),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='json {\"name\":\"env.temperature\",\"timestamp\":\"2022-08-23T13:27:10.562104000\",\"meta\":{\"sensor\":\"bme280\"},\"value\":23.0} {\"name\":\"upload\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{\"camera\":\"top\",\"filename\":\"test.png.webp\"},\"value\":\"/Users/sean/git/pywaggle-log-dir-example/testrun/uploads/1661279233561615000-test.png.webp\"} {\"name\":\"image.cats\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{},\"value\":0} {\"name\":\"image.birds\",\"timestamp\":\"2022-08-23T13:27:13.562104000\",\"meta\":{\"camera\":\"left\"},\"value\":8} {\"name\":\"timeit.inference\",\"timestamp\":\"2022-08-23T13:27:11.562104000\",\"meta\":{},\"value\":1005408000}'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='The contents of the log directory operates in an append mode, so you may safely run the plugin multiple times without losing previous data.\\n\\nAdding \"Hello World\" plugin packaging info\\n\\nNow that we have the basic plugin code working, let\\'s prepare this code to be submitted to the Edge Code Repository.\\n\\n1. Create a Github repo for plugin\\n\\nFirst, we need to create a Github repo for our plugin. Go ahead a create one called \"plugin-hello-world\" and add the contents from our plugin-hello-world directory.\\n\\nFor the purposes of this example, we\\'ll assume our plugin URL is https://github.com/username/plugin-hello-world.\\n\\n2. Add Dockerfile'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='Create and add a new file called Dockerfile with the following contents:\\n\\ndockerfile FROM waggle/plugin-base:1.1.1-ml COPY requirements.txt /app/ RUN pip3 install --no-cache-dir --upgrade -r /app/requirements.txt COPY . /app/ WORKDIR /app ENTRYPOINT [\"python3\", \"/app/main.py\"]\\n\\nThis file defines what base image should be used by a plugin and how it should be run. In more complex examples, additional dependencies may be specified here.\\n\\n3. Add sage.yaml\\n\\nCreate and add a new file called sage.yaml with the following contents:'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='yaml name: \"hello-world\" description: \"My hello world plugin\" keywords: \"hello, testing\" authors: \"Your Name <your.email@somewhere.org>, A Coworker <your.coworker@somewhere.org>\" collaborators: \"Helpful Collaborator <our.collaborator@otherplace.edu>\" funding: \"\" license: \"\" homepage: \"https://github.com/username/plugin-hello-world/blob/main/README.md\" source: architectures: - \"linux/amd64\" - \"linux/arm64\"\\n\\nThis file contains metadata about what your plugin is called and what it\\'s supposed to do. It is used by the Edge Code Repository when submitting plugins.\\n\\n4. Add ECR media\\n\\nCreate a ecr-meta directory and populate it with the following text and media:'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='ecr-science-description.md - Markdown with in depth description of the science being done here (1 page of text).\\n\\necr-icon.jpg - An icon for the project/work 512x512px.\\n\\necr-science-image.jpg - A science image for the project with a minimum size of 1920x1080px.\\n\\nBeyond the basics\\n\\nMore about the publish function\\n\\nIn the previous example, we saw the most basic usage of the publish function. Now, we want to talk about a couple additional features available to you.\\n\\nFirst, metadata can be added to measurements to provide context to how a measurement was created. For example, suppose we had a left and a right facing camera on a node and wanted to track which one was used.'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='python plugin.publish(\"my.sensor.name\", 123, meta={\"camera\": \"left\"})\\n\\nThis will bind the meta data together with the measurement and will be available throughout the rest of the data pipeline.\\n\\nSecond, you can explicitly provide a timestamp for situations where you have more information on when a measurement was taken. For example:\\n\\npython plugin.publish(\"my.sensor.name\", 123, timestamp=my_timestamp_in_ns)\\n\\nNote: Timestamps are expected to be in nanoseconds since epoch. In Python 3.7+, this is available through the standard time.time_ns() function.\\n\\nSubscribing to other measurements'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='Plugins can subscribe to measurements published by other plugins running on the same node. This allows users to leverage existing work or compose a larger application of multiple independent components.\\n\\nThe followng basic example simply waits for measurements named \"my.sensor.name\" and prints the value it received.\\n\\n```python from waggle.plugin import Plugin from random import random\\n\\nwith Plugin() as plugin: plugin.subscribe(\"my.sensor.name\")\\n\\nwhile True:\\n    msg = plugin.get()\\n    print(\"Another plugin published my.sensor.name value\", msg.value)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nIn the case you need multiple multiple measurements, you can simply use:\\n\\npython plugin.subscribe(\"my.sensor.name1\", \"my.sensor.name2\", \"my.sensor.name3\")\\n\\nTo differentiate the results, you can use the message name:\\n\\npython msg = plugin.get() if msg.name == \"my.sensor.name1\": # do something elif msg.name == \"my.sensor.name2\": # do something else\\n\\nIn more complex examples, the full message metadata can also be used to differentiate behavior:\\n\\n```python plugin.subscribe(\"env.temperature\")\\n\\nwhile True: msg = plugin.get() if msg.meta.get(\"sensor\") == \"bme280\": # do something elif msg.meta.get(\"sensor\") == \"bme680\": # do something else'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nMore about the subscribe function\\n\\nThe subscribe function can match two kinds of wildcard patterns. Measurement names are treated as \"segments\" broken up by a dot and we can match various segments using the star and hash operators.\\n\\nFirst, we can match a single wildcard segment using the \"my.sensor.*\" pattern. This will match all measurements with exactly three segments and whose first segment is \"my\", second segment is \"sensor\" and third segment can be anything.\\n\\nSecond, we can match zero or more segments using the \"my.#\" pattern. This will match all measurements whose first segment is \"my\" like \"my.sensor\", \"my.sensor.name\" or \"my.sensor.name.is.cool\".'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='Working with camera and microphone data\\n\\npywaggle provides a simple abstraction to cameras and microphones.\\n\\nAccessing a video stream\\n\\n```python from waggle.plugin import Plugin from waggle.data.vision import Camera import time\\n\\nuse case 1: take a snapshot and process\\n\\nwith Plugin() as plugin: sample = Camera().snapshot() # do processing result = process(sample.data) plugin.publish(\"my.measurement\", result, timestampe=sample.timestamp)\\n\\nuser case 2: process camera frames'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='with Plugin() as plugin, Camera() as camera: # process samples from video stream for sample in camera.stream(): count = count_cars_in_image(sample.data) if count > 10: sample.save(\"cars.jpg\") plugin.upload_file(\"cars.jpg\")'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nThe camera.snapshot() function returns a camera frame. The function provides a convenient way to capture a frame and process it.\\n\\nThe camera.stream() function yields a sequence of ImageSample with the following properties:\\n\\nsample.data. captured image\\'s numpy data array.\\n\\nsample.timestamp. captured image\\'s nanosecond timestamp.\\n\\nAdditionally, the Camera class accepts URLs and video files as input. For example:\\n\\n```python\\n\\nopen an mjpeg-over-http stream\\n\\ncamera = Camera(\"http://camera-server/profile1.mjpeg\")\\n\\nopen an rtsp stream\\n\\ncamera = Camera(\"rtsp://camera-server/v0.mp4\")\\n\\nopen a local file using file:// url\\n\\ncamera = Camera(\"file://path/to/my_cool_video.mp4\")'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='open a camera by device id (when plugin runs on a node)\\n\\ncamera = Camera(\"bottom_camera\")'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nCamera buffering and use cases\\n\\n```\\n\\nQ1. How often do you need camera frames? A1. (As many as possible) ---> Refer to use case 1 A2. (Occasionally) ---> Go to Q2\\n\\nQ2. How sensitive is your application to a short delay when capturing an image? A1. (Very sensitive) ---> Refer to use case 1 A2. (A second is ok) ---> Refer to use case 2'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nThe Camera class wrapped in the Python with statement runs a background thread to keep up with the camera stream. This allows users to get the latest frame whenever .stream() or .snapshot() are called. However, this may be uncessary when users want to close the stream after grabbing a frame or the Camera class is used with a file, not a stream.\\n\\nTherefore, it is highly recommended to use the Camera class with the Python with statement when users want to process consequtive frames.\\n\\nUse case 1\\n\\n```python from time import sleep from waggle.data.vision import Camera'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='with Camera() as camera: former_frame = camera.snapshot() sleep(5) # the current_frame gets the latest frame current_frame = camera.snapshot() calculate_motion(current_frame, former_frame)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nFor simple grab-and-go use cases, users use the Camera class without the with statement to avoid the background process and its resource consumption.\\n\\nUse case 2 ```python from time import sleep from waggle.data.vision import Camera\\n\\nThe Camera class closes the stream after obtaining\\n\\na frame\\n\\ncamera = Camera() former_frame = camera.snapshot() sleep(5)\\n\\nThe Camera class opens the stream and grabs a frame\\n\\ncurrent_frame = camera.snapshot() calculate_motion(current_frame, former_frame)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nRecording video data\\n\\n```python from waggle.data.vision import Camera\\n\\ncamera = Camera()\\n\\nrecord a 30-second video from the camera\\n\\nvideo = camera.record(duration=30) with video: for frame in video: process(frame.data) ```\\n\\nThe Camera class allows users to record a video from camera and store the clip into a file. Because it relies on ffmpeg user code and its container (if in a Docker container) must have ffmpeg installed. You may install it as follow,\\n\\n```bash\\n\\nfor ubuntu\\n\\napt-get update && apt-get install -y ffmpeg'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nAlso, the .record() function may NOT be used with Python with statement for USB cameras.\\n\\n```python from waggle.data.vision import Camera\\n\\nthe camera is a USB camera\\n\\ndevice = \"/dev/camera0\" with Camera(device) as camera: # this raises an exception as the camera stream is already open by the with statement video = camera.record(duration=30)\\n\\nUSB cameras can be used as below\\n\\nvideo = Camera(device).record(duration=30)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nRecording audio data\\n\\n```python from waggle.plugin import Plugin from waggle.data.audio import Microphone import time\\n\\nwith Plugin() as plugin, Microphone() as microphone: # record and upload a 10s sample periodically while True: sample = microphone.record(10) sample.save(\"sample.ogg\") plugin.upload_file(\"sample.ogg\") time.sleep(300)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nSimilar to ImageSample, AudioSample provide the following properties:\\n\\nsample.data. captured audio\\'s numpy data array.\\n\\nsample.timestamp. captured audio\\'s nanosecond timestamp.\\n\\nsample.samplerate. captured audio\\'s sample rate.\\n\\nAudioFolder and ImageFolder for testing\\n\\nWe provide a couple simple classes to provide audio and image data from a directory for testing.\\n\\nIn the following example, we assume we have directories:\\n\\ntxt audio_data/ example1.ogg example2.ogg ... image_data/ img1.png cat-7.png example10.jpg ...\\n\\nWe can load up all the audio files in the audio_data folder for testing as follows:\\n\\n```python from waggle.data.audio import AudioFolder\\n\\ndataset = AudioFolder(\"audio_data\")'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='for sample in dataset: process_data(sample.data)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nSimilarly, we can do something similar for all the image files in the image_data folder.\\n\\n```python from waggle.data.vision import ImageFolder\\n\\ndataset = ImageFolder(\"image_data\")\\n\\nfor sample in dataset: process_image_frame(sample.data) ```\\n\\nAdvanced: Choosing a color format\\n\\nBy default, the waggle.data.vision submodule uses an RGB color format. If you need more control, you can specify one of RGB or BGR to both the Camera and ImageFolder objects as follows:\\n\\n```python from waggle.data.vision import Camera, ImageFolder, RGB, BGR\\n\\nuse BGR data instead of RGB\\n\\ncamera = Camera(format=BGR)\\n\\nuse BGR data instead of RGB\\n\\ncamera = ImageFolder(format=BGR)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nAdvanced: Timing a block\\n\\nThe Plugin class provides a simple utility for timing how long a block of code takes.\\n\\nThe following example shows how we can instrument our code using a typical AI/ML example plugin.\\n\\n```python from waggle.plugin import Plugin\\n\\nwith Plugin() as plugin: # measures duration of input block and publishes to plugin.duration.input with plugin.timeit(\"plugin.duration.input\"): get_inputs(...)\\n\\n# measures duration of inference block and publishes to plugin.duration.inference\\nwith plugin.timeit(\"plugin.duration.inference\"):\\n    do_inference(...)\\n\\npublish_results(...)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nIn the example above, the duration of the input and inference steps are measured and then the plugin publishes the duration in nanoseconds to the name provided to plugin.timeit as each block finishes.\\n\\nSeeing the internal details\\n\\nIf we run the basic example, the only thing we\\'ll see is the message \"publishing a value!\" every second. If you need to see more details, pywaggle is designed to easily interface with Python\\'s standard logging module. To enable debug logging, simply make the following additions:\\n\\n```python from waggle.plugin import Plugin from time import sleep\\n\\n1. import standard logging module\\n\\nimport logging\\n\\n2. enable debug logging\\n\\nlogging.basicConfig(level=logging.DEBUG)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='with Plugin() as plugin: while True: sleep(1) print(\"publishing a value!\") plugin.publish(\"my.sensor.name\", 123)'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content='```\\n\\nYou should see a lot of information like:\\n\\ntext DEBUG:waggle.plugin:starting plugin worker thread DEBUG:waggle.plugin:connecting to rabbitmq broker at rabbitmq:5672 with username \"plugin\" DEBUG:waggle.plugin:rabbitmq connection error: [Errno 8] nodename nor servname provided, or not known publishing a value! DEBUG:waggle.plugin:adding message to outgoing queue: Message(name=\\'my.sensor.name\\', value=123, timestamp=1619628240863845000, meta={}) DEBUG:waggle.plugin:connecting to rabbitmq broker at rabbitmq:5672 with username \"plugin\" DEBUG:waggle.plugin:rabbitmq connection error: [Errno 8] nodename nor servname provided, or not known publishing a value!\\n\\nThe most important lines are:'),\n",
       " Document(metadata={'source': 'pywaggle/docs/writing-a-plugin.md'}, page_content=\"text publishing a value! DEBUG:waggle.plugin:adding message to outgoing queue: Message(name='my.sensor.name', value=123, timestamp=1619628240863845000, meta={})\\n\\nThese are telling us that our messages are being queued up in an outgoing queue to be shipped.\\n\\nYou'll also see a number of messages related to rabbitmq.\\n\\nThese are simply indicating the our plugin is waiting to connect to the Waggle ecosystem. This is normal when testing a standalone plugin without the rest of the Waggle stack. Plugins will simply queue up measurements in-memory until they exit.\"),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/README.md'}, page_content='Waggle Module\\n\\nThis page gives an overview of the core functionality provided by this module.\\n\\nPlugin Submodule\\n\\nProvides functionality for publishing sensor data and for processing messages.\\n\\nProtocol Submodule\\n\\nProvides functionality for packing and unpacking sensor and messaging data.'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='import unittest\\nimport sage_data_client\\nfrom io import BytesIO\\nfrom datetime import datetime, timedelta\\nimport pandas as pd'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='class TestQuery(unittest.TestCase):\\n    def assertValueResponse(self, df):\\n        self.assertIn(\"name\", df.columns)\\n        df.name.str\\n        self.assertIn(\"timestamp\", df.columns)\\n        df.timestamp.dt\\n        self.assertIn(\"value\", df.columns)\\n\\n    def test_empty_response(self):\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"-2000d\",\\n                filter={\\n                    \"name\": \"should.not.every.exist.XYZ\",\\n                },\\n            )\\n        )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='def test_check_one_of_head_or_tail(self):\\n        with self.assertRaises(ValueError):\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00\",\\n                end=\"2021-01-01T10:31:00\",\\n                head=3,\\n                tail=3,\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n\\n    def test_queries(self):\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00\",\\n                end=\"2021-01-01T10:31:00\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00Z\",\\n                end=\"2021-01-01T10:31:00Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00.123Z\",\\n                end=\"2021-01-01T10:31:00.123Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01T10:30:00.123456Z\",\\n                end=\"2021-01-01T10:31:00.123456Z\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"2021-01-01 10:30:00\",\\n                end=\"2021-01-01 10:31:00\",\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='self.assertValueResponse(\\n            sage_data_client.query(\\n                start=datetime(2021, 1, 1, 10, 31, 0),\\n                end=datetime(2021, 1, 1, 10, 32, 0),\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=pd.to_datetime(\"2021-01-01 10:30:00\"),\\n                end=pd.to_datetime(\"2021-01-01 10:31:00\"),\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='for dt in [\"-30s\", \"-3m\", \"-3min\", \"-1d\", \"-1w\"]:\\n            self.assertValueResponse(\\n                sage_data_client.query(\\n                    start=dt,\\n                    tail=1,\\n                    filter={\\n                        \"name\": \"env.temperature\",\\n                    },\\n                )\\n            )\\n\\n        self.assertValueResponse(\\n            sage_data_client.query(\\n                start=\"-4h\",\\n                end=\"-2h\",\\n                tail=1,\\n                filter={\\n                    \"name\": \"env.temperature\",\\n                },\\n            )\\n        )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='for dt in [\\n            timedelta(seconds=-30),\\n            timedelta(minutes=-1),\\n            timedelta(hours=-1),\\n            timedelta(days=-1),\\n        ]:\\n            self.assertValueResponse(\\n                sage_data_client.query(\\n                    start=dt,\\n                    tail=1,\\n                    filter={\\n                        \"name\": \"env.temperature\",\\n                    },\\n                )\\n            )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='def test_load(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2021-10-14T21:42:21.149425156Z\",\"name\":\"env.temperature\",\"value\":21.74,\"meta\":{\"host\":\"0000dca632a3069f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31f\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"env.temperature\",\"value\":26.09,\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='{\"timestamp\":\"2021-10-14T21:42:19.087014343Z\",\"name\":\"env.temperature\",\"value\":28.14,\"meta\":{\"host\":\"0000dca632a3074d.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc73\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W024\"}}\\n{\"timestamp\":\"2021-10-14T21:42:23.475857326Z\",\"name\":\"env.temperature\",\"value\":28.16,\"meta\":{\"host\":\"0000dca632a3076b.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc6d\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01B\"}}'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='{\"timestamp\":\"2021-10-14T21:42:34.995766556Z\",\"name\":\"env.temperature\",\"value\":33.27,\"meta\":{\"host\":\"0000dca632a3078f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bdc7\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W020\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.803472584Z\",\"name\":\"env.temperature\",\"value\":9.8,\"meta\":{\"host\":\"0000dca632a30792.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc42\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01D\"}}'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='{\"timestamp\":\"2021-10-14T21:42:30.9261079Z\",\"name\":\"env.temperature\",\"value\":25.63,\"meta\":{\"host\":\"0000dca632a307b6.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc8c\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W039\"}}\\n{\"timestamp\":\"2021-10-14T21:42:24.228048661Z\",\"name\":\"env.temperature\",\"value\":23.96,\"meta\":{\"host\":\"0000dca632a307bf.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15bc7d\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W028\"}}'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='{\"timestamp\":\"2021-10-14T21:42:13.914329997Z\",\"name\":\"env.temperature\",\"value\":21.84,\"meta\":{\"host\":\"0000dca632a307e6.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d05a1c2\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W02C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:17.300924641Z\",\"name\":\"env.temperature\",\"value\":30.12,\"meta\":{\"host\":\"0000dca632a307fb.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c328\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W016\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertEqual(len(df), 10)\\n        self.assertValueResponse(df)'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='def test_load_small_numbers(self):\\n        sample_data = BytesIO('),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='b\"\"\"{\"timestamp\":\"2023-09-25T19:26:26.18944512Z\",\"name\":\"sensor_body_temperature\",\"value\":-655230609742226300000,\"meta\":{\"applicationId\":\"ac81e18b-1925-47f9-839a-27d999a8af55\",\"applicationName\":\"ATMOS test app\",\"devAddr\":\"00f06b4b\",\"devEui\":\"98208e0000032a15\",\"deviceName\":\"MFR Node\",\"deviceProfileId\":\"cf2aec2f-03e1-4a60-a32c-0faeef5730d8\",\"deviceProfileName\":\"MFR node\",\"host\":\"0000e45f014caee8.ws-rpi\",\"node\":\"000048b02d15bc8c\",\"plugin\":\"registry.sagecontinuum.org/flozano/lorawan-listener:0.0.6\",\"task\":\"lorawan-listener\",\"tenantId\":\"52f14cd4-c6f1-4fbd-8f87-4025e1d49242\",\"tenantName\":\"ChirpStack\",\"vsn\":\"W039\",\"zone\":\"shield\"}}\\n\"\"\"\\n        )'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='df = sage_data_client.load(sample_data)\\n        self.assertValueResponse(df)\\n        self.assertEqual(len(df), 1)'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='def test_load_empty_file(self):\\n        df = sage_data_client.load(\"tests/test-empty.ndjson\")\\n        self.assertEqual(len(df), 0)\\n        self.assertValueResponse(df)\\n\\n        df = sage_data_client.load(\"tests/test-empty.ndjson.gz\")\\n        self.assertEqual(len(df), 0)\\n        self.assertValueResponse(df)'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='def test_load_compressed_file(self):\\n        df1 = sage_data_client.load(\"tests/test-data.ndjson\")\\n        df2 = sage_data_client.load(\"tests/test-data.ndjson.gz\")\\n        self.assertEqual(len(df1), 1611)\\n        self.assertEqual(len(df2), 1611)\\n        # NOTE In Pandas Nones and NaNs do not equal themselves, so will fill them to make df1 == df2 work.\\n        self.assertTrue((df1.fillna(\"\") == df2.fillna(\"\")).all().all())'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='def test_mixed_types(self):\\n        sample_data = BytesIO(\\n            b\"\"\"{\"timestamp\":\"2021-10-14T21:42:21.149425156Z\",\"name\":\"test\",\"value\":21.74,\"meta\":{\"host\":\"0000dca632a3069f.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31f\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-rpi\",\"vsn\":\"W01C\"}}\\n{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"test\",\"value\":\"26.09\",\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='{\"timestamp\":\"2021-10-14T21:42:09.201150729Z\",\"name\":\"test\",\"value\":123,\"meta\":{\"host\":\"0000dca632a306d8.ws-rpi\",\"job\":\"sage\",\"node\":\"000048b02d15c31a\",\"plugin\":\"plugin-iio:0.4.5\",\"sensor\":\"bme680\",\"task\":\"iio-shield\",\"vsn\":\"W01A\"}}\\n\"\"\"\\n        )\\n        df = sage_data_client.load(sample_data)\\n        self.assertEqual(len(df), 3)\\n        self.assertValueResponse(df)\\n        self.assertAlmostEqual(df.iloc[0].value, 21.74)\\n        self.assertEqual(df.iloc[1].value, \"26.09\")\\n        self.assertEqual(df.iloc[2].value, 123)'),\n",
       " Document(metadata={'source': 'sage-data-client/tests/test_query.py'}, page_content='if __name__ == \"__main__\":\\n    unittest.main()'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/load_data_from_file.py'}, page_content='\"\"\"\\nThis example demonstrates loading a local data file containing 5 minutes of temperature data\\nand printing the mean value grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# load results from local file\\ndf = sage_data_client.load(\"data.json\")\\n\\n# print number of results of each name\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.mean())'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/trigger-stream.py'}, page_content='\"\"\"\\nThis is an example of a simple edge-to-cloud stream trigger which uses sage-data-client\\nto watch the latest internal temperature values and print records which exceed a threshold.\\n\\nAlthough it\\'s simple, this example could easily be extended in multiple ways. For example:\\n* Instead of just printing, an alert could be posted to Slack.\\n* Instead of a fixed threshold, you could learn a moving average per node and flag outliers.\\n\\nNote: In the future, this kind of streaming functionality *might* be provided by sage-data-client,\\nbut for now you can adapt this example to fit you use case.\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\nimport time'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/trigger-stream.py'}, page_content='def watch(start=None, filter=None):\\n    if start is None:\\n        start = pd.Timestamp.utcnow()\\n\\n    while True:\\n        df = sage_data_client.query(\\n            start=start,\\n            filter=filter,\\n        )\\n\\n        if len(df) > 0:\\n            start = df.timestamp.max()\\n            yield df\\n\\n        time.sleep(3.0)\\n\\n\\ndef main():\\n    filter = {\\n        \"name\": \"env.temperature\",\\n        \"sensor\": \"bme280\",\\n    }\\n\\n    threshold = 50.0\\n\\n    for df in watch(filter=filter):\\n        # print values which exceed threshold\\n        print(df[df.value > threshold].sort_values(\"timestamp\"))\\n\\n\\nif __name__ == \"__main__\":\\n    main()'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/print_rain_event_image_urls.py'}, page_content='\"\"\"\\nThis example demonstrates cross referencing rain gauge data to find rainy images. It outputs a list\\nof urls which can be saved and downloaded as follows:\\n\\npython3 print_rain_event_image_urls.py > urls.txt\\nwget -r -N -i urls.txt\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\n\\nvsn = \"W039\"\\n\\n# query raingauge data for the last week\\ndf = sage_data_client.query(\\n    start=\"2021-12-20\",\\n    end=\"2021-12-27\",\\n    filter={\\n        \"name\": \"env.raingauge.acc\",\\n        \"vsn\": vsn,\\n    }\\n)\\n\\n# compute mean rain in hour window\\nmean_acc = df.resample(\"1h\", on=\"timestamp\").value.mean()\\n\\n# find rain accumulation events\\nrain_events = mean_acc[mean_acc > 0]'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/print_rain_event_image_urls.py'}, page_content='# collect uploads in each rain event window\\nuploads = pd.concat(sage_data_client.query(\\n        start=ts,\\n        end=ts + pd.to_timedelta(\"1h\"),\\n        filter={\\n            \"name\": \"upload\",\\n            \"vsn\": vsn,\\n            \"task\": \"imagesampler-top\",\\n        }\\n    ) for ts in rain_events.index)\\n\\n# print all urls found\\nfor url in uploads.value.values:\\n    print(url)'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/pressure_event_trigger.py'}, page_content='\"\"\"\\nThis example is a skeleton of how to poll the data system every minute for unusual\\npressure events.\\n\\nIn this case, events are determined windows with a stddev above an example\\nthreshold. For applications, you will need to provide your own criteria for\\nevents.\\n\\nAdditionally, you will need to provide a specific mechanism to carry out the\\nalerts (ex. email, Slack, dedicated alerting / ticketing system, etc).\\n\"\"\"\\nimport sage_data_client\\nimport time\\n\\nwhile True:\\n    # query pressure data in recent 10 minute window\\n    df = sage_data_client.query(\\n        start=\"-10m\",\\n        filter={\\n            \"name\": \"env.pressure\",\\n            \"sensor\": \"bme680\",\\n        }\\n    )'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/pressure_event_trigger.py'}, page_content='# compute stddev for nodes\\' pressure data in window\\n    std = df.groupby(\"meta.vsn\").value.std()\\n\\n    # find all pressure events exceeding an example threshold\\n    events = std[std > 8.0]\\n\\n    # \"post\" vsn to alert system\\n    for vsn in events.index:\\n        print(f\"post {vsn} to alert system\")\\n\\n    time.sleep(60)'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/temperature_stats.py'}, page_content='\"\"\"\\nThis example demonstrates querying all temperature data and printing basic stats grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# query and load data into pandas data frame\\ndf = sage_data_client.query(\\n    start=\"-1h\",\\n    filter={\\n        \"name\": \"env.temperature\",\\n    }\\n)\\n\\n# print stats of the temperature data grouped by node + sensor.\\nprint(df.groupby([\"meta.vsn\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"]))'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/join_queries.py'}, page_content='\"\"\"\\nThis example demonstrates one approach for combining multiple queries by resampling\\nresults into 30 minute windows and merging those into a new data frame.\\n\"\"\"\\nimport sage_data_client\\nimport pandas as pd\\n\\n\\ndef join_resampled_queries(start, end, window, filters):\\n    \"\"\"\\n    join_resampled_queries joins resampled data for a set of filters together\\n    into a single data frame\\n    \"\"\"\\n    return pd.DataFrame({\\n        name: sage_data_client.query(\\n            start=start,\\n            end=end,\\n            filter=filter,\\n        ).resample(window, on=\"timestamp\").value.mean()\\n        for name, filter in filters.items()\\n    })'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/join_queries.py'}, page_content='def main():\\n    start = \"2022-01-10T00:00:00Z\"\\n    end = \"2022-01-11T00:00:00Z\"\\n    vsn = \"W023\"'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/join_queries.py'}, page_content='# combine lat, lon, temperature, pressure and humidity into data frame\\n    df = join_resampled_queries(start, end, \"30min\", {\\n        \"lat\": {\\n            \"name\": \"sys.gps.lat\",\\n            \"vsn\": vsn,\\n        },\\n        \"lon\": {\\n            \"name\": \"sys.gps.lon\",\\n            \"vsn\": vsn,\\n        },\\n        \"temperature\": {\\n            \"name\": \"env.temperature\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n        \"pressure\": {\\n            \"name\": \"env.pressure\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n        \"humidity\": {\\n            \"name\": \"env.relative_humidity\",\\n            \"vsn\": vsn,\\n            \"sensor\": \"bme680\"\\n        },\\n    })'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/join_queries.py'}, page_content='# print out data for quick inspection\\n    print(df)\\n\\n    # save data to csv\\n    df.to_csv(\"combined.csv\")\\n\\n\\nif __name__ == \"__main__\":\\n    main()'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/raingauge_totals.py'}, page_content='\"\"\"\\nThis example demonstrates querying rain gauge data and printing the total\\nnumber of measurements grouped by VSN and sensor.\\n\"\"\"\\nimport sage_data_client\\n\\n# query and load data into pandas data frame\\ndf = sage_data_client.query(\\n    start=\"-1h\",\\n    filter={\\n        \"name\": \"env.raingauge.*\",\\n    }\\n)\\n\\n# print number of results of each name\\nprint(df.groupby([\"meta.vsn\", \"name\"]).size())'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/trigger-batch.py'}, page_content='\"\"\"\\nThis is an example of a simple edge-to-cloud batch trigger which uses sage-data-client\\nto gather and aggregate internal temperature data every 5 minutes and prints all\\nnodes which exceed a threshold.\\n\\nAlthough it\\'s simple, this example could easily be extended in multiple ways. For example:\\n* Instead of just printing, an alert could be posted to Slack.\\n* Instead of a fixed threshold, the typical value across all nodes could be used to determine outliers.\\n\"\"\"\\nimport sage_data_client\\nimport time'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/trigger-batch.py'}, page_content='def main():\\n    filter = {\\n        \"name\": \"env.temperature\",\\n        \"sensor\": \"bme280\",\\n    }\\n\\n    threshold = 55.0\\n\\n    while True:\\n        # get the last 5m of temperature data\\n        df = sage_data_client.query(start=\"-5m\", filter=filter)\\n\\n        # get mean temperature by node in batch query\\n        mean_temps = df.groupby(\"meta.vsn\").value.mean()\\n\\n        # print values which exceed threshold\\n        print(mean_temps[mean_temps > threshold])\\n\\n        # wait 5m\\n        time.sleep(300)\\n\\n\\nif __name__ == \"__main__\":\\n    main()'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='from gzip import GzipFile\\nimport json\\nfrom pathlib import Path\\nfrom urllib.request import urlopen, Request\\nimport pandas as pd\\n\\n\\ndef resolve_time(t):\\n    try:\\n        return pd.to_datetime(t)\\n    except (TypeError, ValueError):\\n        pass\\n    return pd.to_datetime(\"now\", utc=True) + pd.to_timedelta(t)\\n\\n\\ndef timestr(t):\\n    return t.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='def query(\\n    start,\\n    end=None,\\n    head: int = None,\\n    tail: int = None,\\n    experimental_func=None,\\n    bucket: str = None,\\n    filter: dict = None,\\n    endpoint: str = \"https://data.sagecontinuum.org/api/v1/query\",\\n) -> pd.DataFrame:\\n    \"\"\"\\n    query makes a query request to the data API and returns the results in a data frame.\\n\\n    Parameters\\n    ----------\\n    start : query start time, required\\n        Timestamps can be a relative like \"-1h\" or absolute like \"2021-05-01T10:30:00Z\".\\n\\n    end : query end time, default: None\\n        Timestamps can be a relative like \"-1h\" or absolute like \"2021-05-01T10:30:00Z\".'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='head : limit query response to earliest `head` records, default: None (only one of `head` or `tail` can be provided)\\n\\n    tail : limit query response to latest `tail` records, default: None (only one of `head` or `tail` can be provided)\\n\\n    experimental_func : aggregation function to apply to series.\\n\\n    bucket: name of bucket to query\\n\\n    filter : dictionary of query filters, default: None\\n\\n    endpoint : url of query api, default: \"https://data.sagecontinuum.org/api/v1/query\"\\n\\n    Returns\\n    -------\\n    result : pandas.DataFrame\\n        The data frame will contain the query response records.\\n\\n        See the Returns section for the `load` function for more details.'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='Examples\\n    --------\\n\\n    Querying and perform simple data aggregation\\n\\n    ```python\\n    import sage_data_client\\n\\n    # query and load data into pandas data frame\\n    df = sage_data_client.query(\\n        start=\"-1h\",\\n        filter={\\n            \"name\": \"env.temperature\",\\n        }\\n    )\\n\\n    # print results in data frame\\n    print(df)\\n\\n    # meta columns are expanded into meta.fieldname. for example, here we print the unique nodes\\n    print(df[\"meta.node\"].unique())'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='# print stats of the temperature data grouped by node + sensor.\\n    print(df.groupby([\"meta.node\", \"meta.sensor\"]).value.agg([\"size\", \"min\", \"max\", \"mean\"]))\\n    ```\\n    \"\"\"\\n    # build query\\n    q = {\"start\": timestr(resolve_time(start))}\\n    if end is not None:\\n        q[\"end\"] = timestr(resolve_time(end))\\n    if filter is not None:\\n        q[\"filter\"] = filter\\n    if head is not None and tail is not None:\\n        raise ValueError(\"only one of `head` or `tail` can be provided\")\\n    elif head is not None:\\n        q[\"head\"] = head\\n    elif tail is not None:\\n        q[\"tail\"] = tail\\n    if experimental_func is not None:\\n        q[\"experimental_func\"] = experimental_func'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='if bucket is not None:\\n        q[\"bucket\"] = bucket'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='data = json.dumps(q).encode()\\n    headers = {\"Accept-Encoding\": \"gzip\"}\\n    req = Request(endpoint, data, headers=headers)\\n\\n    with urlopen(req) as f:\\n        content_encoding = f.headers.get(\"Content-Encoding\", \"\")\\n        if \"gzip\" in content_encoding:\\n            f = GzipFile(fileobj=f, mode=\"rb\")\\n        return load(f)'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='def load(path_or_buf) -> pd.DataFrame:\\n    \"\"\"\\n    load reads a path or file like object containing a response from the data api and returns the results in a data frame.\\n\\n    Parameters\\n    ----------\\n    path_or_buf : path like or file like object\\n\\n    Returns\\n    -------\\n    result : pandas.DataFrame\\n        The data frame will contain the query response records. Standard columns names are:\\n\\n        `name`: measurement name (ex. \"env.temperature\")\\n\\n        `timestamp`: measurement timestamp (nanoseconds since epoch resolution)\\n\\n        `value`: measurement value\\n\\n        Metadata fields like \"node\" and \"vsn\" are stored in columns named \"meta.node\" or \"meta.vsn\".\\n\\n    Examples\\n    --------'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='Loading saved query results from a file\\n\\n    Suppose we\\'ve saved the results of a query to a file `data.json`. We can load them using the following:\\n\\n    ```python\\n    import sage_data_client\\n\\n    # load results from local file\\n    df = sage_data_client.load(\"data.ndjson\")\\n\\n    # print number of results of each name\\n    print(df.groupby([\"meta.node\", \"name\"]).size())\\n    ```\\n    \"\"\"\\n    if isinstance(path_or_buf, str):\\n        if path_or_buf.endswith(\".gz\"):\\n            with GzipFile(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n        else:\\n            with open(path_or_buf, \"rb\") as f:\\n                return _load(f)'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='if isinstance(path_or_buf, Path):\\n        if path_or_buf.suffix == \".gz\":\\n            with GzipFile(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n        else:\\n            with open(path_or_buf, \"rb\") as f:\\n                return _load(f)\\n\\n    return _load(path_or_buf)'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/query.py'}, page_content='def _load_row(r):\\n    input = json.loads(r)\\n    output = {}\\n    output[\"timestamp\"] = pd.to_datetime(input[\"timestamp\"], unit=\"ns\", utc=True)\\n    output[\"name\"] = input[\"name\"]\\n    output[\"value\"] = input[\"value\"]\\n    for k, v in input[\"meta\"].items():\\n        output[f\"meta.{k}\"] = v\\n    return output\\n\\n\\ndef _load(fileobj) -> pd.DataFrame:\\n    df = pd.DataFrame(map(_load_row, fileobj))\\n\\n    # if dataframe is empty, return empty with known columns\\n    if len(df) == 0:\\n        return pd.DataFrame(\\n            {\\n                \"timestamp\": pd.to_datetime([], utc=True),\\n                \"name\": pd.Series([], dtype=str),\\n                \"value\": [],\\n            }\\n        )\\n\\n    return df'),\n",
       " Document(metadata={'source': 'sage-data-client/src/sage_data_client/__init__.py'}, page_content='\"\"\"\\nsage_data_client - Official Sage Python data API client.\\n========================================================\\n\\nsage_data_client goals are to make writing queries and working with the results easy. It does this by:\\n\\n* Providing a simple query function which talks to the data API.\\n* Providing the results in an easy to use [Pandas](https://pandas.pydata.org) data frame.\\n\"\"\"\\nfrom .query import query, load'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='import unittest\\nfrom pathlib import Path\\nimport json\\nfrom tempfile import TemporaryDirectory\\nimport time\\nfrom datetime import datetime\\nimport os\\nimport pika\\nimport subprocess\\n\\nfrom waggle.plugin import Plugin, PluginConfig, Uploader, get_timestamp\\nimport wagglemsg\\n\\n# TODO(sean) add integration testing against rabbitmq\\n# TODO(sean) clean up the queue interface. it would be better to not know about the plugin.send / plugin.recv queues explicitly.'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='class TestPlugin(unittest.TestCase):\\n    def test_publish(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test.int\", 1)\\n            plugin.publish(\"test.float\", 2.0)\\n            plugin.publish(\"test.str\", \"three\")\\n            plugin.publish(\\n                \"cows.total\",\\n                391,\\n                meta={\\n                    \"camera\": \"bottom_left\",\\n                },\\n            )\\n\\n    def test_publish_check_reserved(self):\\n        with Plugin() as plugin:\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"upload\", \"path/to/data\")'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='def test_get(self):\\n        with Plugin() as plugin:\\n            plugin.subscribe(\"raw.#\")\\n            with self.assertRaises(TimeoutError):\\n                plugin.get(timeout=0)\\n            with self.assertRaises(TimeoutError):\\n                plugin.get(timeout=0.001)\\n\\n            msg = wagglemsg.Message(\"test\", 1.0, 0, {})\\n            plugin.recv.put(msg)\\n            msg2 = plugin.get(timeout=0)\\n            self.assertEqual(msg, msg2)\\n\\n    def test_get_timestamp(self):\\n        ts = get_timestamp()\\n        self.assertIsInstance(ts, int)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='def test_valid_values(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test\", 1)\\n            plugin.publish(\"test\", 1.3)\\n            plugin.publish(\"test\", \"some string\")\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", b\"some bytes\")\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", [1, 2, 3])\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", {1: 1, 2: 2, 3: 3})'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='def test_valid_meta(self):\\n        with Plugin() as plugin:\\n            plugin.publish(\"test\", 1, meta={\"k\": \"v\"})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": 10})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": 12.3})\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, meta={\"k\": []})\\n\\n    def test_valid_timestamp(self):\\n        with Plugin() as plugin:\\n            # valid int, nanosecond timestamp\\n            plugin.publish(\"test\", 1, timestamp=1649694687904754000)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='# must prevent a float type timestamp\\n            ts = datetime(2022, 1, 1, 0, 0, 0).timestamp()\\n            with self.assertRaises(TypeError):\\n                plugin.publish(\"test\", 1, timestamp=ts)\\n\\n            # must prevent int timestamp in seconds from being loaded by flagging\\n            # timestamps that are too early.\\n            testcases = [\\n                datetime(2022, 1, 1, 0, 0, 0),\\n                datetime(3000, 1, 1, 0, 0, 0),\\n                datetime(5000, 1, 1, 0, 0, 0),\\n            ]\\n\\n            for dt in testcases:\\n                with self.assertRaises(ValueError):\\n                    plugin.publish(\"test\", 1, timestamp=int(dt.timestamp()))'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='def test_valid_publish_names(self):\\n        with Plugin() as plugin:\\n            with self.assertRaises(TypeError):\\n                plugin.publish(None, 0)\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"\", 0)\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\".\", 0)\\n\\n            # check for reserved names\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"upload\", 0)\\n\\n            # use _ instead of -\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"my-metric\", 0)\\n            # correct alternative\\n            plugin.publish(\"my_metric\", 0)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='# assert len(name) <= 128\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"x\" * 129, 0)\\n\\n            # no empty parts allowed\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"vision.count..bird\", 0)\\n            # correct alternative\\n            plugin.publish(\"vision.count.bird\", 0)\\n\\n            # no spaces allowed\\n            with self.assertRaises(ValueError):\\n                plugin.publish(\"sys.cpu temp\", 0)\\n            # correct alternative\\n            plugin.publish(\"sys.cpu_temp\", 0)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='# TODO(sean) refactor messaging part to make testing this cleaner\\n    def test_upload_file(self):\\n        with TemporaryDirectory() as tempdir:\\n            pl = Plugin(\\n                PluginConfig(\\n                    host=\"fake-rabbitmq-host\",\\n                    port=5672,\\n                    username=\"plugin\",\\n                    password=\"plugin\",\\n                    app_id=\"0668b12c-0c15-462c-9e06-7239282411e5\",\\n                ),\\n                uploader=Uploader(Path(tempdir, \"uploads\")),\\n            )\\n\\n            data = b\"here some data in a data\"\\n            upload_path = Path(tempdir, \"myfile.txt\")\\n            upload_path.write_bytes(data)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='pl.upload_file(upload_path)\\n            item = pl.send.get_nowait()\\n            msg = wagglemsg.load(item.body)\\n            self.assertEqual(item.scope, \"all\")\\n            self.assertEqual(msg.name, \"upload\")\\n            self.assertIsNotNone(msg.timestamp)\\n            self.assertIsInstance(msg.value, str)\\n            self.assertIsNotNone(msg.meta)\\n            self.assertIn(\"filename\", msg.meta)\\n\\n    def test_timeit(self):\\n        with Plugin() as plugin:\\n            with plugin.timeit(\"dur\"):\\n                time.sleep(0.001)\\n            item = plugin.send.get(0.01)\\n            msg = wagglemsg.load(item.body)\\n            self.assertEqual(msg.name, \"dur\")'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='class TestUploader(unittest.TestCase):\\n    def test_upload_file(self):\\n        with TemporaryDirectory() as tempdir:\\n            uploader = Uploader(Path(tempdir, \"uploads\"))\\n\\n            data = b\"here some data in a data\"\\n\\n            upload_path = Path(tempdir, \"myfile.txt\")\\n            upload_path.write_bytes(data)\\n\\n            path = uploader.upload_file(upload_path)\\n            self.assertFalse(upload_path.exists())'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='self.assertEqual(data, Path(path, \"data\").read_bytes())\\n            meta = json.loads(Path(path, \"meta\").read_text())\\n            self.assertIn(\"timestamp\", meta)\\n            self.assertIn(\"shasum\", meta)\\n            self.assertEqual(meta[\"labels\"][\"filename\"], upload_path.name)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='def rabbitmq_available():\\n    try:\\n        subprocess.check_output([\"docker-compose\", \"exec\", \"rabbitmq\", \"true\"])\\n        return True\\n    except subprocess.CalledProcessError:\\n        return False\\n\\n\\ndef get_admin_connection():\\n    params = pika.ConnectionParameters(\\n        credentials=pika.PlainCredentials(\"admin\", \"admin\")\\n    )\\n    return pika.BlockingConnection(params)\\n\\n\\n@unittest.skipUnless(rabbitmq_available(), \"rabbitmq not available\")'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='class TestPluginWithRabbitMQ(unittest.TestCase):\\n    def setUp(self):\\n        os.environ[\"WAGGLE_PLUGIN_USERNAME\"] = \"plugin\"\\n        os.environ[\"WAGGLE_PLUGIN_PASSWORD\"] = \"plugin\"\\n        os.environ[\"WAGGLE_PLUGIN_HOST\"] = \"127.0.0.1\"\\n        os.environ[\"WAGGLE_PLUGIN_PORT\"] = \"5672\"\\n\\n        with get_admin_connection() as conn, conn.channel() as ch:\\n            ch.queue_purge(\"to-validator\")\\n\\n    def test_publish(self):\\n        now = time.time_ns()\\n\\n        with Plugin() as publisher:\\n            publisher.publish(\"test\", 123, meta={\"sensor\": \"bme680\"}, timestamp=now)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='with get_admin_connection() as conn, conn.channel() as ch:\\n            _, _, body = ch.basic_get(\"to-validator\", auto_ack=True)\\n            msg = wagglemsg.load(body)\\n\\n        self.assertEqual(\\n            msg,\\n            wagglemsg.Message(\\n                name=\"test\",\\n                value=123,\\n                meta={\"sensor\": \"bme680\"},\\n                timestamp=now,\\n            ),\\n        )\\n\\n    def test_subscribe(self):\\n        msg = wagglemsg.Message(\\n            name=\"test\",\\n            value=123,\\n            meta={\"sensor\": \"bme680\"},\\n            timestamp=time.time_ns(),\\n        )'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='with Plugin() as subscriber:\\n            subscriber.subscribe(\"test\")\\n            time.sleep(1)\\n\\n            with get_admin_connection() as conn, conn.channel() as ch:\\n                ch.basic_publish(\"data.topic\", \"test\", wagglemsg.dump(msg))\\n\\n            msg2 = subscriber.get(1)\\n            self.assertEqual(msg, msg2)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='class TestPluginLogDir(unittest.TestCase):\\n    def test_log_dir(self):\\n        import sage_data_client\\n\\n        with TemporaryDirectory() as dir:\\n            dir = Path(dir)\\n\\n            # create dummy upload file\\n            upload_file = Path(dir, \"hello.txt\")\\n            upload_file.write_text(\"hello\")\\n\\n            timestamp = get_timestamp()'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='# set env var and run plugin\\n            try:\\n                os.environ[\"PYWAGGLE_LOG_DIR\"] = str(dir)\\n                with Plugin() as plugin:\\n                    plugin.publish(\"test\", 123, timestamp=timestamp)\\n                    plugin.publish(\\n                        \"test.with.meta\",\\n                        456,\\n                        meta={\"user\": \"data\"},\\n                        timestamp=timestamp + 10000,\\n                    )\\n                    plugin.upload_file(upload_file, timestamp=timestamp + 20000)\\n            finally:\\n                del os.environ[\"PYWAGGLE_LOG_DIR\"]\\n\\n            df = sage_data_client.load(Path(dir, \"data.ndjson\"))'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='# ensure records match what was published\\n            self.assertEqual(len(df), 3)\\n\\n            # TODO(sean) test timestamps\\n            self.assertEqual(df.loc[0, \"name\"], \"test\")\\n            self.assertEqual(df.loc[0, \"value\"], 123)\\n\\n            self.assertEqual(df.loc[1, \"name\"], \"test.with.meta\")\\n            self.assertEqual(df.loc[1, \"value\"], 456)\\n            self.assertEqual(df.loc[1, \"meta.user\"], \"data\")\\n\\n            self.assertEqual(df.loc[2, \"name\"], \"upload\")\\n            self.assertEqual(df.loc[2, \"meta.filename\"], \"hello.txt\")'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='# ensure all uploads exist\\n            for path in df[df.name == \"upload\"].value:\\n                self.assertTrue(Path(path).exists())'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_plugin.py'}, page_content='def assertDictContainsSubset(t, a, b):\\n    t.assertLessEqual(a.items(), b.items())\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='import unittest\\nfrom waggle.data.audio import AudioFolder, AudioSample\\nfrom waggle.data.vision import RGB, BGR, ImageFolder, ImageSample, resolve_device\\nfrom waggle.data.timestamp import get_timestamp\\nimport numpy as np\\nfrom tempfile import TemporaryDirectory\\nfrom pathlib import Path\\nimport os.path\\nfrom itertools import product'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='def generate_audio_data(samplerate, channels, dtype):\\n    if dtype == np.float32:\\n        return np.random.uniform(-1, 1, (samplerate, channels)).astype(dtype)\\n    if dtype == np.float64:\\n        return np.random.uniform(-1, 1, (samplerate, channels)).astype(dtype)\\n    if dtype == np.int16:\\n        return np.random.randint(\\n            -(2**15), 2**15, (samplerate, channels), dtype=dtype\\n        )\\n    if dtype == np.int32:\\n        return np.random.randint(\\n            -(2**31), 2**31, (samplerate, channels), dtype=dtype\\n        )\\n    raise ValueError(\"unsupported audio settings\")'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='def generate_audio_sample(samplerate, channels, dtype):\\n    return AudioSample(generate_audio_data(samplerate, channels, dtype), samplerate, 0)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='class TestData(unittest.TestCase):\\n    def test_colors(self):\\n        for fmt in [RGB, BGR]:\\n            data = np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8)\\n            data2 = fmt.format_to_cv2(fmt.cv2_to_format(data))\\n            self.assertTrue(\\n                np.all(np.isclose(data, data2, 1.0)), f\"checking format {fmt}\"\\n            )'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='def test_resolve_device(self):\\n        self.assertEqual(\\n            resolve_device(Path(\"test.jpg\")), str(Path(\"test.jpg\").absolute())\\n        )\\n        self.assertEqual(\\n            resolve_device(\"file://path/to/test.jpg\"),\\n            str(Path(\"path/to/test.jpg\").absolute()),\\n        )\\n        self.assertEqual(\\n            resolve_device(\"http://camera-ip.org/image.jpg\"),\\n            \"http://camera-ip.org/image.jpg\",\\n        )\\n        self.assertEqual(\\n            resolve_device(\"rtsp://camera-ip.org/image.jpg\"),\\n            \"rtsp://camera-ip.org/image.jpg\",\\n        )\\n        self.assertEqual(resolve_device(0), 0)'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='def test_image_save(self):\\n        with TemporaryDirectory() as dir:\\n            sample = ImageSample(\\n                np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8), 0, RGB\\n            )\\n            for fmt in [\"jpg\", \"png\"]:\\n                name = f\"sample.{fmt}\"\\n                sample.save(os.path.join(dir, name))\\n                sample.save(Path(dir, name))'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='def test_image_save_load(self):\\n        with TemporaryDirectory() as dir:\\n            sample = ImageSample(\\n                np.random.randint(0, 255, (100, 120, 3), dtype=np.uint8), 0, RGB\\n            )\\n            sample.save(Path(dir, \"sample.png\"))\\n            samples = ImageFolder(dir, RGB)\\n            self.assertTrue(np.allclose(sample.data, samples[0].data))\\n\\n    def test_audio_save(self):\\n        test_formats = [\"wav\", \"flac\"]\\n        test_samplerates = [22050, 44100, 48000]\\n        test_channels = [1, 2]\\n        test_dtypes = [np.float32, np.float64, np.int16, np.int32]'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='for format, samplerate, channels, dtype in product(\\n            test_formats, test_samplerates, test_channels, test_dtypes\\n        ):\\n            name = f\"sample.{format}\"\\n            with TemporaryDirectory() as dir:\\n                sample = generate_audio_sample(\\n                    samplerate, channels=channels, dtype=dtype\\n                )\\n                # test saving as any PathLike\\n                sample.save(os.path.join(dir, name))\\n                sample.save(Path(dir, name))'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='def test_audio_save_load(self):\\n        test_formats = [\"wav\", \"flac\"]\\n        test_samplerates = [22050, 44100, 48000]\\n        test_channels = [1, 2]\\n        test_dtypes = [np.float32, np.float64]'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='for format, samplerate, channels, dtype in product(\\n            test_formats, test_samplerates, test_channels, test_dtypes\\n        ):\\n            name = f\"sample.{format}\"\\n            with TemporaryDirectory() as dir:\\n                sample = generate_audio_sample(\\n                    samplerate, channels=channels, dtype=dtype\\n                )\\n                sample.save(Path(dir, name))\\n                samples = AudioFolder(dir)\\n                self.assertTrue(\\n                    np.allclose(sample.data, samples[0].data, atol=1e-4),\\n                    msg=f\"failed: format={format} samplerate={samplerate} channels={channels} dtypes={dtype}\",\\n                )'),\n",
       " Document(metadata={'source': 'pywaggle/tests/test_data.py'}, page_content='def test_get_timestamp(self):\\n        ts = get_timestamp()\\n        self.assertIsInstance(ts, int)\\n\\n\\nif __name__ == \"__main__\":\\n    unittest.main()'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/time.py'}, page_content=\"import time\\n\\n# BUG This *must* be addressed with the behavior written up in the plugin spec.\\n# We don't want any surprises in terms of accuraccy\\ntry:\\n    from time import time_ns as get_timestamp\\nexcept ImportError:\\n\\n    def get_timestamp():\\n        return int(time.time() * 1e9)\\n\\n\\n# NOTE to preserve the best accuracy, we implement the backwards compatible perf\\n# counter by only abstracting how to measure the duration between two times in\\n# nanoseconds\\ntry:\\n    from time import perf_counter_ns as timeit_perf_counter\\n\\n    def timeit_perf_counter_duration(start, finish):\\n        return finish - start\\n\\nexcept ImportError:\\n    from time import perf_counter as timeit_perf_counter\"),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/time.py'}, page_content='def timeit_perf_counter_duration(start, finish):\\n        return int((finish - start) * 1e9)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/config.py'}, page_content='from typing import NamedTuple\\n\\n\\nclass PluginConfig(NamedTuple):\\n    \"\"\"\\n    PluginConfig represents the config required to setup and run a Plugin.\\n    \"\"\"\\n\\n    # TODO generalize to support different backends\\n    username: str\\n    password: str\\n    host: str\\n    port: int\\n    app_id: str'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/uploader.py'}, page_content='import hashlib\\nimport json\\nfrom pathlib import Path\\nfrom shutil import copyfile\\nfrom .time import get_timestamp'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/uploader.py'}, page_content='class Uploader:\\n    def __init__(self, root):\\n        self.root = Path(root)\\n\\n    # NOTE uploads are stored in the following directory structure:\\n    # root/\\n    #   timestamp-sha1sum/\\n    #     data\\n    #     meta\\n    def upload_file(self, path, meta={}, timestamp=None, keep=False):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n\\n        path = Path(path)\\n        checksum = sha1sum_for_file(path)\\n\\n        # create upload dir\\n        upload_dir = Path(self.root, f\"{timestamp}-{checksum}\")\\n        upload_dir.mkdir(parents=True, exist_ok=True)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/uploader.py'}, page_content='# stage data file\\n        # NOTE we do a copy instead of move, as the upload dir may\\n        # be mounted from another disk.\\n        copyfile(path, Path(upload_dir, \"data\"))\\n        if not keep:\\n            path.unlink()\\n\\n        # stage meta file\\n        metafile = {\\n            \"timestamp\": timestamp,\\n            \"shasum\": checksum,\\n            \"labels\": {k: v for k, v in meta.items()},\\n        }\\n        metafile[\"labels\"][\"filename\"] = path.name\\n        write_json_file(Path(upload_dir, \"meta\"), metafile)\\n\\n        return upload_dir'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/uploader.py'}, page_content='def sha1sum_for_file(path):\\n    h = hashlib.sha1()\\n    with open(path, \"rb\") as f:\\n        while True:\\n            chunk = f.read(32768)\\n            if chunk == b\"\":\\n                break\\n            h.update(chunk)\\n    return h.hexdigest()\\n\\n\\ndef write_json_file(path, obj):\\n    with open(path, \"w\") as f:\\n        json.dump(obj, f, separators=(\",\", \":\"), sort_keys=True)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/__init__.py'}, page_content='from .config import PluginConfig\\nfrom .plugin import Plugin\\nfrom .uploader import Uploader\\nfrom .time import get_timestamp'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='import logging\\nfrom threading import Thread, Event\\nfrom queue import Queue, Empty\\nimport time\\nimport pika\\nimport pika.exceptions\\nimport wagglemsg\\nfrom .config import PluginConfig\\n\\n\\nlogger = logging.getLogger(__name__)\\n# pika is very verbose at DEBUG level. we turn it down here.\\nlogging.getLogger(\"pika\").setLevel(logging.CRITICAL)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='class RabbitMQPublisher:\\n    \"\"\"\\n    RabbitMQPublisher manages a connection to RabbitMQ and publishes messages from the provided queue.\\n\\n    This is done in a background thread which must be stopped by setting the provided stop Event.\\n    \"\"\"\\n\\n    def __init__(self, config: PluginConfig, messages: Queue, stop: Event):\\n        self.config = config\\n        self.params = get_connection_parameters_for_config(config)\\n        self.messages = messages\\n        self.stop = stop\\n        self.done = Event()\\n        Thread(target=self.__main).start()'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='def __main(self):\\n        logger.debug(\"publisher started.\")\\n        try:\\n            while not self.stop.is_set():\\n                try:\\n                    self.__connect_and_flush_messages()\\n                except Exception:\\n                    if logger.isEnabledFor(logging.DEBUG):\\n                        logger.exception(\"__connect_and_flush_messages exception\")\\n                    time.sleep(1)\\n        finally:\\n            self.done.set()\\n            logger.debug(\"publisher stopped.\")'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='def __connect_and_flush_messages(self):\\n        logger.debug(\"publisher connecting to rabbitmq...\")\\n        with pika.BlockingConnection(self.params) as conn, conn.channel() as ch:\\n            while not self.stop.is_set():\\n                self.__flush_messages(ch)\\n            logger.debug(\"publisher stopping...\")\\n            # attempt to flush any remaining messages\\n            self.__flush_messages(ch)\\n\\n    def __flush_messages(self, ch):\\n        while True:\\n            try:\\n                logger.debug(\"publisher checking for message...\")\\n                item = self.messages.get(timeout=1)\\n            except Empty:\\n                return'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='properties = pika.BasicProperties(\\n                delivery_mode=2, user_id=self.params.credentials.username\\n            )\\n\\n            # NOTE app_id is used by data service to validate and tag additional metadata provided by k3s scheduler.\\n            if self.config.app_id != \"\":\\n                properties.app_id = self.config.app_id\\n\\n            if logger.isEnabledFor(logging.DEBUG):\\n                logger.debug(\\n                    \"publishing message to rabbitmq: %s\", wagglemsg.load(item.body)\\n                )'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='try:\\n                ch.basic_publish(\\n                    exchange=\"to-validator\",\\n                    routing_key=item.scope,\\n                    properties=properties,\\n                    body=item.body,\\n                )\\n            except Exception:\\n                if logger.isEnabledFor(logging.DEBUG):\\n                    logger.exception(\\n                        \"basic_publish to rabbitmq failed. will requeue message...\"\\n                    )\\n                # requeue message so we can again later\\n                # NOTE(sean) this will reorder messages. if we realized we *must* preserve message\\n                # order, we must to change this to avoid subtle bugs!'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='self.messages.put(item)\\n                # propagate error up to trigger reconnect\\n                raise'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='class RabbitMQConsumer:\\n    \"\"\"\\n    RabbitMQConsumer manages a connection to RabbitMQ and puts received messages into the provided queue.\\n\\n    This is done in a background thread which must be stopped by setting the provided stop Event.\\n    \"\"\"\\n\\n    def __init__(self, topics, config: PluginConfig, messages: Queue, stop: Event):\\n        self.topics = topics\\n        self.config = config\\n        self.params = get_connection_parameters_for_config(config)\\n        self.messages = messages\\n        self.stop = stop\\n        self.done = Event()\\n        Thread(target=self.__main).start()'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='def __main(self):\\n        logger.debug(\"consumer started.\")\\n        try:\\n            while not self.stop.is_set():\\n                try:\\n                    self.__connect_and_consume_messages()\\n                except Exception:\\n                    if logger.isEnabledFor(logging.DEBUG):\\n                        logger.exception(\"__connect_and_consume_messages exception\")\\n                    time.sleep(1)\\n        finally:\\n            self.done.set()\\n            logger.debug(\"consumer stopped.\")'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='def __connect_and_consume_messages(self):\\n        logger.debug(\"consumer connecting to rabbitmq...\")\\n        with pika.BlockingConnection(self.params) as conn, conn.channel() as ch:\\n            # setup subscriber queue and bind to topics\\n            queue = ch.queue_declare(\"\", exclusive=True).method.queue\\n            ch.basic_consume(queue, self.__process_message, auto_ack=True)\\n\\n            for topic in self.topics:\\n                ch.queue_bind(queue, \"data.topic\", topic)\\n                logger.debug(\"consumer binding queue %s to topic %s\", queue, topic)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='def check_stop():\\n                if self.stop.is_set():\\n                    logger.debug(\"consumer stopping...\")\\n                    ch.stop_consuming()\\n                else:\\n                    conn.call_later(1, check_stop)\\n\\n            conn.call_later(1, check_stop)\\n            logger.debug(\"consumer start processing messages...\")\\n            ch.start_consuming()'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='def __process_message(self, ch, method, properties, body):\\n        try:\\n            logger.debug(\"consumer processing message %s...\", body)\\n            msg = wagglemsg.load(body)\\n        except TypeError:\\n            logger.debug(\"unsupported message type: %s %s\", properties, body)\\n            return\\n        logger.debug(\"consumer putting message in waiting queue\")\\n        self.messages.put(msg)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/rabbitmq.py'}, page_content='def get_connection_parameters_for_config(\\n    config: PluginConfig,\\n) -> pika.ConnectionParameters:\\n    return pika.ConnectionParameters(\\n        host=config.host,\\n        port=config.port,\\n        credentials=pika.PlainCredentials(\\n            username=config.username,\\n            password=config.password,\\n        ),\\n        connection_attempts=1,\\n        socket_timeout=1.0,\\n    )'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='import logging\\nimport re\\nimport wagglemsg\\n\\nfrom contextlib import contextmanager\\nfrom datetime import datetime\\nfrom os import getenv\\nfrom pathlib import Path\\nfrom queue import Queue, Empty\\nfrom threading import Event\\nfrom typing import NamedTuple\\n\\nfrom .config import PluginConfig\\nfrom .rabbitmq import RabbitMQPublisher, RabbitMQConsumer\\nfrom .time import get_timestamp, timeit_perf_counter, timeit_perf_counter_duration\\nfrom .uploader import Uploader\\n\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass PublishData(NamedTuple):\\n    scope: str\\n    body: bytes\\n\\n\\n# Nanoseconds since epoch for 2000-01-01T00:00:00Z\\nMIN_TIMESTAMP_NS = 946706400000000000'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='class FilesystemPublisher:\\n    def __init__(self, root):\\n        self.root = Path(root)\\n        self.root.mkdir(parents=True, exist_ok=True)\\n        self.datafile = Path(root, \"data.ndjson\").open(\"a\")\\n        self.uploads_dir = Path(root, \"uploads\")\\n        self.uploads_dir.mkdir(parents=True, exist_ok=True)\\n\\n    def close(self):\\n        self.datafile.close()\\n\\n    def publish(self, msg: wagglemsg.Message):\\n        import json'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='out = {\\n            \"name\": msg.name,\\n            \"value\": msg.value,\\n            \"meta\": msg.meta,\\n            # python doesn\\'t have builtin support for nanosecond\\n            \"timestamp\": isoformat_time_ns(msg.timestamp),\\n        }\\n        print(\\n            json.dumps(out, sort_keys=True, separators=(\",\", \":\")),\\n            file=self.datafile,\\n            flush=True,\\n        )\\n\\n    def upload_file(self, path, timestamp, meta):\\n        from shutil import copyfile'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='src = Path(path)\\n        dst = Path(self.uploads_dir, f\"{timestamp}-{src.name}\")\\n        copyfile(src, dst)\\n        meta = meta.copy()\\n        meta[\"filename\"] = Path(src).name\\n        self.publish(\\n            wagglemsg.Message(\\n                name=\"upload\",\\n                value=str(dst.absolute()),\\n                meta=meta,\\n                timestamp=timestamp,\\n            )\\n        )'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='def isoformat_time_ns(ns: int) -> str:\\n    # python doesn\\'t have builtin support for nanosecond timestamps and formatting, so we provide\\n    # a backfill for it. this is only intended to be used in the run log for testing.\\n    nanostr = f\"{ns%1000:03d}\"\\n    return datetime.fromtimestamp(ns / 1e9).isoformat() + nanostr'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='class Plugin:\\n    \"\"\"\\n    Plugin provides methods to publish and consume messages inside the Waggle ecosystem.\\n\\n    Examples\\n    --------\\n\\n    The simplest example is creating a Plugin and publishing a message. This can be done using:\\n\\n    ```python\\n    from waggle.plugin import Plugin\\n\\n    with Plugin() as plugin:\\n        plugin.publish(\"test_value\", 99)\\n    ```\\n    \"\"\"'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='def __init__(\\n        self, config=None, uploader=None, file_publisher: FilesystemPublisher = None\\n    ):\\n        self.config = config or get_default_plugin_config()\\n        self.uploader = uploader or get_default_plugin_uploader()\\n        self.send = Queue()\\n        self.recv = Queue()\\n        self.stop = Event()\\n        self.tasks = []\\n\\n        # TODO(sean) can we use ExitStack to clean up???\\n\\n        self.file_publisher = file_publisher\\n\\n        if self.file_publisher is None and getenv(\"PYWAGGLE_LOG_DIR\") is not None:\\n            self.file_publisher = FilesystemPublisher(getenv(\"PYWAGGLE_LOG_DIR\"))'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='def __enter__(self):\\n        self.tasks.append(RabbitMQPublisher(self.config, self.send, self.stop))\\n        return self\\n\\n    def __exit__(self, exc_type, exc_value, exc_traceback):\\n        self.stop.set()\\n\\n        if self.file_publisher is not None:\\n            self.file_publisher.close()\\n\\n        for task in self.tasks:\\n            task.done.wait()\\n\\n    def subscribe(self, *topics):\\n        self.tasks.append(RabbitMQConsumer(topics, self.config, self.recv, self.stop))\\n        # TODO(sean) add mock or integration testing against rabbitmq to actually test this'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='def get(self, timeout=None):\\n        try:\\n            return self.recv.get(timeout=timeout)\\n        except Empty:\\n            pass\\n        raise TimeoutError(\"plugin get timed out\")\\n\\n    def publish(self, name, value, meta={}, timestamp=None, scope=\"all\", timeout=None):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n        raise_for_invalid_publish_name(name)\\n        self.__publish(name, value, meta, timestamp, scope, timeout)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='# NOTE __publish is used internally by publish and upload_file to do an unchecked\\n    # message publish. the main reason this exists is to guard against reserved names\\n    # like \"upload\" in publish but still allow upload_file to use it.\\n    def __publish(self, name, value, meta, timestamp, scope=\"all\", timeout=None):\\n        if not isinstance(value, (int, float, str)):\\n            raise TypeError(\"Value must be an int, float or str.\")\\n        if not isinstance(timestamp, int):\\n            raise TypeError(\\n                \"Timestamp must be an int and have units of nanoseconds since epoch. Please see the documentation for more information on setting timestamps.\"\\n            )'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='if timestamp < MIN_TIMESTAMP_NS:\\n            raise ValueError(\\n                \"Timestamp probably has wrong units and is being processed as before 2000-01-01T00:00:00Z. Timestamp must have units of nanoseconds since epoch. Please see the documentation for more information on setting timestamps.\"\\n            )\\n        if not valid_meta(meta):\\n            raise TypeError(\"Meta must be a dictionary of strings to strings.\")\\n        msg = wagglemsg.Message(name=name, value=value, timestamp=timestamp, meta=meta)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='# hack to use file publisher for everything except uploads\\n        if self.file_publisher is not None and name != \"upload\":\\n            self.file_publisher.publish(msg)\\n\\n        logger.debug(\"adding message to outgoing queue: %s\", msg)\\n        self.send.put(PublishData(scope, wagglemsg.dump(msg)), timeout=timeout)\\n\\n    def upload_file(self, path, meta={}, timestamp=None, keep=False):\\n        # get timestamp before doing other work\\n        timestamp = timestamp or get_timestamp()\\n\\n        if self.file_publisher is not None:\\n            self.file_publisher.upload_file(path, meta=meta, timestamp=timestamp)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='if self.uploader is not None:\\n            meta = meta.copy()\\n            meta[\"filename\"] = Path(path).name\\n            upload_path = self.uploader.upload_file(\\n                path=path, meta=meta, timestamp=timestamp, keep=keep\\n            )\\n            self.__publish(\"upload\", upload_path.name, meta, timestamp)\\n\\n    @contextmanager\\n    def timeit(self, name):\\n        logger.debug(\"starting timeit block %s\", name)\\n        start = timeit_perf_counter()\\n        yield\\n        finish = timeit_perf_counter()\\n        duration = timeit_perf_counter_duration(start, finish)\\n        self.publish(name, duration)\\n        logger.debug(\"finished timeit block %s\", name)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='def get_default_plugin_config() -> PluginConfig:\\n    return PluginConfig(\\n        username=getenv(\"WAGGLE_PLUGIN_USERNAME\", \"plugin\"),\\n        password=getenv(\"WAGGLE_PLUGIN_PASSWORD\", \"plugin\"),\\n        host=getenv(\"WAGGLE_PLUGIN_HOST\", \"rabbitmq\"),\\n        port=int(getenv(\"WAGGLE_PLUGIN_PORT\", 5672)),\\n        app_id=getenv(\"WAGGLE_APP_ID\", \"\"),\\n    )\\n\\n\\ndef valid_meta(meta):\\n    return isinstance(meta, dict) and all(isinstance(v, str) for v in meta.values())'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='def get_default_plugin_uploader():\\n    if (\\n        getenv(\"WAGGLE_PLUGIN_UPLOAD_PATH\") is None\\n        and getenv(\"PYWAGGLE_LOG_DIR\") is not None\\n    ):\\n        return None\\n    return Uploader(Path(getenv(\"WAGGLE_PLUGIN_UPLOAD_PATH\", \"/run/waggle/uploads\")))\\n\\n\\npublish_name_part_pattern = re.compile(\"^[a-z0-9_]+$\")'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/plugin/plugin.py'}, page_content='def raise_for_invalid_publish_name(s: str):\\n    if not isinstance(s, str):\\n        raise TypeError(f\"publish name must be a string: {s!r}\")\\n    if len(s) > 128:\\n        raise ValueError(f\"publish must be at most 128 characters: {s!r}\")\\n    if s == \"upload\":\\n        raise ValueError(f\"name {s!r} is reserved for system use only\")\\n    parts = s.split(\".\")\\n    for p in parts:\\n        if not publish_name_part_pattern.match(p):\\n            raise ValueError(\\n                f\"publish name invalid: {s!r} part: {p!r} (names must consist of [a-z0-9_] and may be joined by .)\"\\n            )'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='import cv2\\nfrom pathlib import Path\\nimport numpy\\nfrom typing import Union\\nimport os\\nfrom os import PathLike\\nimport random\\nimport json\\nimport re\\nimport threading\\nimport time\\nfrom base64 import b64encode\\nfrom .timestamp import get_timestamp\\nfrom shutil import which\\nimport ffmpeg\\nimport logging\\n\\nlogger = logging.getLogger(__name__)\\n\\n\\nclass BGR:\\n    @classmethod\\n    def cv2_to_format(cls, data):\\n        return data\\n\\n    @classmethod\\n    def format_to_cv2(cls, data):\\n        return data'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='class RGB:\\n    @classmethod\\n    def cv2_to_format(cls, data):\\n        return cv2.cvtColor(data, cv2.COLOR_BGR2RGB)\\n\\n    @classmethod\\n    def format_to_cv2(cls, data):\\n        return cv2.cvtColor(data, cv2.COLOR_RGB2BGR)\\n\\n\\nWAGGLE_DATA_CONFIG_PATH = Path(\\n    os.environ.get(\"WAGGLE_DATA_CONFIG_PATH\", \"/run/waggle/data-config.json\")\\n)\\n\\n\\ndef read_device_config(path):\\n    config = json.loads(Path(path).read_text())\\n    return {\\n        section[\"match\"][\"id\"]: section\\n        for section in config\\n        if \"id\" in section[\"match\"]\\n    }\\n\\n\\n# TODO use format spec like rgb vs bgr in config file'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='class ImageSample:\\n    data: numpy.ndarray\\n    timestamp: int\\n    format: Union[BGR, RGB]\\n\\n    def __init__(self, data, timestamp, format):\\n        self.format = format\\n        self.data = self.format.cv2_to_format(data)\\n        self.timestamp = timestamp\\n\\n    def save(self, path: PathLike):\\n        path = Path(path)\\n        data = self.format.format_to_cv2(self.data)\\n        cv2.imwrite(str(path), data)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def _repr_html_(self):\\n        data = self.format.format_to_cv2(self.data)\\n        ok, buf = cv2.imencode(\".png\", data)\\n        if not ok:\\n            raise RuntimeError(\"could not encode image\")\\n        b64data = b64encode(buf.ravel()).decode()\\n        return f\\'<img src=\"data:image/png;base64,{b64data}\" />\\''),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='class VideoSample:\\n    path: str\\n    timestamp: int\\n\\n    def __init__(self, path, timestamp, format=RGB):\\n        self.format = format\\n        self.path = path\\n        self.timestamp = timestamp\\n        self.capture = None'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def __enter__(self):\\n        self.capture = cv2.VideoCapture(self.path)\\n        if not self.capture.isOpened():\\n            raise RuntimeError(\\n                f\"unable to open video capture for file {self.path!r}\"\\n            )\\n        self.fps = self.capture.get(cv2.CAP_PROP_FPS)\\n        if self.fps > 100.:\\n            self.fps = 0.\\n            logger.debug(f\\'pywaggle cannot calculate timestamp because the fps ({self.fps}) is too high.\\')\\n            self.timestamp_delta = 0\\n        else:\\n            self.timestamp_delta = 1 / self.fps\\n        self._frame_count = 0\\n        return self'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def __exit__(self, exc_type, exc_val, exc_tb):\\n        if self.capture.isOpened():\\n            self.capture.release()\\n\\n    def __iter__(self):\\n        self._frame_count = 0\\n        return self'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def __next__(self):\\n        if self.capture == None or not self.capture.isOpened():\\n            raise RuntimeError(\"video is not opened. use the Python WITH statement to open the video\")\\n        ok, data = self.capture.read()\\n        if not ok or data is None:\\n            raise StopIteration\\n        # timestamp must be an integer in nanoseconds\\n        approx_timestamp = self.timestamp + int(self.timestamp_delta * self._frame_count * 1e9)\\n        self._frame_count += 1\\n        return ImageSample(data=data, timestamp=approx_timestamp, format=self.format)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def resolve_device(device):\\n    if isinstance(device, Path):\\n        return resolve_device_from_path(device)\\n    # objects that are not paths or strings are considered already resolved\\n    if not isinstance(device, str):\\n        return device\\n    match = re.match(r\"([A-Za-z0-9]+)://(.*)$\", device)\\n    # non-url like paths refer to data shim devices\\n    if match is None:\\n        return resolve_device_from_data_config(device)\\n    # return file:// urls as path\\n    if match.group(1) == \"file\":\\n        return resolve_device_from_path(Path(match.group(2)))\\n    # return other urls as-is\\n    return device\\n\\n\\ndef resolve_device_from_path(path):\\n    return str(path.absolute())'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def resolve_device_from_data_config(device):\\n    config = read_device_config(WAGGLE_DATA_CONFIG_PATH)\\n    section = config.get(device)\\n    if section is None:\\n        raise KeyError(f\"no device found {device!r}\")\\n    try:\\n        return section[\"handler\"][\"args\"][\"url\"]\\n    except KeyError:\\n        raise KeyError(f\"missing .handler.args.url field for device {device!r}.\")'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='class Camera:\\n    INPUT_TYPE_FILE = \"file\"\\n    INPUT_TYPE_OTHER = \"other\"\\n\\n    def __init__(self, device=0, format=RGB):\\n        self.capture = _Capture(resolve_device(device), format)\\n        match = re.match(r\"([A-Za-z0-9]+)://(.*)$\", device)\\n        if match is not None and match.group(1) == \"file\":\\n            self.input_type = self.INPUT_TYPE_FILE\\n        else:\\n            self.input_type = self.INPUT_TYPE_OTHER'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content=\"def __enter__(self):\\n        if self.input_type == self.INPUT_TYPE_FILE:\\n            logger.info(f'input is a file. the background thread disabled for grabbing frames')\\n            self.capture.enable_daemon = False\\n        else:\\n            self.capture.enable_daemon = True\\n        self.capture.__enter__()\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.capture.__exit__(exc_type, exc_val, exc_tb)\\n\\n    def snapshot(self):\\n        with self.capture:\\n            return self.capture.snapshot()\\n\\n    def stream(self):\\n        with self.capture:\\n            yield from self.capture.stream()\"),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def record(self, duration, file_path=\"./sample.mp4\", skip_second=1):\\n        return self.capture.record(duration, file_path, skip_second)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='class _Capture:\\n    def __init__(self, device, format):\\n        self.device = device\\n        self.format = format\\n        self.context_depth = 0\\n        self.enable_daemon = False\\n        self.daemon_need_to_stop = threading.Event()\\n        self._ready_for_next_frame = threading.Event()\\n        self.daemon = threading.Thread(target=self._run, daemon=True)\\n        self.lock = threading.Lock()'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def __enter__(self):\\n        if self.context_depth == 0:\\n            self.capture = cv2.VideoCapture(self.device)\\n            if not self.capture.isOpened():\\n                raise RuntimeError(\\n                    f\"unable to open video capture for device {self.device!r}\"\\n                )\\n            # spin up a thread to keep up with the camera frame rate\\n            if self.enable_daemon:\\n                self.daemon_need_to_stop.clear()\\n                self.daemon.start()\\n        self.context_depth += 1\\n        return self'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content=\"def __exit__(self, exc_type, exc_val, exc_tb):\\n        self.context_depth -= 1\\n        if self.context_depth == 0:\\n            if self.enable_daemon:\\n                self.daemon_need_to_stop.set()\\n            self.capture.release()\\n    \\n    def _run(self):\\n        # we sleep slighly shorter than FPS to drain the buffer efficiently\\n        # NOTE: OpenCV's FPS get function is inaccurate as a USB webcam gives 1 FPS while\\n        #       a RTSP stream returns 180000. none of them are correct. therefore, we cannot\\n        #       decide the sleep time based on obtained FPS\\n        # fps = self.capture.get(cv2.CAP_PROP_FPS)\\n        sleep = 0.01\\n        # if fps > 0 and fps < 100:\"),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='#    sleep = 1 / (fps + 1)\\n        # logging.debug(f\\'camera FPS is {fps}. the background thread sleeps {sleep} seconds in between grab()\\')\\n        while not self.daemon_need_to_stop.is_set():\\n            try:\\n                self.lock.acquire()\\n                ok = self.capture.grab()\\n                if not ok:\\n                    raise RuntimeError(\"failed to grab a frame\")\\n                self.timestamp = get_timestamp()\\n            finally:\\n                self.lock.release()\\n            self._ready_for_next_frame.set()\\n            time.sleep(sleep)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def grab_frame(self):\\n        if self.daemon.is_alive():\\n            if not self._ready_for_next_frame.wait(timeout=10.):\\n                raise RuntimeError(\"failed to grab a frame from the background thread: timed out\")\\n            self._ready_for_next_frame.clear()\\n            try:\\n                self.lock.acquire(timeout=1)\\n                timestamp = self.timestamp\\n                ok, data = self.capture.retrieve()\\n                if not ok:\\n                    raise RuntimeError(\"failed to retrieve the taken snapshot\")\\n            finally:\\n                self.lock.release()\\n            return ImageSample(data=data, timestamp=timestamp, format=self.format)\\n        else:'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='ok = self.capture.grab()\\n            if not ok:\\n                raise RuntimeError(\"failed to take a snapshot\")\\n            timestamp = get_timestamp()\\n            ok, data = self.capture.retrieve()\\n            if not ok:\\n                raise RuntimeError(\"failed to retrieve the taken snapshot\")\\n            return ImageSample(data=data, timestamp=timestamp, format=self.format)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def snapshot(self):\\n        return self.grab_frame()\\n\\n    def stream(self):\\n        try:\\n            while True:\\n                yield self.grab_frame()\\n        except:\\n            pass'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def record(self, duration, file_path=\"./sample.mp4\", skip_second=1):\\n        if which(\"ffmpeg\") == None:\\n            raise RuntimeError(\"ffmpeg does not exist to record video. please install ffmpeg\")\\n        if self.context_depth > 0:\\n            raise RuntimeError(f\\'the stream {self.device} is already open. please close first or use without the Python\\\\\\'s WITH statement\\')\\n        if isinstance(self.device, str) and self.device.startswith(\"rtsp\"):\\n            c = ffmpeg.input(self.device, rtsp_transport=\"tcp\", ss=skip_second)\\n        else:\\n            c = ffmpeg.input(self.device, ss=skip_second)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='c = ffmpeg.output(c, file_path, codec=\"copy\", f=\\'mp4\\', t=duration).overwrite_output()\\n        timestamp = get_timestamp()\\n        _, stderr = ffmpeg.run(c, quiet=True)\\n        if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\\n            return VideoSample(path=file_path, timestamp=timestamp)\\n        else:\\n            raise RuntimeError(f\\'error while recording: {stderr}\\')'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='class ImageFolder:\\n    available_formats = {\".jpg\", \".jpeg\", \".png\"}\\n\\n    def __init__(self, root, format=RGB, shuffle=False):\\n        self.files = sorted(\\n            p.absolute()\\n            for p in Path(root).glob(\"*\")\\n            if p.suffix in self.available_formats\\n        )\\n        self.format = format\\n        if shuffle:\\n            random.shuffle(self.files)\\n\\n    def __len__(self):\\n        return len(self.files)\\n\\n    def __getitem__(self, i):\\n        data = cv2.imread(str(self.files[i]))\\n        timestamp = Path(self.files[i]).stat().st_mtime_ns\\n        return ImageSample(data=data, timestamp=timestamp, format=self.format)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/vision.py'}, page_content='def __repr__(self):\\n        return f\"ImageFolder{self.files!r}\"'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/__init__.py'}, page_content='# Maintaining backwards compatibility for now.\\nfrom .data_shim import open_data_source'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/audio.py'}, page_content='from os import PathLike\\nfrom pathlib import Path\\nimport numpy\\nimport soundfile\\nfrom typing import NamedTuple\\nimport random\\nfrom base64 import b64encode\\nfrom io import BytesIO\\nfrom .timestamp import get_timestamp'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/audio.py'}, page_content='class AudioSample(NamedTuple):\\n    data: numpy.ndarray\\n    samplerate: int\\n    timestamp: int\\n\\n    def save(self, path: PathLike):\\n        path = Path(path)\\n        soundfile.write(str(path), self.data, self.samplerate)\\n\\n    def _repr_html_(self):\\n        with BytesIO() as buf:\\n            soundfile.write(\\n                buf, self.data, self.samplerate, format=\"flac\", closefd=False\\n            )\\n            b64data = b64encode(buf.getvalue()).decode()\\n        return f\"\"\"\\n<audio controls=\"controls\" autobuffer=\"autobuffer\">\\n<source src=\"data:audio/wav;base64,{b64data}\" />\\n</audio>\\n\"\"\"'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/audio.py'}, page_content='class Microphone:\\n    def __init__(self, samplerate=48000, channels=1, name=None):\\n        import soundcard\\n\\n        self.microphone = soundcard.default_microphone()\\n        self.samplerate = samplerate\\n        self.channels = channels\\n        self.name = name\\n\\n    def record(self, duration):\\n        timestamp = get_timestamp()\\n        data = self.microphone.record(\\n            samplerate=self.samplerate,\\n            numframes=int(duration * self.samplerate),\\n            channels=self.channels,\\n        )\\n        return AudioSample(data, self.samplerate, timestamp=timestamp)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/audio.py'}, page_content='class AudioFolder:\\n    available_formats = {\".\" + s.lower() for s in soundfile.available_formats().keys()}\\n\\n    def __init__(self, root, shuffle=False):\\n        self.files = sorted(\\n            p.absolute()\\n            for p in Path(root).glob(\"*\")\\n            if p.suffix in self.available_formats\\n        )\\n        if shuffle:\\n            random.shuffle(self.files)\\n\\n    def __len__(self):\\n        return len(self.files)\\n\\n    def __getitem__(self, i):\\n        data, samplerate = soundfile.read(str(self.files[i]), always_2d=True)\\n        timestamp = Path(self.files[i]).stat().st_mtime_ns\\n        return AudioSample(data, samplerate, timestamp=timestamp)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/audio.py'}, page_content='def __repr__(self):\\n        return f\"AudioFolder{self.files!r}\"'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/timestamp.py'}, page_content='try:\\n    from time import time_ns as get_timestamp\\nexcept ImportError:\\n    from time import time\\n\\n    def get_timestamp():\\n        return int(time() * 1e9)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='import logging\\nimport numpy as np\\nfrom urllib.request import urlopen\\nfrom threading import Thread, Event\\nfrom queue import Queue, Empty, Full\\nimport time\\nimport os\\nimport socket\\nfrom pathlib import Path\\nimport json\\nimport random\\nimport re\\n\\nlogger = logging.getLogger(__name__)\\n\\ntry:\\n    import cv2\\nexcept ImportError:\\n    logger.warning(\\n        \"cv2 module not found. pywaggle requires this to capture image and video data.\"\\n    )\\n\\n\\n# BUG This *must* be addressed with the behavior written up in the plugin spec.\\n# We don\\'t want any surprises in terms of accuraccy\\ntry:\\n    from time import time_ns\\nexcept ImportError:\\n    logger.warning(\"using backwards compatible implementation of time_ns\")'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='def time_ns():\\n        return int(time.time() * 1e9)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='def cvtColor(bgr_img, pixel_format=\"rgb\"):\\n    if pixel_format == \"rgb\":\\n        return cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\\n    return bgr_img'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='class ImageHandler:\\n    def __init__(self, query, url, pixel_format=\"rgb\"):\\n        self.url = url\\n        self.pixel_format = pixel_format\\n\\n    def get(self, timeout=None):\\n        try:\\n            with urlopen(self.url, timeout=timeout) as f:\\n                data = f.read()\\n                ts = time_ns()\\n                arr = np.frombuffer(data, np.uint8)\\n                bgr_img = cv2.imdecode(arr, cv2.IMREAD_COLOR)\\n                return ts, cvtColor(bgr_img, self.pixel_format)\\n        except socket.timeout:\\n            raise TimeoutError(\"get timed out\")\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, *exc):\\n        pass'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='def video_worker(handler):\\n    try:\\n        while not handler.quit.is_set():\\n            ok, bgr_img = handler.cap.read()\\n            if not ok:\\n                break\\n            img = cvtColor(bgr_img, handler.pixel_format)\\n            item = (time_ns(), img)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='# attempt to add an item to the queue\\n            try:\\n                handler.queue.put_nowait(item)\\n                continue\\n            except Full:\\n                logger.debug(\"video frame queue full. evicting oldest frame...\")\\n            # evict an item from queue\\n            try:\\n                handler.queue.get_nowait()\\n            except Empty:\\n                pass\\n            # queue should have space to add now. (assuming this\\n            # is the only producer adding to this queue)\\n            handler.queue.put_nowait(item)\\n    finally:\\n        handler.cap.release()\\n        handler.released.set()'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='# TODO We need to use a flexible model where the data returned is\\n# extensible. For example, serial data won\\'t really have a good\\n# notion of \"timestamp\". Maybe it\\'s better to not include that.'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='class VideoHandler:\\n    def __init__(self, query, url, pixel_format=\"rgb\"):\\n        self.pixel_format = pixel_format\\n        self.cap = cv2.VideoCapture(url)\\n        if not self.cap.isOpened():\\n            raise RuntimeError(f\\'could not open camera at \"{url}\".\\')\\n        self.queue = Queue(8)\\n        self.quit = Event()\\n        self.released = Event()\\n        # NOTE(sean) no further mutation can be done on VideoHandler state. all\\n        # interaction with cap *must* be done in the worker thread or via queue\\n        # and quit primitives\\n        worker = Thread(target=video_worker, args=(self,), daemon=True)\\n        worker.start()'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='def get(self, timeout=None):\\n        try:\\n            return self.queue.get(timeout=timeout)\\n        except Empty:\\n            raise TimeoutError(\"get timed out\")\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, *exc):\\n        self.quit.set()\\n        self.released.wait()  # <- wait for cleanup in worker thread\\n\\n\\nWAGGLE_DATA_CONFIG_PATH = Path(\\n    os.environ.get(\"WAGGLE_DATA_CONFIG_PATH\", \"/run/waggle/data-config.json\")\\n)\\n\\ntry:\\n    config = json.loads(WAGGLE_DATA_CONFIG_PATH.read_text())\\nexcept FileNotFoundError:\\n    logger.debug(\\n        \"could not find data config file %s. using empty resource list.\",\\n        WAGGLE_DATA_CONFIG_PATH,\\n    )\\n    config = []'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='def dict_is_subset(a, b):\\n    return all(k in b and re.match(b[k], a[k]) for k in a.keys())\\n\\n\\ndef find_all_matches(query):\\n    return [c for c in config if dict_is_subset(query, c[\"match\"])]\\n\\n\\ndef find_match(query):\\n    matches = find_all_matches(query)\\n    if len(matches) == 0:\\n        raise RuntimeError(\"no matches found\")\\n    if len(matches) > 1:\\n        raise RuntimeError(\"multiple devices found\")\\n    return matches[0]\\n\\n\\nhandlers = {\\n    \"image\": ImageHandler,\\n    \"video\": VideoHandler,\\n}\\n\\n\\n# optimizations *could* happen here, on demand...'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/data_shim.py'}, page_content='def open_data_source(**query):\\n    match = find_match(query)\\n    handler = handlers[match[\"handler\"][\"type\"]]\\n    args = match[\"handler\"][\"args\"]\\n    return handler(query, **args)'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/measurements.py'}, page_content='from datetime import datetime\\nimport json\\nimport time'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/measurements.py'}, page_content='class MeasurementsFile:\\n    def __init__(self, filename):\\n        self.records = []\\n\\n        with open(filename, \"r\") as f:\\n            for r in map(json.loads, f):\\n                # 2021-06-25T18:52:15.404690128Z\\n                r[\"timestamp\"] = datetime.strptime(\\n                    r[\"timestamp\"][:26], \"%Y-%m-%dT%H:%M:%S.%f\"\\n                )\\n                self.records.append(r)\\n        self.records.sort(key=lambda r: r[\"timestamp\"])'),\n",
       " Document(metadata={'source': 'pywaggle/src/waggle/data/measurements.py'}, page_content='def play(self, nodelay=False):\\n        if len(self.records) == 0:\\n            return\\n        last_record = self.records[0]\\n        for r in self.records:\\n            delta = r[\"timestamp\"] - last_record[\"timestamp\"]\\n            if not nodelay:\\n                time.sleep(delta.total_seconds())\\n            yield r\\n            last_record = r\\n\\n\\n# MessagePlayer can take a SDR format file and replay the contents\\n# this will help support use cases where someone wants to inject known\\n# data into their plugin from a file.\\n# (think about name?)\\n# other features:\\n# should sort by timestamp / or leave no sort as flag?\\n# should be able to decide starting time'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/plotting_example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Basic Plotting Example\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import sage_data_client\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"First, we\\'ll query the last 7 days of temperature data from W022\\'s BME680 sensor.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df = sage_data_client.query(\\\\n\\', \\'    start=\"-7d\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\",\\\\n\\', \\'        \"vsn\": \"W022\",\\\\n\\', \\'        \"sensor\": \"bme680\",\\\\n\\', \\'    }\\\\n\\', \\')\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"Next, we\\'ll plot a simple line chart.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df.set_index(\"timestamp\").value.plot()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"Finally, we\\'ll plot the temperature distribution.\"]\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.hist(bins=100)\\']\\'\\n\\n\\'code\\' cell: \\'[]\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content=\"'markdown' cell: '['# Geospatial Mapping Example\\\\n', 'Within this example, we walk through how to query for SAGE data, filter our values, and plot maps of the data using Cartopy, Matplotlib, and hvPlot!\\\\n', '\\\\n', '\\\\n', '\\\\n', '\\\\n', '\\\\n', '\\\\n', '\\\\n', '## Imports\\\\n', 'We import our sage_data_client, along with the plotting libraries matplotlib, Cartopy, hvPlot and holoviews.\\\\n', '\\\\n', 'If you have not installed these packages already, make sure to run this line!']'\\n\\n'code' cell: '['!pip3 install matplotlib bokeh holoviews hvplot cartopy pandas metpy']'\"),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'code\\' cell: \\'[\\'import sage_data_client\\\\n\\', \\'from bokeh.models.formatters import DatetimeTickFormatter\\\\n\\', \\'import hvplot.pandas\\\\n\\', \\'import holoviews as hv\\\\n\\', \\'import cartopy.crs as ccrs\\\\n\\', \\'import cartopy.feature as cfeature\\\\n\\', \\'import matplotlib.pyplot as plt\\\\n\\', \\'from metpy.plots import USCOUNTIES\\\\n\\', \\'import pandas as pd\\\\n\\', \\'import warnings\\\\n\\', \\'\\\\n\\', \\'warnings.filterwarnings(\"ignore\")\\\\n\\', \\'hv.extension(\"bokeh\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Query and load data into pandas data frame\\\\n\\', \\'We have two queries we are interested in:\\\\n\\', \\'- Temperature\\\\n\\', \\'- Location data (latitude and longitude of the sensor)\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'code\\' cell: \\'[\\'temperature_df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\",\\\\n\\', \\'    }\\\\n\\', \\')\\\\n\\', \\'\\\\n\\', \\'location_df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"sys.gps.*\",\\\\n\\', \\'    }\\\\n\\', \\')\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Investigate the Temperature Dataframe\\\\n\\', \\'Notice how the dataframe containing temperature data stores the temperature value as `value`, along with several `meta.` fields.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'temperature_df\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content=\"'markdown' cell: '['### Investigate the Location Dataframe\\\\n', 'This dataframe does not have as many metadata fields... but we do have enough to join our two dataframes. Another issue with this dataframe is that the location values are stored as individual rows, when we would ideally like these to be their own columns (ex. latitude and longitude for a given location)']'\\n\\n'code' cell: '['location_df']'\"),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'## Clean Up the Data\\\\n\\', \\'\\\\n\\', \"Let\\'s start the data cleaning process!\\\\n\", \\'\\\\n\\', \\'### Join Latitude and Longitude into the Same Rows\\\\n\\', \\'Our first step is to join our latitude and longitude values into the same row. We start by\\\\n\\', \\'- Subsetting for latitude and longitude in the dataframe\\\\n\\', \\'- Rename the fields accordingly\\\\n\\', \\'- Join the columns based on the timestamp, host, and node\\\\n\\', \\'- Drop any extra columns\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'code\\' cell: \\'[\\'# Subset the latitude values, and rename to latitude\\\\n\\', \"lat = location_df.loc[location_df.name == \\'sys.gps.lat\\']\\\\n\", \"lat_df = lat.rename(columns={\\'value\\':\\'latitude\\'})\\\\n\", \\'\\\\n\\', \\'# Subset the longitude values, and rename to latitude\\\\n\\', \"lon = location_df.loc[location_df.name == \\'sys.gps.lon\\']\\\\n\", \"lon_df = lon.rename(columns={\\'value\\':\\'longitude\\'})\\\\n\", \\'\\\\n\\', \\'# Join the latitude and longitude dataframes, returning a dataframe with shared latitude and longitude information\\\\n\\', \"joined_lats_lons = pd.merge(lat_df, lon_df, on=[\\'timestamp\\', \\'meta.host\\', \\'meta.node\\'],  how=\\'outer\\', )\\\\n\", \\'\\\\n\\', \\'# Filter out unwanted columns\\\\n\\', \"joined_lats_lons = joined_lats_lons[[x for x in'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='joined_lats_lons.columns if ((\\'x\\' not in x) and (\\'y\\' not in x) and (\\'timestamp\\' not in x))]]\\\\n\", \\'\\\\n\\', \\'# Return our dataframe\\\\n\\', \\'joined_lats_lons\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'### Join our Latitude and Longitude Information with the Temperature Dataframe\\\\n\\', \\'Now that we have our location dataframe cleaned up, we can join this with the temperature dataframe, so we know where our temperature values are collected!\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Merge the dataframes using the host and node as the shared fields to join on\\\\n\\', \"df = pd.merge(temperature_df, joined_lats_lons,  on=[\\'meta.host\\', \\'meta.node\\'], how=\\'right\\')\\\\n\", \\'\\\\n\\', \\'# Drop any duplicates, based on the timestamp, host, and node\\\\n\\', \"df_filtered = df.drop_duplicates([\\'timestamp\\', \\'meta.host\\', \\'meta.node\\'])\"]\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'## Run Statistics on our Dataframe\\\\n\\', \"Let\\'s say we are interested in hourly mean temperature... we can calculate that!\"]\\'\\n\\n\\'code\\' cell: \\'[\\'hours = df_filtered.timestamp.dt.hour.unique()\\']\\'\\n\\n\\'code\\' cell: \\'[\\'hourly_mean = df_filtered.groupby([df_filtered.timestamp.dt.hour,\\\\n\\', \"                                   \\'meta.node\\']).mean()\"]\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \\'    fig = plt.figure(figsize=(14,8))\\\\n\\', \\'    ax = plt.subplot(projection=ccrs.PlateCarree())\\\\n\\', \"    s = hourly_mean.loc[hour].plot.scatter(ax=ax, x=\\'longitude\\', y=\\'latitude\\', c=\\'value\\', cmap=\\'Spectral_r\\')\\\\n\", \\'    ax.add_feature(cfeature.LAND)\\\\n\\', \\'    ax.add_feature(cfeature.STATES)\\\\n\\', \\'    ax.add_feature(cfeature.LAKES)\\\\n\\', \\'    ax.set_extent([-125, -66.5, 20, 50])\\\\n\\', \\'    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\\\\n\\', \"                  linewidth=2, color=\\'gray\\', alpha=0.5, linestyle=\\'--\\')\\\\n\", \\'    gl.top_labels = False\\\\n\\', \\'    gl.right_labels = False\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='{hour} UTC\\')\\\\n\", \"    plt.title(f\\'Average Temperature (degF) at \\\\\\\\n {time_label}\\', fontsize=16);\\\\n\", \\'    plt.show()\\\\n\\', \\'    plt.close()\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'### Zoom in on the Chicago Area\\\\n\\', \"It\\'s nice having a national map, but let\\'s zoom into Chicago for a higher resolution view of the sensors around the city and surrounding suburbs.\"]\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \\'    fig = plt.figure(figsize=(14,8))\\\\n\\', \\'    ax = plt.subplot(projection=ccrs.PlateCarree())\\\\n\\', \"    s = hourly_mean.loc[hour].plot.scatter(ax=ax, x=\\'longitude\\', y=\\'latitude\\', c=\\'value\\', cmap=\\'Spectral_r\\', vmin=28, vmax=38)\\\\n\", \\'    ax.add_feature(cfeature.LAND)\\\\n\\', \\'    ax.add_feature(cfeature.STATES)\\\\n\\', \\'    ax.add_feature(cfeature.LAKES)\\\\n\\', \\'    ax.add_feature(USCOUNTIES)\\\\n\\', \\'    gl = ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\\\\n\\', \"                  linewidth=2, color=\\'gray\\', alpha=0.5, linestyle=\\':\\')\\\\n\", \\'    gl.top_labels = False\\\\n\\', \\'    gl.right_labels = False\\\\n\\', \\'    ax.set_extent([-89, -87, 41, 43])\\\\n\\', \"    time_label ='),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    plt.title(f\\'Average Temperature (degF) at \\\\\\\\n {time_label}\\', fontsize=16);\\\\n\", \\'    plt.show()\\\\n\\', \\'    plt.close()\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content=\"'markdown' cell: '['### Plot Interactive National Maps\\\\n', 'We can use hvPlot here to plot interactive national maps']'\"),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    display(hourly_mean.loc[hour].hvplot.points(x=\\'longitude\\',\\\\n\", \"                                                y=\\'latitude\\',\\\\n\", \"                                                color=\\'value\\',\\\\n\", \"                                                cmap=\\'Spectral_r\\',\\\\n\", \\'                                                geo=True,\\\\n\\', \"                                                tiles=\\'CartoLight\\',\\\\n\", \"                                                title=f\\'Average Temperature (degF) at {time_label}\\',\\\\n\", \"'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='clabel=\\'Temperature (degF)\\',\\\\n\", \\'                                                crs=ccrs.PlateCarree()))\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content=\"'markdown' cell: '['### Plot an Interactive Regional Map of Chicago\\\\n', 'And the same for the region around Chicago\\\\n', '\\\\n', 'We start first by subsetting points out of our dataframe around the Northern Illinois area.']'\\n\\n'code' cell: '['illinois_points = hourly_mean.loc[(hourly_mean.latitude > 41.) & (hourly_mean.longitude > -89)]']'\\n\\n'markdown' cell: '['Now that we have this, we loop through and plot our data!']'\"),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='\\'code\\' cell: \\'[\\'for hour in hours:\\\\n\\', \"    time_label = df_filtered.timestamp.min().strftime(f\\'%b %d %Y {hour} UTC\\')\\\\n\", \"    display(illinois_points.loc[hour].hvplot.points(x=\\'longitude\\',\\\\n\", \"                                                    y=\\'latitude\\',\\\\n\", \"                                                    color=\\'value\\',\\\\n\", \"                                                    cmap=\\'Spectral_r\\',\\\\n\", \"                                                    tiles=\\'CartoLight\\',\\\\n\", \\'                                                    geo=True,\\\\n\\', \"                                                    title=f\\'Average Temperature (degF) at {time_label}\\',\\\\n\", \"'),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content='clabel=\\'Temperature (degF)\\'))\"]\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/geospatial-mapping-example.ipynb'}, page_content=\"'code' cell: '[]'\"),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/sage-interactive-plot.ipynb'}, page_content=\"'markdown' cell: '['# Interactive Plotting Example\\\\n', 'Within this example, we walk through how to query for SAGE data, filter our values, and plot using the hvPlot interactive plotting library!']'\\n\\n'markdown' cell: '['## Imports and Query\\\\n', 'We import our `sage_data_client`, along with the plotting libraries `hvPlot` and `holoviews`\\\\n', '\\\\n', '**If you have not installed these packages already, make sure to run this line!**']'\\n\\n'code' cell: '['!pip3 install matplotlib bokeh holoviews hvplot']'\"),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/sage-interactive-plot.ipynb'}, page_content='\\'code\\' cell: \\'[\\'import sage_data_client\\\\n\\', \\'from bokeh.models.formatters import DatetimeTickFormatter\\\\n\\', \\'import hvplot.pandas\\\\n\\', \\'import holoviews as hv\\\\n\\', \\'import warnings\\\\n\\', \\'\\\\n\\', \\'# query and load data into pandas data frame\\\\n\\', \\'df = sage_data_client.query(\\\\n\\', \\'    start=\"-3h\",\\\\n\\', \\'    filter={\\\\n\\', \\'        \"name\": \"env.temperature\"\\\\n\\', \\'    }\\\\n\\', \\')\\\\n\\', \\'\\\\n\\', \\'warnings.filterwarnings(\"ignore\")\\\\n\\', \\'hv.extension(\"bokeh\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Clean our Data\\\\n\\', \\'When we first visualize our dataset (temperature), notice how we have some **very** low values (< -100 degrees Fahrenheit).\\']\\'\\n\\n\\'code\\' cell: \\'[\\'df.value.plot();\\']\\''),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/sage-interactive-plot.ipynb'}, page_content=\"'markdown' cell: '['We can flag these values as bad data using the following:']'\\n\\n'code' cell: '['df = df[df.value > -100]']'\\n\\n'markdown' cell: '['Now, if we visualize our data again, notice how we do not have these abnormally low values.']'\\n\\n'code' cell: '['df.value.plot();']'\\n\\n'markdown' cell: '['## Create an Interactive Plot\\\\n', 'We can use hvPlot to plot our data! Instead of using `.plot()` like we did before, which creates a matplotlib static plot, we can use `.hvplot()` which creates an interactive plot. ']'\"),\n",
       " Document(metadata={'source': 'sage-data-client/examples/contrib/sage-interactive-plot.ipynb'}, page_content='\\'code\\' cell: \\'[\\'# Set\\\\n\\', \\'formatter = DatetimeTickFormatter(days=\"%d %b %Y \\\\\\\\n %H:%M UTC\",\\\\n\\', \\'                                  hours=\"%d %b %Y \\\\\\\\n %H:%M UTC\",\\\\n\\', \\'                                  minutes=\"%d %b %Y \\\\\\\\n %H:%M UTC\",)\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \"df.hvplot.line(x=\\'timestamp\\',\\\\n\", \"               y=\\'value\\',\\\\n\", \"               ylabel=\\'Temperature (degF)\\',\\\\n\", \"               xlabel=\\'Time\\',\\\\n\", \"               by=\\'meta.vsn\\',\\\\n\", \\'               groupby=[\"meta.sensor\"],\\\\n\\', \\'               xformatter=formatter, \\\\n\\', \"               color=hv.Palette(\\'Category20\\'),\\\\n\", \\'               height=400,\\\\n\\', \\'               width=600)\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine splits\n",
    "combined_splits = md_splits + py_splits + ipynb_splits\n",
    "combined_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding & Vector Database Creation\n",
    "> Try different model and VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model was already installed through rag-basic.ipynb\n",
    "ollama_emb = OllamaEmbeddings(model='mxbai-embed-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 1.89 s, sys: 374 ms, total: 2.26 s\n",
      "Wall time: 8min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Creates a Chroma VB from a list of Documents\n",
    "# Make sure to run `Ollama serve` in terminal first to host\n",
    "# ~399 embeddings\n",
    "vectordb = Chroma.from_documents(combined_splits, ollama_emb)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Testing Different Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model='llama3.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever(search_type='similarity', search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='Using the following this context: {context}. Respond to this question: {question}'))])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Simple template and prompt\n",
    "simple_template = \"Using the following this context: {context}. Respond to this question: {question}\"\n",
    "simple_prompt = ChatPromptTemplate.from_template(simple_template)\n",
    "simple_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sage project appears to be a distributed software-defined sensor network, as stated in the context provided.\n",
      "\n",
      "\n",
      "CPU times: user 79.5 ms, sys: 42.8 ms, total: 122 ms\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Simple Chain About Sage\n",
    "simple_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in simple_chain.stream(\"What is the Sage project?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You didn't ask a question about Chirpstack or the provided text. You appeared to be presenting a block of text and then asked \"Why is the sky blue?\", which isn't related to the content.\n",
      "\n",
      "If you'd like to ask something specific, I'd be happy to help with your question about Chirpstack!\n",
      "\n",
      "\n",
      "CPU times: user 189 ms, sys: 96.4 ms, total: 286 ms\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Simple Chain NOT About Sage\n",
    "simple_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | simple_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in simple_chain.stream(\"Why is the sky blue?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enhanced Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='You are an assistant in question-answering tasks.\\nUse the following pieces of context to answer the question at the end.\\nIf you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\nAlways say \"thanks for asking!\" at the end of the answer.\\n\\n{context}\\n\\nQuestion: {question}'))])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Enhanced template and prompt\n",
    "enhanced_template = \"\"\"You are an assistant in question-answering tasks.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "enhanced_prompt = ChatPromptTemplate.from_template(enhanced_template)\n",
    "enhanced_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sage project is a distributed software-defined sensor network, which enables fast and efficient analysis of large volumes of data from geographically distributed sensors, such as cameras, microphones, and weather stations. It uses machine learning algorithms to process data and transmits results over the network to central computer servers.\n",
      "\n",
      "thanks for asking!\n",
      "\n",
      "\n",
      "CPU times: user 147 ms, sys: 65.9 ms, total: 213 ms\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Enhanced Chain About Sage\n",
    "enhanced_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | enhanced_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in enhanced_chain.stream(\"What is the Sage project?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. \n",
      "\n",
      "Thanks for asking!\n",
      "\n",
      "\n",
      "CPU times: user 95.6 ms, sys: 56 ms, total: 152 ms\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Enhanced Chain NOT About Sage\n",
    "enhanced_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | enhanced_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in enhanced_chain.stream(\"Why is the sky blue?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spanish Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant in question-answering tasks that speaks Spanish.\\nUse the following pieces of context to answer the question at the end.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nGive your answer in Spanish.\\n\\n{context}\\n\\nQuestion: {question}\"))])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Enhanced template and prompt\n",
    "spanish_template = \"\"\"You are an assistant in question-answering tasks that speaks Spanish.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Give your answer in Spanish.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "spanish_prompt = ChatPromptTemplate.from_template(spanish_template)\n",
    "spanish_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La pregunta es sobre el proyecto \"Sage\". Según la información proporcionada, Sage es una red de sensores definida por software distribuida. Es un sistema geográficamente distribuido que incluye cámaras, microfones y estaciones meteorológicas y de calidad del aire, entre otros. El objetivo de Sage es explorar nuevas técnicas para aplicar algoritmos de aprendizaje automático a los datos de sensores inteligentes y crear software reutilizable que pueda ejecutar programas en la computadora embebida y transmitir los resultados a servidores de ordenadores centrales.\n",
      "\n",
      "En resumen, el proyecto Sage es una red de sensores distribuida que utiliza técnicas de aprendizaje automático para analizar datos de diferentes fuentes y proporcionar información valiosa para científicos que estudian la impacto de fenómenos naturales y urbanización en ecosistemas naturales y infraestructura urbana.\n",
      "\n",
      "\n",
      "CPU times: user 361 ms, sys: 132 ms, total: 493 ms\n",
      "Wall time: 2min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Spanish Chain About Sage\n",
    "spanish_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | spanish_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in spanish_chain.stream(\"What is the Sage project?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Testing More Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waggle provides a number of interfaces which other computing and HPC systems can build on top of, allowing for monitoring data from the edge and triggering actions when values exceed a threshold or an unusual event is detected. It also enables running plugins @ the edge through its operating system, which includes k3s to create a protected & isolated run-time environment.\n",
      "\n",
      "thanks for asking!\n",
      "\n",
      "\n",
      "CPU times: user 186 ms, sys: 126 ms, total: 312 ms\n",
      "Wall time: 3min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Enhanced Chain\n",
    "enhanced_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | enhanced_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in enhanced_chain.stream(\"What is the role of Waggle in Sage?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided document, it's recommended that you start by reading the Sage Overview in the Sage docs to understand what Sage is and how it might connect to your work. It also suggests ensuring that your team members have accounts in the Sage Portal, and reviewing the Edge apps tutorial in the Sage docs.\n",
      "\n",
      "Additionally, it's mentioned that during the Hackathon, they will review how to use the portal and parts of the Edge app tutorial. However, having done some preliminary work ahead of time will allow more time for their team to provide support for your unique application.\n",
      "\n",
      "So, to get in contact with the Sage team, you should start by following these steps:\n",
      "\n",
      "1. Read the Sage Overview in the Sage docs.\n",
      "2. Ensure that your team members have accounts in the Sage Portal.\n",
      "3. Review the Edge apps tutorial in the Sage docs.\n",
      "\n",
      "By doing so, you'll be well-prepared for the Hackathon and can get support from their team.\n",
      "\n",
      "Thanks for asking!\n",
      "\n",
      "\n",
      "CPU times: user 403 ms, sys: 181 ms, total: 584 ms\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Enhanced Chain\n",
    "enhanced_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | enhanced_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in enhanced_chain.stream(\"How can I get in contact with the Sage team?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing prompt and receiver for better experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template='You are an expert on the Sage project, and you only answer any question related to it.\\nUse the following pieces of context about the Sage project to answer the question at the end.\\nProvide as much detail as possible in answering the question from the provided context.\\nYour answer should be super informative.\\nIf you don\\'t know the answer or the question is unrelated to Sage, just say that you don\\'t know, don\\'t try to make up an answer.\\nIf asked to provide code, only generate code provided in the context.\\nAlways say \"Thanks for asking!\" at the end of the answer.\\n\\n{context}\\n\\nQuestion: {question}'))])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sage template and prompt\n",
    "## Changed to be extremely specific\n",
    "sage_template = \"\"\"You are an expert on the Sage project, and you only answer any question related to it.\n",
    "Use the following pieces of context about the Sage project to answer the question at the end.\n",
    "Provide as much detail as possible in answering the question from the provided context.\n",
    "Your answer should be super informative.\n",
    "If you don't know the answer or the question is unrelated to Sage, just say that you don't know, don't try to make up an answer.\n",
    "If asked to provide code, only generate code provided in the context.\n",
    "Always say \"Thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "sage_prompt = ChatPromptTemplate.from_template(sage_template)\n",
    "sage_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_retriever = vectordb.as_retriever(search_type='similarity', search_kwargs={\"k\":15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, according to the provided context, the Wild Sage Node (or Wild Waggle Node) is mentioned as a custom built weather-proof enclosure intended for remote outdoor installation. It features software and hardware resilience via a custom operating system and custom circuit board.\n",
      "\n",
      "Additionally, there's an example of using Sage Data Client to watch the data stream and print nodes where the internal temperature exceeds a threshold. This suggests that Sage nodes are used in various settings, possibly for monitoring environmental conditions or other types of data.\n",
      "\n",
      "The Node installation manual (https://sagecontinuum.org/docs/installation-manuals/wsn-manual) also mentions details on how to install and use Wild Sage Nodes, which implies that these devices have practical applications in real-world scenarios.\n",
      "\n",
      "Thanks for asking!\n",
      "\n",
      "\n",
      "CPU times: user 335 ms, sys: 129 ms, total: 464 ms\n",
      "Wall time: 5min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Sage Chain\n",
    "sage_chain = (\n",
    "    {\"context\": sage_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | sage_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in sage_chain.stream(\"Are there any specific examples on how Sage nodes are used?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, here is a breakdown of the Sage infrastructure:\n",
      "\n",
      "1. **Waggle**: A computing platform that provides a number of interfaces which other computing and HPC systems can build on top.\n",
      "\t* Allows for cloud compute & HPC on edge data\n",
      "2. **Sage Data Client**: An official Python API client for interacting with the Sage data service.\n",
      "\t* Provides a simple query function to talk to the data API\n",
      "\t* Returns results in an easy-to-use Pandas DataFrame\n",
      "3. **Globus**: A login system used by Sage, which requires organization credentials for access.\n",
      "4. **ECR (Elastic Container Registry)**: A container registry service where apps can be published and deployed.\n",
      "5. **Nodes**: Physical devices that support machine learning frameworks and store data in environmental testbeds or urban environments.\n",
      "6. **Sage Data Service**: A platform that provides a data API for querying and retrieving data from nodes.\n",
      "\n",
      "The Sage infrastructure seems to be designed for managing large volumes of sensor data, providing computing resources for edge analysis, and enabling scientists to build their own intelligent sensor networks.\n",
      "\n",
      "\n",
      "CPU times: user 445 ms, sys: 142 ms, total: 586 ms\n",
      "Wall time: 3min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Sage Chain\n",
    "sage_chain = (\n",
    "    {\"context\": sage_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | sage_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in sage_chain.stream(\"Can you give me a breakdown of the Sage infrastructure?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Testing Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the code snippet from the context:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from waggle.plugin import Plugin\n",
      "from waggle.data.vision import Camera\n",
      "\n",
      "def main():\n",
      "    with Plugin() as plugin:\n",
      "        # open camera and take snapshot\n",
      "        with Camera() as camera:\n",
      "            snapshot = camera.snapshot()\n",
      "\n",
      "            # compute mean color\n",
      "            mean_color = compute_mean_color(snapshot.data)\n",
      "\n",
      "            # publish mean color\n",
      "            plugin.publish(\"color.mean.r\", mean_color[0], timestamp=snapshot.timestamp)\n",
      "            plugin.publish(\"color.mean.g\", mean_color[1], timestamp=snapshot.timestamp)\n",
      "            plugin.publish(\"color.mean.b\", mean_color[2], timestamp=snapshot.timestamp)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n",
      "```\n",
      "\n",
      "This code captures an image using the `Camera` class, computes its mean color, and publishes it with a message name of \"color.mean.r\", \"color.mean.g\", and \"color.mean.b\" respectively. \n",
      "\n",
      "Please note that you might need to install the required libraries (`numpy`, `waggle.plugin`, and `waggle.data.vision`) in your environment for this code to work.\n",
      "\n",
      "Thanks for asking!\n",
      "\n",
      "\n",
      "CPU times: user 533 ms, sys: 246 ms, total: 780 ms\n",
      "Wall time: 6min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Sage Chain\n",
    "sage_chain = (\n",
    "    {\"context\": sage_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | sage_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in sage_chain.stream(\"Can you provide me code that captures an image and publishes it?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\") # Result: Perfect code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the two plugins written in Python:\n",
      "\n",
      "**Plugin 1: `camera_plugin.py`**\n",
      "```python\n",
      "import numpy as np\n",
      "from waggle.plugin import Plugin\n",
      "from waggle.data.vision import Camera\n",
      "\n",
      "def count_pedestrians(image):\n",
      "    # TO DO: implement pedestrian counting function\n",
      "    return np.random.randint(0, 100)  # placeholder for now\n",
      "\n",
      "with Plugin() as plugin:\n",
      "    camera = Camera()\n",
      "    snapshot = camera.snapshot()\n",
      "    pedestrians = count_pedestrians(snapshot.data)\n",
      "    plugin.publish(\"number.pedestrians\", pedestrians)\n",
      "```\n",
      "\n",
      "**Plugin 2: `subscriber_plugin.py`**\n",
      "```python\n",
      "import numpy as np\n",
      "from waggle.plugin import Plugin, Subscriber\n",
      "from waggle.data.vision import Camera\n",
      "\n",
      "def count_pedestrians(image):\n",
      "    # TO DO: implement pedestrian counting function\n",
      "    return np.random.randint(0, 100)  # placeholder for now\n",
      "\n",
      "with Plugin() as plugin:\n",
      "    subscriber = Subscriber()\n",
      "    while True:\n",
      "        data = subscriber.subscribe(\"number.pedestrians\")\n",
      "        if data is not None:\n",
      "            snapshot = Camera().snapshot()\n",
      "            pedestrians = count_pedestrians(snapshot.data)\n",
      "            print(f\"Image with most pedestrians: {data} at timestamp: {subscriber.timestamp}\")\n",
      "```\n",
      "Note that the `count_pedestrians` function is a placeholder and should be replaced with actual pedestrian counting logic. Also, this code assumes that you have already installed the necessary dependencies (e.g., ffmpeg for video recording).\n",
      "\n",
      "\n",
      "CPU times: user 565 ms, sys: 244 ms, total: 808 ms\n",
      "Wall time: 5min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "long_question = \"\"\"\n",
    "Can you provide me all the necessary code for two plugins? \n",
    "The first plugin captures an image from the camera, processes it with the function “count_pedestrians” and publishes the number with the name “number.pedestrians”. \n",
    "The second plugin subscribes to that data stream, captures a picture, runs the same “count_pedestrians” function, and prints out which image had the most pedestrians with the associated timestamp. \n",
    "Write it as two separate files. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## Sage Chain\n",
    "sage_chain = (\n",
    "    {\"context\": sage_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | sage_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in sage_chain.stream(long_question):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\") # Result: almost perfect. Did not include timestamps and no comparison logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example of how you could implement such a plugin:\n",
      "\n",
      "```python\n",
      "class BirdWatcherPlugin(Plugin):\n",
      "    def __init__(self, name=\"BirdWatcher\"):\n",
      "        self.name = name\n",
      "        self.bird_database = None\n",
      "\n",
      "    def setup(self, camera):\n",
      "        self.bird_database = DataRepository.get_instance().get_bird_database()\n",
      "\n",
      "    def process_sample(self, sample):\n",
      "        # Compare the current microphone data with stored bird recordings\n",
      "        similar_birds = []\n",
      "        for bird in self.bird_database:\n",
      "            if compare_birds(sample.data, bird['audio_data']):\n",
      "                similar_birds.append(bird['name'])\n",
      "\n",
      "        # Publish the results\n",
      "        plugin.upload_string(f\"Detected {len(similar_birds)} birds: {', '.join(similar_birds)}\")\n",
      "```\n",
      "\n",
      "You would need to implement `compare_birds` function to compare two audio samples and return True if they are similar, False otherwise.\n",
      "\n",
      "Also, this example assumes that you have a `DataRepository.get_instance()` method to get the instance of the Data Repository and a `get_bird_database()` method in it to get the list of stored bird recordings.\n",
      "\n",
      "Please note that this is just an example code snippet. The actual implementation would depend on your specific requirements and data formats.\n",
      "\n",
      "\n",
      "CPU times: user 527 ms, sys: 203 ms, total: 730 ms\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "long_question = \"\"\"\n",
    "Write me a plugin that accesses stored bird recordings and compares them to what the current node is picking up on a microphone to detect if there are any similar birds. \n",
    "These results are published. You may assume the function to compare is given as compare_birds.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## Sage Chain\n",
    "sage_chain = (\n",
    "    {\"context\": sage_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | sage_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in sage_chain.stream(long_question):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\") # Result: Gave rough outline and made up a few functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm afraid I can't do that. Creating a fully-fledged first-person shooter (FPS) game is a complex task that requires significant expertise in programming, computer graphics, game design, and sound design. \n",
      "\n",
      "However, if you'd like, I can provide some general information or point you to resources about FPS games.\n",
      "\n",
      "Alternatively, if you have any specific questions about how to implement certain aspects of an FPS game (e.g., collision detection, camera control, etc.), I'll do my best to help with that!\n",
      "\n",
      "\n",
      "CPU times: user 306 ms, sys: 188 ms, total: 493 ms\n",
      "Wall time: 6min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Sage Chain\n",
    "sage_chain = (\n",
    "    {\"context\": sage_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | sage_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in sage_chain.stream(\"Can you provide me the code to make a first-person shooter video game?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an example of how to create a small blue box using Python and the Tkinter library:\n",
      "\n",
      "```python\n",
      "import tkinter as tk\n",
      "\n",
      "# Create the main window\n",
      "root = tk.Tk()\n",
      "\n",
      "# Set the size of the window\n",
      "root.geometry(\"200x100\")\n",
      "\n",
      "# Create a blue rectangle (box)\n",
      "blue_box = tk.Frame(root, bg=\"blue\", width=50, height=20)\n",
      "\n",
      "# Place the blue box at position (0, 30) on the window\n",
      "blue_box.place(x=0, y=30)\n",
      "\n",
      "# Run the application\n",
      "root.mainloop()\n",
      "```\n",
      "\n",
      "This code will create a small blue rectangle in a window.\n",
      "\n",
      "\n",
      "CPU times: user 368 ms, sys: 224 ms, total: 592 ms\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Sage Chain\n",
    "sage_chain = (\n",
    "    {\"context\": sage_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | sage_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in sage_chain.stream(\"Can you provide me code that makes a small blue box in python?\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\") # Still generated code despite being advised not to provide code for unrelated Sage questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how you can do it. I will use the `sage_data_client` library to query the data from the past 3 hours.\n",
      "\n",
      "```python\n",
      "import sage_data_client\n",
      "import pandas as pd\n",
      "import holoviews as hv\n",
      "from holoviews import opts\n",
      "\n",
      "# Set up Holoviews with a suitable backend (in this case, Plotly)\n",
      "hv.extension('bokeh')\n",
      "opts.defaults(\n",
      "    opts.Scatter(line_width=2),\n",
      ")\n",
      "\n",
      "temperature_df = sage_data_client.query(\n",
      "    start=\"-3h\",\n",
      "    filter={\n",
      "        \"name\": \"env.temperature\",\n",
      "        \"meta.vsn\": \"W0B0\"\n",
      "    }\n",
      ")\n",
      "\n",
      "pressure_df = sage_data_client.query(\n",
      "    start=\"-3h\",\n",
      "    filter={\n",
      "        \"name\": \"env.pressure\",\n",
      "        \"meta.vsn\": \"W0B0\"\n",
      "    }\n",
      ")\n",
      "\n",
      "\n",
      "# Create the plots\n",
      "temperature_line_plot = temperature_df.hvplot.line(x='timestamp', y='value')\n",
      "pressure_line_plot = pressure_df.hvplot.line(x='timestamp', y='value')\n",
      "\n",
      "temperature_histogram = temperature_df.hvplot.hist(x='value', bins=20)\n",
      "pressure_histogram = pressure_df.hvplot.hist(x='value', bins=20)\n",
      "\n",
      "# Display the plots\n",
      "temperature_line_plot\n",
      "pressure_line_plot\n",
      "\n",
      "temperature_histogram\n",
      "pressure_histogram\n",
      "```\n",
      "This will generate two line plots and two histograms of the temperature and pressure for node W0B0 over the past three hours.\n",
      "\n",
      "When running this code, you might see a warning message about not having access to Plotly. This is expected because it requires a separate installation.\n",
      "\n",
      "\n",
      "CPU times: user 649 ms, sys: 310 ms, total: 960 ms\n",
      "Wall time: 7min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## In manual augmentation, I provided clean dataframes and asked it to provide code in two files\n",
    "\n",
    "long_question = \"\"\"\n",
    "Can you generate the code to graph the line plot and histogram of the temperature and pressure for the last three hours for node W0B0?\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## Sage Chain\n",
    "sage_chain = (\n",
    "    {\"context\": sage_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | sage_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in sage_chain.stream(long_question):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\") # Result: no such thing as \"meta.vsn\" and I couldn't get holoview to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on your specifications, I will write the code to answer your question. Here it is:\n",
      "\n",
      "```python\n",
      "import sage_data_client\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "# query temperature data for last 3h\n",
      "temperature_df = sage_data_client.query(start=\"-3h\", filter={\"name\": \"env.temperature\"})\n",
      "\n",
      "# query pressure data for last 3h\n",
      "pressure_df = sage_data_client.query(start=\"-3h\", filter={\"name\": \"env.pressure\"})\n",
      "\n",
      "# filter data for node W0B0\n",
      "filtered_temperature_df = temperature_df[temperature_df[\"meta.vsn\"] == \"W0B0\"]\n",
      "filtered_pressure_df = pressure_df[pressure_df[\"meta.vsn\"] == \"W0B0\"]\n",
      "\n",
      "# create line plot of temperature and pressure\n",
      "plt.figure(figsize=(10, 6))\n",
      "plt.plot(filtered_temperature_df[\"timestamp\"], filtered_temperature_df[\"value\"], label=\"Temperature\")\n",
      "plt.plot(filtered_pressure_df[\"timestamp\"], filtered_pressure_df[\"value\"], label=\"Pressure\")\n",
      "\n",
      "# add title and labels\n",
      "plt.title(\"Temperature and Pressure for Node W0B0 (Last 3h)\")\n",
      "plt.xlabel(\"Timestamp\")\n",
      "plt.ylabel(\"Value\")\n",
      "plt.legend()\n",
      "\n",
      "# display plot\n",
      "print(\"Plotting temperature and pressure data...\")\n",
      "plt.show()\n",
      "\n",
      "# create histogram of temperature and pressure\n",
      "temperature_hist, _ = plt.hist(filtered_temperature_df[\"value\"], bins=10, label=\"Temperature\")\n",
      "pressure_hist, _ = plt.hist(filtered_pressure_df[\"value\"], bins=10, label=\"Pressure\")\n",
      "\n",
      "# add title and labels\n",
      "plt.title(\"Histogram of Temperature and Pressure for Node W0B0 (Last 3h)\")\n",
      "plt.xlabel(\"Value\")\n",
      "plt.ylabel(\"Frequency\")\n",
      "plt.legend()\n",
      "\n",
      "# display histogram\n",
      "print(\"Plotting histogram of temperature and pressure data...\")\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "Please note that this code assumes you have the `sage_data_client` library installed, which is not a standard Python library. You might need to modify the code to fit your actual implementation.\n",
      "\n",
      "Also, I used matplotlib for plotting, as per your request. If you want to use another library (e.g., seaborn), please let me know and I'll adjust the code accordingly.\n",
      "\n",
      "\n",
      "CPU times: user 739 ms, sys: 318 ms, total: 1.06 s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## In manual augmentation, I provided clean dataframes and asked it to provide code in two files\n",
    "\n",
    "long_question = \"\"\"\n",
    "Can you generate the code to plot the line plot and histogram of the temperature and pressure for the last three hours for node W0B0?\n",
    "Use matplotlib for plotting.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## Sage Chain\n",
    "sage_chain = (\n",
    "    {\"context\": sage_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | sage_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in sage_chain.stream(long_question):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\") \n",
    "# Result: plt.hist does not return two values \"temperature_hist, _\" \n",
    "## also, the temp and pressure values are on completely different scales so plots are unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is how you can create a code that generates line plots and histograms for both temperature and pressure data for the last 3 hours:\n",
      "\n",
      "```python\n",
      "import sage_data_client\n",
      "import pandas as pd\n",
      "from metpy.plots import USCOUNTIES\n",
      "import matplotlib.pyplot as plt\n",
      "import cartopy.crs as ccrs\n",
      "import cartopy.feature as cfeature\n",
      "\n",
      "def plot_temperature():\n",
      "    # query temperature data for node W0B0 and last 3 hours\n",
      "    df = sage_data_client.query(\n",
      "        start='-3h',\n",
      "        filter={\n",
      "            \"name\": \"env.temperature\",\n",
      "            \"meta.vsn\": \"W0B0\"\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    # plot line graph of temperature over time\n",
      "    plt.figure(figsize=(10,6))\n",
      "    plt.plot(df['timestamp'], df['value'])\n",
      "    plt.title('Temperature (degF) for Node W0B0')\n",
      "    plt.xlabel('Time')\n",
      "    plt.ylabel('Temperature (degF)')\n",
      "    plt.show()\n",
      "\n",
      "def plot_pressure():\n",
      "    # query pressure data for node W0B0 and last 3 hours\n",
      "    df = sage_data_client.query(\n",
      "        start='-3h',\n",
      "        filter={\n",
      "            \"name\": \"env.pressure\",\n",
      "            \"meta.vsn\": \"W0B0\"\n",
      "        }\n",
      "    )\n",
      "    \n",
      "    # plot histogram of pressure values\n",
      "    plt.figure(figsize=(10,6))\n",
      "    plt.hist(df['value'], bins=20)\n",
      "    plt.title('Pressure Histogram for Node W0B0')\n",
      "    plt.xlabel('Pressure (in)')\n",
      "    plt.ylabel('Frequency')\n",
      "    plt.show()\n",
      "\n",
      "# call functions to generate plots\n",
      "plot_temperature()\n",
      "plot_pressure()\n",
      "```\n",
      "\n",
      "This script first queries the temperature and pressure data from Sage Data Client API, filtering results for node W0B0 and last 3 hours. Then it uses matplotlib library to plot a line graph of temperature values over time and a histogram of pressure values. The resulting images are displayed with `plt.show()`.\n",
      "\n",
      "\n",
      "CPU times: user 636 ms, sys: 185 ms, total: 821 ms\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## In manual augmentation, I provided clean dataframes and asked it to provide code in two files\n",
    "\n",
    "long_question = \"\"\"\n",
    "Can you generate the code to plot the line plot and histogram of the temperature and pressure for the last three hours for node W0B0?\n",
    "Use two separate functions, one for temperature and one for pressure, and plot with matplotlib.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "## Sage Chain\n",
    "sage_chain = (\n",
    "    {\"context\": sage_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | sage_prompt ## Up to here it produces the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in sage_chain.stream(long_question):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sdc-plots/plot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"sdc-plots/hist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
