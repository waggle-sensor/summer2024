{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG-llama3.1 Example\n",
    "\n",
    "This notebook shows the following: \n",
    "* Explains the typical process of a RAG system\n",
    "* A query about Sage without RAG\n",
    "* A query about Sage with RAG and a few tests\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting up RAG**\n",
    "* Data retrieval and ingestion\n",
    "  * Chunking\n",
    "  * Data representation\n",
    "* Embed data\n",
    "  * Choice of model\n",
    "* Store embeddings in vector database\n",
    "  * Choice of vector base\n",
    "\n",
    "**LLM-RAG Pipeline**\n",
    "* Receive prompt\n",
    "* Embed prompt\n",
    "* Query vector database for vectors (RETRIEVAL)\n",
    "  * Similarity process (e.g., cosine, ANN) \n",
    "  * Selection of k top passages\n",
    "  * Optional: advanced techniques like reranking and activate retrieval\n",
    "* Decode vectors to context\n",
    "* Feed new context into model with prompt (GENERATION)\n",
    "\n",
    "> Notes\n",
    "> * Context is not saved betwen each `ollama.generate()`\n",
    "> * Site for embedding models: https://huggingface.co/spaces/mteb/leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ollama.generate(model='llama3.1', prompt='What is the Sage project?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sage project is a free and open-source mathematical software system that provides a comprehensive platform for research, education, and development in mathematics. It was started in 1997 by William Stein, a mathematician at Harvard University.\n",
      "\n",
      "Sage is designed to be a \"one-stop shop\" for mathematical computations, providing an integrated environment that allows users to work with various mathematical structures, such as numbers (integers, rationals, reals), algebraic objects (groups, rings, fields), and geometric objects (points, curves, surfaces). The system includes a wide range of features and tools, including:\n",
      "\n",
      "1. **Symbolic Computation**: Sage can perform symbolic manipulations, solving equations, and computing integrals.\n",
      "2. **Numerical Computation**: It provides numerical computation capabilities using libraries like NumPy and SciPy.\n",
      "3. **Algebraic Geometry**: Sage has a robust library for algebraic geometry, including tools for working with curves and surfaces.\n",
      "4. **Combinatorics**: The system includes extensive combinatorial capabilities, such as working with permutations, combinations, and partitions.\n",
      "5. **Group Theory**: Sage provides an interface to work with finite groups, particularly useful in computer science and cryptography.\n",
      "6. **Graph Theory**: It has a library for graph theory, enabling users to work with graphs and network structures.\n",
      "\n",
      "Sage is built on top of several popular open-source projects, including Python, Cython, and PuLP (a linear programming solver). The system's architecture allows it to be easily extended and customized by developers. Sage supports multiple interfaces, including a graphical user interface (GUI), a web interface (via a Jupyter notebook), and a command-line interface.\n",
      "\n",
      "Sage is used in various academic and research settings, particularly in mathematics education and computer science. It has also been adopted by industry professionals, such as researchers and engineers working with cryptography, optimization problems, and computational number theory.\n",
      "\n",
      "Overall, the Sage project aims to provide an open-source alternative to commercial software like Mathematica or Maple, serving as a collaborative platform for advancing mathematical research and education.\n"
     ]
    }
   ],
   "source": [
    "print(output['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG\n",
    "\n",
    "* This does not include chunking, reranking, or specialized vector databases/embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a list of strings and encodes them using \"embed_model\" from Ollama\n",
    "# outputs those vector encodings as numpy array\n",
    "def embed_data (data, embed_model):\n",
    "    vector_list = []\n",
    "\n",
    "    # embedding data\n",
    "    for sentence in data:\n",
    "        sent_emb = ollama.embeddings(model=embed_model, prompt=sentence)\n",
    "        vector_list.append(sent_emb['embedding'])\n",
    "\n",
    "    # stored vectors\n",
    "    data_vectors = np.array(vector_list)\n",
    "\n",
    "    return data_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a prompt and encodes it using \"embed_model\" from Ollama\n",
    "# oututs the associated vector\n",
    "def embed_prompt (user_prompt, embed_model):\n",
    "    user_emb = ollama.embeddings(model=embed_model, prompt=user_prompt)\n",
    "    vector_prompt = np.array(user_emb['embedding'])\n",
    "    return vector_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls helper functions to encode the data and prompt\n",
    "# it then finds the best context within the data with a cosine similarity test\n",
    "# outputs the index for the best context and its similarity score with the prompt\n",
    "def find_best_similarity (data, user_prompt, embed_model):\n",
    "    best_similarity = 0\n",
    "    best_data_index = 0\n",
    "\n",
    "    vector_prompt = embed_prompt(user_prompt, embed_model)\n",
    "    data_vectors = embed_data(data, embed_model)\n",
    "\n",
    "    # finding the best \n",
    "    for i, vec in enumerate(data_vectors):\n",
    "        similarity = np.dot(vector_prompt,vec)/(norm(vector_prompt)*norm(vec))\n",
    "        print(f'Similarity score at index {i}: {similarity}')\n",
    "        \n",
    "        if similarity > best_similarity:\n",
    "            best_similarity = similarity\n",
    "            best_data_index = i\n",
    "\n",
    "    return best_similarity, best_data_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score at index 0: 0.29802225530402804\n",
      "Similarity score at index 1: 0.39987415625480366\n",
      "Similarity score at index 2: 0.17156026505283603\n",
      "Best score: 0.39987415625480366; index: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    'The Sage project is an NSF-funded project at Argonne National Laboratory that explores AI at the Edge', \n",
    "    'The Philippines is an archipelago, which means it’s made up of a group of islands—7,641 islands, to be exact.',\n",
    "    'The Windy City nickname has nothing to do with Chicago’s weather, but it comes from the politics'\n",
    "    ]\n",
    "\n",
    "user_prompt = 'What is the Sage project?'\n",
    "embed_model = 'llama3.1'\n",
    "\n",
    "score, idx = find_best_similarity(data, user_prompt, embed_model)\n",
    "print (f'Best score: {score}; index: {idx}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, I don't have any information about the \"Sage project\" or its connection to the data you provided. The data mentions that the Philippines is an archipelago with 7,641 islands, but it doesn't mention anything about a project called \"Sage\".\n",
      "\n",
      "If you could provide more context or information about the Sage project, I'd be happy to try and help!\n"
     ]
    }
   ],
   "source": [
    "output = ollama.generate(\n",
    "  model=\"llama3.1\",\n",
    "  prompt=f\"Using this data: {data[idx]}. Respond to this prompt: {user_prompt}\"\n",
    ")\n",
    "print(output['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Two -- Tricking llama3.1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score at index 0: 0.6315181663914815\n",
      "Similarity score at index 1: 0.4002249339454586\n",
      "Similarity score at index 2: 1.0000000000000002\n",
      "Score: 1.0000000000000002; index: 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    'Sage Sage Sage Sage Sage', \n",
    "    'The Philippines is an archipelago, which means it’s made up of a group of islands—7,641 islands, to be exact.',\n",
    "    'What is the Sage project?'\n",
    "    ]\n",
    "\n",
    "user_prompt = 'What is the Sage project?'\n",
    "embed_model = 'llama3.1'\n",
    "\n",
    "\n",
    "score, idx = find_best_similarity(data, user_prompt, embed_model)\n",
    "print (f'Score: {score}; index: {idx}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, you didn't provide any specific data for me to draw from. However, I can still give a general response about the Sage project.\n",
      "\n",
      "The Sage project is an open-source mathematical software system that aims to create a comprehensive and user-friendly platform for computational mathematics. The project was started in 2005 by William Stein, a mathematician at Harvard University, with the goal of creating a free and open-source alternative to commercial computer algebra systems (CAS) like Mathematica.\n",
      "\n",
      "Sage is designed to provide an integrated environment for various mathematical tasks, including:\n",
      "\n",
      "1. Computer algebra: Sage includes a powerful CAS that can perform symbolic and numerical computations.\n",
      "2. Number theory: Sage has extensive support for number theory, including algebraic number fields and elliptic curves.\n",
      "3. Algebra: Sage includes tools for working with groups, rings, and other algebraic structures.\n",
      "4. Geometry: Sage provides functionality for geometry, including projective spaces and algebraic curves.\n",
      "5. Statistics: Sage has built-in support for statistical analysis and visualization.\n",
      "\n",
      "The Sage project is developed in a collaborative manner by a community of volunteers from around the world. The software system is released under the GNU General Public License (GPL), which allows users to freely use, modify, and distribute the software.\n",
      "\n",
      "Overall, the Sage project aims to provide a versatile and user-friendly platform for mathematical research and education, making it accessible to students, researchers, and professionals alike.\n"
     ]
    }
   ],
   "source": [
    "output = ollama.generate(\n",
    "  model=\"llama3.1\",\n",
    "  prompt=f\"Using this data: {data[idx]}. Respond to this prompt: {user_prompt}\"\n",
    ")\n",
    "print(output['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Three -- Using MXBAI for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score at index 0: 0.7829698088610217\n",
      "Similarity score at index 1: 0.27547573347012566\n",
      "Similarity score at index 2: 0.280417930363531\n",
      "Best score: 0.7829698088610217; index: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    'The Sage project is an NSF-funded project at Argonne National Laboratory that explores AI at the Edge', \n",
    "    'The Philippines is an archipelago, which means it’s made up of a group of islands—7,641 islands, to be exact.',\n",
    "    'The Windy City nickname has nothing to do with Chicago’s weather, but it comes from the politics'\n",
    "    ]\n",
    "\n",
    "user_prompt = 'What is the Sage project?'\n",
    "embed_model = 'mxbai-embed-large'\n",
    "\n",
    "score, idx = find_best_similarity(data, user_prompt, embed_model)\n",
    "print (f'Best score: {score}; index: {idx}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, here's a response:\n",
      "\n",
      "The Sage project is an initiative funded by the National Science Foundation (NSF) and conducted at Argonne National Laboratory. It specifically focuses on exploring Artificial Intelligence (AI) applications in Edge computing environments. The details about the project's goals, objectives, and outcomes are not provided, but it appears to be a research-oriented effort investigating AI capabilities within Edge settings.\n"
     ]
    }
   ],
   "source": [
    "## With llama3.1\n",
    "output = ollama.generate(\n",
    "  model=\"llama3.1\",\n",
    "  prompt=f\"Using this data: {data[idx]}. Respond to this prompt: {user_prompt}\"\n",
    ")\n",
    "print(output['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sage project is a research initiative funded by the National Science Foundation (NSF) and based at Argonne National Laboratory, which focuses on exploring Artificial Intelligence (AI) in Edge computing environments.\n"
     ]
    }
   ],
   "source": [
    "## With llama3\n",
    "output = ollama.generate(\n",
    "  model=\"llama3\",\n",
    "  prompt=f\"Using this data: {data[idx]}. Respond to this prompt: {user_prompt}\"\n",
    ")\n",
    "print(output['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Four -- Tricking MXBAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score at index 0: 0.7829698088610217\n",
      "Similarity score at index 1: 0.6319757336654319\n",
      "Similarity score at index 2: 0.9999999999999998\n",
      "Best score: 0.9999999999999998; index: 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    'The Sage project is an NSF-funded project at Argonne National Laboratory that explores AI at the Edge', \n",
    "    'Sage Sage Sage Sage Sage Sage Sage',\n",
    "    'What is the Sage project?'\n",
    "    ]\n",
    "\n",
    "user_prompt = 'What is the Sage project?'\n",
    "embed_model = 'mxbai-embed-large'\n",
    "\n",
    "score, idx = find_best_similarity(data, user_prompt, embed_model)\n",
    "print (f'Best score: {score}; index: {idx}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
